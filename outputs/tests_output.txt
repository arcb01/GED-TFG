============================= test session starts ==============================
platform linux -- Python 3.8.8, pytest-7.2.0, pluggy-1.0.0
rootdir: /code/mmf
collected 264 items

tests/common/test_batch_collator.py .                                    [  0%]
tests/common/test_meter.py .                                             [  0%]
tests/common/test_report.py ...                                          [  1%]
tests/common/test_sample.py .....                                        [  3%]
tests/configs/test_configs_for_keys.py ..                                [  4%]
tests/configs/test_zoo_urls.py ..                                        [  5%]
tests/datasets/test_base_dataset.py .                                    [  5%]
tests/datasets/test_bert_processors.py ....                              [  7%]
tests/datasets/test_iteration_strategies.py .....                        [  9%]
tests/datasets/test_mmf_dataset_builder.py ...                           [ 10%]
tests/datasets/test_multi_dataset_loader.py ..                           [ 10%]
tests/datasets/test_prediction_processors.py .                           [ 11%]
tests/datasets/test_processors.py ......s.                               [ 14%]
tests/models/test_albef.py .                                             [ 14%]
tests/models/test_cnn_lstm.py F                                          [ 15%]
tests/models/test_mmbt.py ......                                         [ 17%]
tests/models/test_mmf_transformer.py ...........s...                     [ 23%]
tests/models/test_uniter.py ..FF..                                       [ 25%]
tests/models/test_vilbert.py ....                                        [ 26%]
tests/models/test_vilt.py ....                                           [ 28%]
tests/models/test_vinvl.py FFF..                                         [ 30%]
tests/models/test_visual_bert.py ...                                     [ 31%]
tests/models/interfaces/test_interfaces.py .                             [ 31%]
tests/models/transformers/test_heads.py .............                    [ 36%]
tests/models/transformers/test_heads_dict.py .....                       [ 38%]
tests/modules/test_encoders.py ...ss.......                              [ 43%]
tests/modules/test_fusions.py ..........                                 [ 46%]
tests/modules/test_hf_layers.py .                                        [ 47%]
tests/modules/test_layers.py F..F.                                       [ 49%]
tests/modules/test_losses.py ..........                                  [ 53%]
tests/modules/test_metrics.py ...........................                [ 63%]
tests/modules/test_optimizers.py ..                                      [ 64%]
tests/modules/test_poolers.py ......                                     [ 66%]
tests/modules/test_vit.py ...                                            [ 67%]
tests/trainers/test_device.py .                                          [ 67%]
tests/trainers/test_eval_loop.py F                                       [ 68%]
tests/trainers/test_fp16.py ..                                           [ 68%]
tests/trainers/test_sharded_ddp.py ss                                    [ 69%]
tests/trainers/test_training_loop.py ......F..                           [ 73%]
tests/trainers/callbacks/test_logistics.py ....                          [ 74%]
tests/trainers/callbacks/test_lr_scheduler.py .                          [ 75%]
tests/trainers/callbacks/test_user_callback.py .                         [ 75%]
tests/trainers/lightning/test_checkpoint.py FFFFFF.FF.F                  [ 79%]
tests/trainers/lightning/test_grad_accumulate.py F                       [ 79%]
tests/trainers/lightning/test_grad_clipping.py F                         [ 80%]
tests/trainers/lightning/test_logging.py F                               [ 80%]
tests/trainers/lightning/test_loop_conditions.py FFF                     [ 81%]
tests/trainers/lightning/test_loss.py F                                  [ 82%]
tests/trainers/lightning/test_lr_schedule.py FF                          [ 82%]
tests/trainers/lightning/test_validation.py FFF                          [ 84%]
tests/utils/test_checkpoint.py F........                                 [ 87%]
tests/utils/test_configuration.py ..                                     [ 88%]
tests/utils/test_distributed.py .                                        [ 88%]
tests/utils/test_download.py ...                                         [ 89%]
tests/utils/test_env.py F...F                                            [ 91%]
tests/utils/test_file_io.py ....                                         [ 93%]
tests/utils/test_general.py ..                                           [ 93%]
tests/utils/test_logger.py ..                                            [ 94%]
tests/utils/test_patch.py ..                                             [ 95%]
tests/utils/test_quality_checks.py ..                                    [ 96%]
tests/utils/test_text.py ......                                          [ 98%]
tests/utils/test_timer.py ...                                            [ 99%]
tests/utils/test_visualize.py .                                          [100%]

=================================== FAILURES ===================================
________________________ TestModelCNNLSTM.test_forward _________________________

self = <tests.models.test_cnn_lstm.TestModelCNNLSTM testMethod=test_forward>

    def test_forward(self):
        model_config = self.config.model_config.cnn_lstm
    
        cnn_lstm = CNNLSTM(model_config)
        cnn_lstm.build()
        cnn_lstm.init_losses()
    
        self.assertTrue(isinstance(cnn_lstm, torch.nn.Module))
    
        test_sample = Sample()
        test_sample.text = torch.randint(1, 79, (10,), dtype=torch.long)
        test_sample.image = torch.randn(3, 320, 480)
        test_sample.targets = torch.randn(32)
    
        test_sample_list = SampleList([test_sample])
        test_sample_list.dataset_type = "train"
        test_sample_list.dataset_name = "clevr"
    
        test_sample_list = test_sample_list.to(get_current_device())
        cnn_lstm = cnn_lstm.to(get_current_device())
    
        output = cnn_lstm(test_sample_list)
    
        scores = output["scores"]
        loss = output["losses"]["train/clevr/logit_bce"]
    
>       np.testing.assert_almost_equal(loss.item(), 19.2635, decimal=4)
E       AssertionError: 
E       Arrays are not almost equal to 4 decimals
E        ACTUAL: 19.263776779174805
E        DESIRED: 19.2635

tests/models/test_cnn_lstm.py:72: AssertionError
______________ TestUniterWithHeads.test_uniter_for_classification ______________

self = <tests.models.test_uniter.TestUniterWithHeads testMethod=test_uniter_for_classification>

    def test_uniter_for_classification(self):
        heads = {"test": {"type": "mlp", "num_labels": 3129}}
        tasks = "test"
        losses = {"test": "logit_bce"}
        model = UNITERForClassification(
            head_configs=heads, loss_configs=losses, tasks=tasks
        )
    
        model.eval()
        model = model.to(get_current_device())
        sample_list = self._get_sample_list()
    
        with torch.no_grad():
>           model_output = model(sample_list)

tests/models/test_uniter.py:121: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../mmf_venv/lib/python3.8/site-packages/torch/nn/modules/module.py:889: in _call_impl
    result = self.forward(*input, **kwargs)
mmf/models/uniter.py:342: in forward
    return _infer_with_heads(
mmf/models/uniter.py:251: in _infer_with_heads
    sequence_output = uniter_model(
../mmf_venv/lib/python3.8/site-packages/torch/nn/modules/module.py:889: in _call_impl
    result = self.forward(*input, **kwargs)
mmf/models/uniter.py:225: in forward
    embedding_output = self._compute_img_txt_embeddings(
mmf/models/uniter.py:185: in _compute_img_txt_embeddings
    txt_emb = self._compute_txt_embeddings(input_ids, position_ids, txt_type_ids)
mmf/models/uniter.py:153: in _compute_txt_embeddings
    output = self.text_embeddings(
../mmf_venv/lib/python3.8/site-packages/torch/nn/modules/module.py:889: in _call_impl
    result = self.forward(*input, **kwargs)
mmf/modules/hf_layers.py:129: in forward
    inputs_embeds = self.word_embeddings(input_ids)
../mmf_venv/lib/python3.8/site-packages/torch/nn/modules/module.py:889: in _call_impl
    result = self.forward(*input, **kwargs)
../mmf_venv/lib/python3.8/site-packages/torch/nn/modules/sparse.py:156: in forward
    return F.embedding(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1],
        [1, 1, 1, 1, 1, ...1, 1, 1, 1,
         1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1]])
weight = Parameter containing:
tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [-1.3443,  0.3480,...13],
        [ 0.9266,  0.4205,  1.7636,  ...,  0.0355,  0.5066,  0.3794]],
       device='cuda:0', requires_grad=True)
padding_idx = 0, max_norm = None, norm_type = 2.0, scale_grad_by_freq = False
sparse = False

    def embedding(
        input: Tensor,
        weight: Tensor,
        padding_idx: Optional[int] = None,
        max_norm: Optional[float] = None,
        norm_type: float = 2.0,
        scale_grad_by_freq: bool = False,
        sparse: bool = False,
    ) -> Tensor:
        r"""A simple lookup table that looks up embeddings in a fixed dictionary and size.
    
        This module is often used to retrieve word embeddings using indices.
        The input to the module is a list of indices, and the embedding matrix,
        and the output is the corresponding word embeddings.
    
        See :class:`torch.nn.Embedding` for more details.
    
        Args:
            input (LongTensor): Tensor containing indices into the embedding matrix
            weight (Tensor): The embedding matrix with number of rows equal to the maximum possible index + 1,
                and number of columns equal to the embedding size
            padding_idx (int, optional): If specified, the entries at :attr:`padding_idx` do not contribute to the gradient;
                                         therefore, the embedding vector at :attr:`padding_idx` is not updated during training,
                                         i.e. it remains as a fixed "pad".
            max_norm (float, optional): If given, each embedding vector with norm larger than :attr:`max_norm`
                                        is renormalized to have norm :attr:`max_norm`.
                                        Note: this will modify :attr:`weight` in-place.
            norm_type (float, optional): The p of the p-norm to compute for the :attr:`max_norm` option. Default ``2``.
            scale_grad_by_freq (boolean, optional): If given, this will scale gradients by the inverse of frequency of
                                                    the words in the mini-batch. Default ``False``.
            sparse (bool, optional): If ``True``, gradient w.r.t. :attr:`weight` will be a sparse tensor. See Notes under
                                     :class:`torch.nn.Embedding` for more details regarding sparse gradients.
    
        Shape:
            - Input: LongTensor of arbitrary shape containing the indices to extract
            - Weight: Embedding matrix of floating point type with shape `(V, embedding_dim)`,
                                where V = maximum index + 1 and embedding_dim = the embedding size
            - Output: `(*, embedding_dim)`, where `*` is the input shape
    
        Examples::
    
            >>> # a batch of 2 samples of 4 indices each
            >>> input = torch.tensor([[1,2,4,5],[4,3,2,9]])
            >>> # an embedding matrix containing 10 tensors of size 3
            >>> embedding_matrix = torch.rand(10, 3)
            >>> F.embedding(input, embedding_matrix)
            tensor([[[ 0.8490,  0.9625,  0.6753],
                     [ 0.9666,  0.7761,  0.6108],
                     [ 0.6246,  0.9751,  0.3618],
                     [ 0.4161,  0.2419,  0.7383]],
    
                    [[ 0.6246,  0.9751,  0.3618],
                     [ 0.0237,  0.7794,  0.0528],
                     [ 0.9666,  0.7761,  0.6108],
                     [ 0.3385,  0.8612,  0.1867]]])
    
            >>> # example with padding_idx
            >>> weights = torch.rand(10, 3)
            >>> weights[0, :].zero_()
            >>> embedding_matrix = weights
            >>> input = torch.tensor([[0,2,0,5]])
            >>> F.embedding(input, embedding_matrix, padding_idx=0)
            tensor([[[ 0.0000,  0.0000,  0.0000],
                     [ 0.5609,  0.5384,  0.8720],
                     [ 0.0000,  0.0000,  0.0000],
                     [ 0.6262,  0.2438,  0.7471]]])
        """
    
        if padding_idx is not None:
            if padding_idx > 0:
                assert padding_idx < weight.size(0), "Padding_idx must be within num_embeddings"
            elif padding_idx < 0:
                assert padding_idx >= -weight.size(0), "Padding_idx must be within num_embeddings"
                padding_idx = weight.size(0) + padding_idx
        else:
            padding_idx = -1
        if max_norm is not None:
            # Note [embedding_renorm contiguous]
            # `embedding_renorm_` will call .contiguous() on input anyways, so we
            # call it here and take advantage of the improved locality in the
            # `embedding` call below too.
            input = input.contiguous()
            # Note [embedding_renorm set_grad_enabled]
            # XXX: equivalent to
            # with torch.no_grad():
            #   torch.embedding_renorm_
            # remove once script supports set_grad_enabled
            _no_grad_embedding_renorm_(weight, input, max_norm, norm_type)
>       return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
E       RuntimeError: Input, output and indices must be on the current device

../mmf_venv/lib/python3.8/site-packages/torch/nn/functional.py:1916: RuntimeError
----------------------------- Captured stderr call -----------------------------
loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.10.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.10.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
_______________ TestUniterWithHeads.test_uniter_for_pretraining ________________

self = <tests.models.test_uniter.TestUniterWithHeads testMethod=test_uniter_for_pretraining>

    def test_uniter_for_pretraining(self):
        # UNITER pretraining has 5 pretraining tasks,
        # we have one unique head for each, and in each
        # forward pass we train on a different task.
        # In this test we try running a forward pass
        # through each head.
        heads = {
            "mlm": {"type": "mlm"},
            "itm": {"type": "itm"},
            "mrc": {"type": "mrc"},
            "mrfr": {"type": "mrfr"},
            "wra": {"type": "wra"},
        }
        tasks = "mlm,itm,mrc,mrfr,wra"
        mask_probability = 0.15
        model = UNITERForPretraining(
            head_configs=heads, tasks=tasks, mask_probability=mask_probability
        )
        model.eval()
        model = model.to(get_current_device())
        sample_list = self._get_sample_list()
        self._enhance_sample_list_for_pretraining(sample_list)
    
        expected_loss_names = {
            "mlm": "masked_lm_loss",
            "itm": "itm_loss",
            "mrc": "mrc_loss",
            "mrfr": "mrfr_loss",
            "wra": "wra_loss",
        }
    
        for task_name, loss_name in expected_loss_names.items():
            sample_list["task"] = task_name
            with torch.no_grad():
>               model_output = model(sample_list)

tests/models/test_uniter.py:177: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../mmf_venv/lib/python3.8/site-packages/torch/nn/modules/module.py:889: in _call_impl
    result = self.forward(*input, **kwargs)
mmf/models/uniter.py:439: in forward
    return _infer_with_heads(
mmf/models/uniter.py:251: in _infer_with_heads
    sequence_output = uniter_model(
../mmf_venv/lib/python3.8/site-packages/torch/nn/modules/module.py:889: in _call_impl
    result = self.forward(*input, **kwargs)
mmf/models/uniter.py:225: in forward
    embedding_output = self._compute_img_txt_embeddings(
mmf/models/uniter.py:185: in _compute_img_txt_embeddings
    txt_emb = self._compute_txt_embeddings(input_ids, position_ids, txt_type_ids)
mmf/models/uniter.py:153: in _compute_txt_embeddings
    output = self.text_embeddings(
../mmf_venv/lib/python3.8/site-packages/torch/nn/modules/module.py:889: in _call_impl
    result = self.forward(*input, **kwargs)
mmf/modules/hf_layers.py:129: in forward
    inputs_embeds = self.word_embeddings(input_ids)
../mmf_venv/lib/python3.8/site-packages/torch/nn/modules/module.py:889: in _call_impl
    result = self.forward(*input, **kwargs)
../mmf_venv/lib/python3.8/site-packages/torch/nn/modules/sparse.py:156: in forward
    return F.embedding(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1],
        [1, 1, 1, 1, 1, ...1, 1, 1, 1,
         1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1]])
weight = Parameter containing:
tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [-1.1268,  0.6885,...68],
        [-0.3899, -1.3882, -1.3762,  ...,  0.9283,  1.2140, -0.4024]],
       device='cuda:0', requires_grad=True)
padding_idx = 0, max_norm = None, norm_type = 2.0, scale_grad_by_freq = False
sparse = False

    def embedding(
        input: Tensor,
        weight: Tensor,
        padding_idx: Optional[int] = None,
        max_norm: Optional[float] = None,
        norm_type: float = 2.0,
        scale_grad_by_freq: bool = False,
        sparse: bool = False,
    ) -> Tensor:
        r"""A simple lookup table that looks up embeddings in a fixed dictionary and size.
    
        This module is often used to retrieve word embeddings using indices.
        The input to the module is a list of indices, and the embedding matrix,
        and the output is the corresponding word embeddings.
    
        See :class:`torch.nn.Embedding` for more details.
    
        Args:
            input (LongTensor): Tensor containing indices into the embedding matrix
            weight (Tensor): The embedding matrix with number of rows equal to the maximum possible index + 1,
                and number of columns equal to the embedding size
            padding_idx (int, optional): If specified, the entries at :attr:`padding_idx` do not contribute to the gradient;
                                         therefore, the embedding vector at :attr:`padding_idx` is not updated during training,
                                         i.e. it remains as a fixed "pad".
            max_norm (float, optional): If given, each embedding vector with norm larger than :attr:`max_norm`
                                        is renormalized to have norm :attr:`max_norm`.
                                        Note: this will modify :attr:`weight` in-place.
            norm_type (float, optional): The p of the p-norm to compute for the :attr:`max_norm` option. Default ``2``.
            scale_grad_by_freq (boolean, optional): If given, this will scale gradients by the inverse of frequency of
                                                    the words in the mini-batch. Default ``False``.
            sparse (bool, optional): If ``True``, gradient w.r.t. :attr:`weight` will be a sparse tensor. See Notes under
                                     :class:`torch.nn.Embedding` for more details regarding sparse gradients.
    
        Shape:
            - Input: LongTensor of arbitrary shape containing the indices to extract
            - Weight: Embedding matrix of floating point type with shape `(V, embedding_dim)`,
                                where V = maximum index + 1 and embedding_dim = the embedding size
            - Output: `(*, embedding_dim)`, where `*` is the input shape
    
        Examples::
    
            >>> # a batch of 2 samples of 4 indices each
            >>> input = torch.tensor([[1,2,4,5],[4,3,2,9]])
            >>> # an embedding matrix containing 10 tensors of size 3
            >>> embedding_matrix = torch.rand(10, 3)
            >>> F.embedding(input, embedding_matrix)
            tensor([[[ 0.8490,  0.9625,  0.6753],
                     [ 0.9666,  0.7761,  0.6108],
                     [ 0.6246,  0.9751,  0.3618],
                     [ 0.4161,  0.2419,  0.7383]],
    
                    [[ 0.6246,  0.9751,  0.3618],
                     [ 0.0237,  0.7794,  0.0528],
                     [ 0.9666,  0.7761,  0.6108],
                     [ 0.3385,  0.8612,  0.1867]]])
    
            >>> # example with padding_idx
            >>> weights = torch.rand(10, 3)
            >>> weights[0, :].zero_()
            >>> embedding_matrix = weights
            >>> input = torch.tensor([[0,2,0,5]])
            >>> F.embedding(input, embedding_matrix, padding_idx=0)
            tensor([[[ 0.0000,  0.0000,  0.0000],
                     [ 0.5609,  0.5384,  0.8720],
                     [ 0.0000,  0.0000,  0.0000],
                     [ 0.6262,  0.2438,  0.7471]]])
        """
    
        if padding_idx is not None:
            if padding_idx > 0:
                assert padding_idx < weight.size(0), "Padding_idx must be within num_embeddings"
            elif padding_idx < 0:
                assert padding_idx >= -weight.size(0), "Padding_idx must be within num_embeddings"
                padding_idx = weight.size(0) + padding_idx
        else:
            padding_idx = -1
        if max_norm is not None:
            # Note [embedding_renorm contiguous]
            # `embedding_renorm_` will call .contiguous() on input anyways, so we
            # call it here and take advantage of the improved locality in the
            # `embedding` call below too.
            input = input.contiguous()
            # Note [embedding_renorm set_grad_enabled]
            # XXX: equivalent to
            # with torch.no_grad():
            #   torch.embedding_renorm_
            # remove once script supports set_grad_enabled
            _no_grad_embedding_renorm_(weight, input, max_norm, norm_type)
>       return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
E       RuntimeError: Input, output and indices must be on the current device

../mmf_venv/lib/python3.8/site-packages/torch/nn/functional.py:1916: RuntimeError
----------------------------- Captured stderr call -----------------------------
loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.10.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.10.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
------------------------------ Captured log call -------------------------------
WARNING  root:uniter.py:408 No loss defined for mlm. Head is expected to return dict with 'losses'
WARNING  root:uniter.py:408 No loss defined for itm. Head is expected to return dict with 'losses'
WARNING  root:uniter.py:408 No loss defined for mrc. Head is expected to return dict with 'losses'
WARNING  root:uniter.py:408 No loss defined for mrfr. Head is expected to return dict with 'losses'
WARNING  root:uniter.py:408 No loss defined for wra. Head is expected to return dict with 'losses'
__________________________ TestVinVLBase.test_forward __________________________

self = <tests.models.test_vinvl.TestVinVLBase testMethod=test_forward>

    def test_forward(self):
        img_feature_dim = 2054
        bert_model_name = "bert-base-uncased"
        use_img_layernorm = True
        img_layer_norm_eps = 1e-12
        bert_config = BertConfig.from_pretrained(bert_model_name)
        # augment hf BertConfig for vinvl BertImgModel config
        bert_config.img_feature_dim = img_feature_dim
        bert_config.use_img_layernorm = use_img_layernorm
        bert_config.img_layer_norm_eps = img_layer_norm_eps
        model = VinVLBase(bert_config)
    
        model.eval()
        model = model.to(get_current_device())
    
        bs = 8
        num_feats = 70
        max_sentence_len = 25
        input_ids = torch.ones((bs, max_sentence_len), dtype=torch.long)
        img_feat = torch.rand((bs, num_feats, img_feature_dim))
    
        with torch.no_grad():
>           model_output = model(input_ids, img_feat).last_hidden_state

tests/models/test_vinvl.py:40: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../mmf_venv/lib/python3.8/site-packages/torch/nn/modules/module.py:889: in _call_impl
    result = self.forward(*input, **kwargs)
mmf/models/vinvl.py:100: in forward
    text_embedding_output = self.embeddings(
../mmf_venv/lib/python3.8/site-packages/torch/nn/modules/module.py:889: in _call_impl
    result = self.forward(*input, **kwargs)
../mmf_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:215: in forward
    inputs_embeds = self.word_embeddings(input_ids)
../mmf_venv/lib/python3.8/site-packages/torch/nn/modules/module.py:889: in _call_impl
    result = self.forward(*input, **kwargs)
../mmf_venv/lib/python3.8/site-packages/torch/nn/modules/sparse.py:156: in forward
    return F.embedding(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1],
        [1, 1, 1, 1, 1, ...1, 1, 1, 1,
         1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1]])
weight = Parameter containing:
tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.1608, -0.7847,...40],
        [ 0.8472,  1.4418,  0.4726,  ..., -0.5818, -0.8659, -0.9895]],
       device='cuda:0', requires_grad=True)
padding_idx = 0, max_norm = None, norm_type = 2.0, scale_grad_by_freq = False
sparse = False

    def embedding(
        input: Tensor,
        weight: Tensor,
        padding_idx: Optional[int] = None,
        max_norm: Optional[float] = None,
        norm_type: float = 2.0,
        scale_grad_by_freq: bool = False,
        sparse: bool = False,
    ) -> Tensor:
        r"""A simple lookup table that looks up embeddings in a fixed dictionary and size.
    
        This module is often used to retrieve word embeddings using indices.
        The input to the module is a list of indices, and the embedding matrix,
        and the output is the corresponding word embeddings.
    
        See :class:`torch.nn.Embedding` for more details.
    
        Args:
            input (LongTensor): Tensor containing indices into the embedding matrix
            weight (Tensor): The embedding matrix with number of rows equal to the maximum possible index + 1,
                and number of columns equal to the embedding size
            padding_idx (int, optional): If specified, the entries at :attr:`padding_idx` do not contribute to the gradient;
                                         therefore, the embedding vector at :attr:`padding_idx` is not updated during training,
                                         i.e. it remains as a fixed "pad".
            max_norm (float, optional): If given, each embedding vector with norm larger than :attr:`max_norm`
                                        is renormalized to have norm :attr:`max_norm`.
                                        Note: this will modify :attr:`weight` in-place.
            norm_type (float, optional): The p of the p-norm to compute for the :attr:`max_norm` option. Default ``2``.
            scale_grad_by_freq (boolean, optional): If given, this will scale gradients by the inverse of frequency of
                                                    the words in the mini-batch. Default ``False``.
            sparse (bool, optional): If ``True``, gradient w.r.t. :attr:`weight` will be a sparse tensor. See Notes under
                                     :class:`torch.nn.Embedding` for more details regarding sparse gradients.
    
        Shape:
            - Input: LongTensor of arbitrary shape containing the indices to extract
            - Weight: Embedding matrix of floating point type with shape `(V, embedding_dim)`,
                                where V = maximum index + 1 and embedding_dim = the embedding size
            - Output: `(*, embedding_dim)`, where `*` is the input shape
    
        Examples::
    
            >>> # a batch of 2 samples of 4 indices each
            >>> input = torch.tensor([[1,2,4,5],[4,3,2,9]])
            >>> # an embedding matrix containing 10 tensors of size 3
            >>> embedding_matrix = torch.rand(10, 3)
            >>> F.embedding(input, embedding_matrix)
            tensor([[[ 0.8490,  0.9625,  0.6753],
                     [ 0.9666,  0.7761,  0.6108],
                     [ 0.6246,  0.9751,  0.3618],
                     [ 0.4161,  0.2419,  0.7383]],
    
                    [[ 0.6246,  0.9751,  0.3618],
                     [ 0.0237,  0.7794,  0.0528],
                     [ 0.9666,  0.7761,  0.6108],
                     [ 0.3385,  0.8612,  0.1867]]])
    
            >>> # example with padding_idx
            >>> weights = torch.rand(10, 3)
            >>> weights[0, :].zero_()
            >>> embedding_matrix = weights
            >>> input = torch.tensor([[0,2,0,5]])
            >>> F.embedding(input, embedding_matrix, padding_idx=0)
            tensor([[[ 0.0000,  0.0000,  0.0000],
                     [ 0.5609,  0.5384,  0.8720],
                     [ 0.0000,  0.0000,  0.0000],
                     [ 0.6262,  0.2438,  0.7471]]])
        """
    
        if padding_idx is not None:
            if padding_idx > 0:
                assert padding_idx < weight.size(0), "Padding_idx must be within num_embeddings"
            elif padding_idx < 0:
                assert padding_idx >= -weight.size(0), "Padding_idx must be within num_embeddings"
                padding_idx = weight.size(0) + padding_idx
        else:
            padding_idx = -1
        if max_norm is not None:
            # Note [embedding_renorm contiguous]
            # `embedding_renorm_` will call .contiguous() on input anyways, so we
            # call it here and take advantage of the improved locality in the
            # `embedding` call below too.
            input = input.contiguous()
            # Note [embedding_renorm set_grad_enabled]
            # XXX: equivalent to
            # with torch.no_grad():
            #   torch.embedding_renorm_
            # remove once script supports set_grad_enabled
            _no_grad_embedding_renorm_(weight, input, max_norm, norm_type)
>       return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
E       RuntimeError: Input, output and indices must be on the current device

../mmf_venv/lib/python3.8/site-packages/torch/nn/functional.py:1916: RuntimeError
----------------------------- Captured stderr call -----------------------------
loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.10.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

_____ TestVinVLForClassificationAndPretraining.test_classification_forward _____

self = <tests.models.test_vinvl.TestVinVLForClassificationAndPretraining testMethod=test_classification_forward>

    def test_classification_forward(self):
        model = VinVLForClassification().to(get_current_device())
        model.eval()
    
        with torch.no_grad():
>           model_output = model(
                input_ids=self.input_ids,
                img_feats=self.img_feats,
                attention_mask=self.attention_mask,
                token_type_ids=self.token_type_ids,
                labels=self.labels,
            )

tests/models/test_vinvl.py:68: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../mmf_venv/lib/python3.8/site-packages/torch/nn/modules/module.py:889: in _call_impl
    result = self.forward(*input, **kwargs)
mmf/models/vinvl.py:211: in forward
    sequence_output = self.bert(
../mmf_venv/lib/python3.8/site-packages/torch/nn/modules/module.py:889: in _call_impl
    result = self.forward(*input, **kwargs)
mmf/models/vinvl.py:100: in forward
    text_embedding_output = self.embeddings(
../mmf_venv/lib/python3.8/site-packages/torch/nn/modules/module.py:889: in _call_impl
    result = self.forward(*input, **kwargs)
../mmf_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:215: in forward
    inputs_embeds = self.word_embeddings(input_ids)
../mmf_venv/lib/python3.8/site-packages/torch/nn/modules/module.py:889: in _call_impl
    result = self.forward(*input, **kwargs)
../mmf_venv/lib/python3.8/site-packages/torch/nn/modules/sparse.py:156: in forward
    return F.embedding(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1],
        [1, 1, 1, 1, 1, ...1, 1, 1, 1,
         1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1]])
weight = Parameter containing:
tensor([[-0.0102, -0.0615, -0.0265,  ..., -0.0199, -0.0372, -0.0098],
        [-0.0117, -0.0600,...95],
        [ 0.0015, -0.0821, -0.0160,  ..., -0.0081, -0.0475,  0.0753]],
       device='cuda:0', requires_grad=True)
padding_idx = 0, max_norm = None, norm_type = 2.0, scale_grad_by_freq = False
sparse = False

    def embedding(
        input: Tensor,
        weight: Tensor,
        padding_idx: Optional[int] = None,
        max_norm: Optional[float] = None,
        norm_type: float = 2.0,
        scale_grad_by_freq: bool = False,
        sparse: bool = False,
    ) -> Tensor:
        r"""A simple lookup table that looks up embeddings in a fixed dictionary and size.
    
        This module is often used to retrieve word embeddings using indices.
        The input to the module is a list of indices, and the embedding matrix,
        and the output is the corresponding word embeddings.
    
        See :class:`torch.nn.Embedding` for more details.
    
        Args:
            input (LongTensor): Tensor containing indices into the embedding matrix
            weight (Tensor): The embedding matrix with number of rows equal to the maximum possible index + 1,
                and number of columns equal to the embedding size
            padding_idx (int, optional): If specified, the entries at :attr:`padding_idx` do not contribute to the gradient;
                                         therefore, the embedding vector at :attr:`padding_idx` is not updated during training,
                                         i.e. it remains as a fixed "pad".
            max_norm (float, optional): If given, each embedding vector with norm larger than :attr:`max_norm`
                                        is renormalized to have norm :attr:`max_norm`.
                                        Note: this will modify :attr:`weight` in-place.
            norm_type (float, optional): The p of the p-norm to compute for the :attr:`max_norm` option. Default ``2``.
            scale_grad_by_freq (boolean, optional): If given, this will scale gradients by the inverse of frequency of
                                                    the words in the mini-batch. Default ``False``.
            sparse (bool, optional): If ``True``, gradient w.r.t. :attr:`weight` will be a sparse tensor. See Notes under
                                     :class:`torch.nn.Embedding` for more details regarding sparse gradients.
    
        Shape:
            - Input: LongTensor of arbitrary shape containing the indices to extract
            - Weight: Embedding matrix of floating point type with shape `(V, embedding_dim)`,
                                where V = maximum index + 1 and embedding_dim = the embedding size
            - Output: `(*, embedding_dim)`, where `*` is the input shape
    
        Examples::
    
            >>> # a batch of 2 samples of 4 indices each
            >>> input = torch.tensor([[1,2,4,5],[4,3,2,9]])
            >>> # an embedding matrix containing 10 tensors of size 3
            >>> embedding_matrix = torch.rand(10, 3)
            >>> F.embedding(input, embedding_matrix)
            tensor([[[ 0.8490,  0.9625,  0.6753],
                     [ 0.9666,  0.7761,  0.6108],
                     [ 0.6246,  0.9751,  0.3618],
                     [ 0.4161,  0.2419,  0.7383]],
    
                    [[ 0.6246,  0.9751,  0.3618],
                     [ 0.0237,  0.7794,  0.0528],
                     [ 0.9666,  0.7761,  0.6108],
                     [ 0.3385,  0.8612,  0.1867]]])
    
            >>> # example with padding_idx
            >>> weights = torch.rand(10, 3)
            >>> weights[0, :].zero_()
            >>> embedding_matrix = weights
            >>> input = torch.tensor([[0,2,0,5]])
            >>> F.embedding(input, embedding_matrix, padding_idx=0)
            tensor([[[ 0.0000,  0.0000,  0.0000],
                     [ 0.5609,  0.5384,  0.8720],
                     [ 0.0000,  0.0000,  0.0000],
                     [ 0.6262,  0.2438,  0.7471]]])
        """
    
        if padding_idx is not None:
            if padding_idx > 0:
                assert padding_idx < weight.size(0), "Padding_idx must be within num_embeddings"
            elif padding_idx < 0:
                assert padding_idx >= -weight.size(0), "Padding_idx must be within num_embeddings"
                padding_idx = weight.size(0) + padding_idx
        else:
            padding_idx = -1
        if max_norm is not None:
            # Note [embedding_renorm contiguous]
            # `embedding_renorm_` will call .contiguous() on input anyways, so we
            # call it here and take advantage of the improved locality in the
            # `embedding` call below too.
            input = input.contiguous()
            # Note [embedding_renorm set_grad_enabled]
            # XXX: equivalent to
            # with torch.no_grad():
            #   torch.embedding_renorm_
            # remove once script supports set_grad_enabled
            _no_grad_embedding_renorm_(weight, input, max_norm, norm_type)
>       return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
E       RuntimeError: Input, output and indices must be on the current device

../mmf_venv/lib/python3.8/site-packages/torch/nn/functional.py:1916: RuntimeError
----------------------------- Captured stderr call -----------------------------
loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.10.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
Some weights of the model checkpoint at bert-base-uncased were not used when initializing VinVLBase: ['bert.pooler.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'bert.pooler.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing VinVLBase from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing VinVLBase from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of VinVLBase were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.img_embedding.0.weight', 'bert.img_embedding.1.weight', 'bert.img_embedding.1.bias', 'bert.img_embedding.0.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
______ TestVinVLForClassificationAndPretraining.test_pretraining_forward _______

self = <tests.models.test_vinvl.TestVinVLForClassificationAndPretraining testMethod=test_pretraining_forward>

    def test_pretraining_forward(self):
        model = VinVLForPretraining().to(get_current_device())
        model.eval()
    
        with torch.no_grad():
>           model_output = model(
                img_feats=self.img_feats,
                attention_mask=self.attention_mask,
                token_type_ids=self.token_type_ids,
                input_ids_masked=self.input_ids,
                lm_label_ids=self.lm_label_ids,
                contrastive_labels=self.contrastive_labels,
                input_ids_corrupt=self.input_ids,
                token_type_ids_corrupt=self.token_type_ids,
                attention_mask_corrupt=self.attention_mask,
            )

tests/models/test_vinvl.py:84: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../mmf_venv/lib/python3.8/site-packages/torch/nn/modules/module.py:889: in _call_impl
    result = self.forward(*input, **kwargs)
mmf/models/vinvl.py:365: in forward
    mlm_result = self.mlm_forward(
mmf/models/vinvl.py:303: in mlm_forward
    hidden_layers = self.bert(
../mmf_venv/lib/python3.8/site-packages/torch/nn/modules/module.py:889: in _call_impl
    result = self.forward(*input, **kwargs)
mmf/models/vinvl.py:100: in forward
    text_embedding_output = self.embeddings(
../mmf_venv/lib/python3.8/site-packages/torch/nn/modules/module.py:889: in _call_impl
    result = self.forward(*input, **kwargs)
../mmf_venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:215: in forward
    inputs_embeds = self.word_embeddings(input_ids)
../mmf_venv/lib/python3.8/site-packages/torch/nn/modules/module.py:889: in _call_impl
    result = self.forward(*input, **kwargs)
../mmf_venv/lib/python3.8/site-packages/torch/nn/modules/sparse.py:156: in forward
    return F.embedding(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1],
        [1, 1, 1, 1, 1, ...1, 1, 1, 1,
         1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1]])
weight = Parameter containing:
tensor([[-0.0102, -0.0615, -0.0265,  ..., -0.0199, -0.0372, -0.0098],
        [-0.0117, -0.0600,...95],
        [ 0.0015, -0.0821, -0.0160,  ..., -0.0081, -0.0475,  0.0753]],
       device='cuda:0', requires_grad=True)
padding_idx = 0, max_norm = None, norm_type = 2.0, scale_grad_by_freq = False
sparse = False

    def embedding(
        input: Tensor,
        weight: Tensor,
        padding_idx: Optional[int] = None,
        max_norm: Optional[float] = None,
        norm_type: float = 2.0,
        scale_grad_by_freq: bool = False,
        sparse: bool = False,
    ) -> Tensor:
        r"""A simple lookup table that looks up embeddings in a fixed dictionary and size.
    
        This module is often used to retrieve word embeddings using indices.
        The input to the module is a list of indices, and the embedding matrix,
        and the output is the corresponding word embeddings.
    
        See :class:`torch.nn.Embedding` for more details.
    
        Args:
            input (LongTensor): Tensor containing indices into the embedding matrix
            weight (Tensor): The embedding matrix with number of rows equal to the maximum possible index + 1,
                and number of columns equal to the embedding size
            padding_idx (int, optional): If specified, the entries at :attr:`padding_idx` do not contribute to the gradient;
                                         therefore, the embedding vector at :attr:`padding_idx` is not updated during training,
                                         i.e. it remains as a fixed "pad".
            max_norm (float, optional): If given, each embedding vector with norm larger than :attr:`max_norm`
                                        is renormalized to have norm :attr:`max_norm`.
                                        Note: this will modify :attr:`weight` in-place.
            norm_type (float, optional): The p of the p-norm to compute for the :attr:`max_norm` option. Default ``2``.
            scale_grad_by_freq (boolean, optional): If given, this will scale gradients by the inverse of frequency of
                                                    the words in the mini-batch. Default ``False``.
            sparse (bool, optional): If ``True``, gradient w.r.t. :attr:`weight` will be a sparse tensor. See Notes under
                                     :class:`torch.nn.Embedding` for more details regarding sparse gradients.
    
        Shape:
            - Input: LongTensor of arbitrary shape containing the indices to extract
            - Weight: Embedding matrix of floating point type with shape `(V, embedding_dim)`,
                                where V = maximum index + 1 and embedding_dim = the embedding size
            - Output: `(*, embedding_dim)`, where `*` is the input shape
    
        Examples::
    
            >>> # a batch of 2 samples of 4 indices each
            >>> input = torch.tensor([[1,2,4,5],[4,3,2,9]])
            >>> # an embedding matrix containing 10 tensors of size 3
            >>> embedding_matrix = torch.rand(10, 3)
            >>> F.embedding(input, embedding_matrix)
            tensor([[[ 0.8490,  0.9625,  0.6753],
                     [ 0.9666,  0.7761,  0.6108],
                     [ 0.6246,  0.9751,  0.3618],
                     [ 0.4161,  0.2419,  0.7383]],
    
                    [[ 0.6246,  0.9751,  0.3618],
                     [ 0.0237,  0.7794,  0.0528],
                     [ 0.9666,  0.7761,  0.6108],
                     [ 0.3385,  0.8612,  0.1867]]])
    
            >>> # example with padding_idx
            >>> weights = torch.rand(10, 3)
            >>> weights[0, :].zero_()
            >>> embedding_matrix = weights
            >>> input = torch.tensor([[0,2,0,5]])
            >>> F.embedding(input, embedding_matrix, padding_idx=0)
            tensor([[[ 0.0000,  0.0000,  0.0000],
                     [ 0.5609,  0.5384,  0.8720],
                     [ 0.0000,  0.0000,  0.0000],
                     [ 0.6262,  0.2438,  0.7471]]])
        """
    
        if padding_idx is not None:
            if padding_idx > 0:
                assert padding_idx < weight.size(0), "Padding_idx must be within num_embeddings"
            elif padding_idx < 0:
                assert padding_idx >= -weight.size(0), "Padding_idx must be within num_embeddings"
                padding_idx = weight.size(0) + padding_idx
        else:
            padding_idx = -1
        if max_norm is not None:
            # Note [embedding_renorm contiguous]
            # `embedding_renorm_` will call .contiguous() on input anyways, so we
            # call it here and take advantage of the improved locality in the
            # `embedding` call below too.
            input = input.contiguous()
            # Note [embedding_renorm set_grad_enabled]
            # XXX: equivalent to
            # with torch.no_grad():
            #   torch.embedding_renorm_
            # remove once script supports set_grad_enabled
            _no_grad_embedding_renorm_(weight, input, max_norm, norm_type)
>       return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
E       RuntimeError: Input, output and indices must be on the current device

../mmf_venv/lib/python3.8/site-packages/torch/nn/functional.py:1916: RuntimeError
----------------------------- Captured stderr call -----------------------------
loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.10.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
Some weights of the model checkpoint at bert-base-uncased were not used when initializing VinVLBase: ['bert.pooler.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'bert.pooler.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing VinVLBase from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing VinVLBase from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of VinVLBase were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.img_embedding.0.weight', 'bert.img_embedding.1.weight', 'bert.img_embedding.1.bias', 'bert.img_embedding.0.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
__________________ TestModuleLayers.test_bert_classifier_head __________________

self = <tests.modules.test_layers.TestModuleLayers testMethod=test_bert_classifier_head>

    def test_bert_classifier_head(self):
        config = {}
        config["hidden_size"] = 768
        config["hidden_act"] = "gelu"
        config["layer_norm_eps"] = 1e-12
        config["hidden_dropout_prob"] = 0.1
        config = OmegaConf.create(config)
        clf = layers.ClassifierLayer("bert", 768, 1, config=config)
        self.assertEqual(len(list(clf.module.children())), 3)
        self.assertEqual(len(list(clf.parameters())), 6)
    
        inp = torch.rand(3, 768)
    
        output = clf(inp)
        self.assertEqual(output.size(), torch.Size((3, 1)))
>       np.testing.assert_almost_equal(
            output.squeeze().tolist(), [0.5452202, -0.0437842, -0.377468], decimal=3
        )
E       AssertionError: 
E       Arrays are not almost equal to 3 decimals
E       
E       Mismatched elements: 3 / 3 (100%)
E       Max absolute difference: 0.45450842
E       Max relative difference: 10.38064924
E        x: array([ 0.741,  0.411, -0.254])
E        y: array([ 0.545, -0.044, -0.377])

tests/modules/test_layers.py:112: AssertionError
__________________________ TestModuleLayers.test_mlp ___________________________

self = <tests.modules.test_layers.TestModuleLayers testMethod=test_mlp>

    def test_mlp(self):
        mlp = layers.ClassifierLayer("mlp", in_dim=300, out_dim=1)
        self.assertEqual(len(list(mlp.module.layers.children())), 1)
        self.assertEqual(len(list(mlp.parameters())), 2)
    
        inp = torch.rand(3, 300)
    
        output = mlp(inp)
        self.assertEqual(output.size(), torch.Size((3, 1)))
        np.testing.assert_almost_equal(
            output.squeeze().tolist(), [0.1949174, 0.4030975, -0.0109139]
        )
    
        mlp = layers.ClassifierLayer(
            "mlp", in_dim=300, out_dim=1, hidden_dim=150, num_layers=1
        )
    
        self.assertEqual(len(list(mlp.module.layers.children())), 5)
        self.assertEqual(len(list(mlp.parameters())), 6)
    
        inp = torch.rand(3, 300)
    
        output = mlp(inp)
        self.assertEqual(output.size(), torch.Size((3, 1)))
>       np.testing.assert_almost_equal(
            output.squeeze().tolist(), [-0.503411, 0.1725615, -0.6833304], decimal=3
        )
E       AssertionError: 
E       Arrays are not almost equal to 3 decimals
E       
E       Mismatched elements: 3 / 3 (100%)
E       Max absolute difference: 0.40513362
E       Max relative difference: 1.94583172
E        x: array([-0.637,  0.508, -0.278])
E        y: array([-0.503,  0.173, -0.683])

tests/modules/test_layers.py:93: AssertionError
_________________________ TestEvalLoop.test_eval_loop __________________________

self = <tests.trainers.test_eval_loop.TestEvalLoop testMethod=test_eval_loop>
a = <MagicMock name='get_mmf_env' id='140116956216240'>
b = <MagicMock name='PathManager' id='140116919015936'>

    @patch(
        "mmf.common.test_reporter.PathManager",
        return_value=MagicMock(return_value=None),
    )
    @patch("mmf.common.test_reporter.get_mmf_env", return_value="")
    def test_eval_loop(self, a, b):
        config = get_config_with_defaults(
            {"training": {"max_updates": 2, "max_epochs": 2}}
        )
        trainer = get_mmf_trainer(config=config)
        combined_report, meter = trainer.evaluation_loop("val")
>       self.assertAlmostEqual(combined_report["losses"]["loss"], 493377.5312)
E       TypeError: type Tensor doesn't define __round__ method

tests/trainers/test_eval_loop.py:25: TypeError
----------------------------- Captured stdout call -----------------------------
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
_______________ TestTrainingLoop.test_update_frequency_reporting _______________

self = <tests.trainers.test_training_loop.TestTrainingLoop testMethod=test_update_frequency_reporting>
a = <MagicMock name='PathManager' id='140113654185888'>

    @patch("mmf.common.test_reporter.PathManager", return_value=MagicMock())
    def test_update_frequency_reporting(self, a):
        def _on_update_end(report, meter, should_log):
            # the losses here should be the sum of two losses in
            # iteration 0 and iteration 1 (both constitute update 0).
            # Here iter 1 loss: 0.2599, iter 2 loss: 4.2090
            loss = report.losses["loss"].detach().cpu().item()
            self.assertAlmostEqual(loss, 4.4688, 4)
    
>       self._train_with_condition(
            num_train_data=100,
            max_updates=1,
            max_epochs=None,
            update_frequency=2,
            batch_size=2,
            on_update_end_fn=_on_update_end,
        )

tests/trainers/test_training_loop.py:52: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/trainers/test_training_loop.py:115: in _train_with_condition
    trainer.training_loop()
mmf/trainers/core/training_loop.py:33: in training_loop
    self.run_training_epoch()
mmf/trainers/core/training_loop.py:126: in run_training_epoch
    self.on_update_end(
tests/trainers/test_training_loop.py:50: in _on_update_end
    self.assertAlmostEqual(loss, 4.4688, 4)
E   AssertionError: 4.468684673309326 != 4.4688 within 4 places (0.0001153266906737116 difference)
----------------------------- Captured stdout call -----------------------------
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
__________ TestLightningCheckpoint.test_lightning_checkpoint_interval __________

self = <tests.trainers.lightning.test_checkpoint.TestLightningCheckpoint testMethod=test_lightning_checkpoint_interval>

    def test_lightning_checkpoint_interval(self):
        with mock_env_with_temp("mmf.trainers.lightning_trainer.get_mmf_env") as tmp_d:
            # generate checkpoint, val_check_interval=2, checkpoint_inteval=2
            lightning_gen = self._get_lightning_trainer(max_steps=6)
>           lightning_gen.trainer.fit(
                lightning_gen.model,
                train_dataloaders=lightning_gen.train_loader,
                val_dataloaders=lightning_gen.val_loader,
            )

tests/trainers/lightning/test_checkpoint.py:442: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:771: in fit
    self._call_and_handle_interrupt(
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:724: in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:812: in _fit_impl
    results = self._run(model, ckpt_path=self.ckpt_path)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1236: in _run
    results = self._run_stage()
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1323: in _run_stage
    return self._run_train()
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1353: in _run_train
    self.fit_loop.run()
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/loops/base.py:204: in run
    self.advance(*args, **kwargs)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/loops/fit_loop.py:281: in advance
    self._outputs = self.epoch_loop.run(self._data_fetcher)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/loops/base.py:204: in run
    self.advance(*args, **kwargs)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py:200: in advance
    batch_output = self.batch_loop.run(batch, batch_idx)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/loops/base.py:204: in run
    self.advance(*args, **kwargs)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py:88: in advance
    outputs = self.optimizer_loop.run(split_batch, optimizers, batch_idx)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/loops/base.py:204: in run
    self.advance(*args, **kwargs)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:203: in advance
    result = self._run_optimization(
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:256: in _run_optimization
    self._optimizer_step(optimizer, opt_idx, batch_idx, closure)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:365: in _optimizer_step
    self.trainer._call_lightning_module_hook(
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1595: in _call_lightning_module_hook
    output = fn(*args, **kwargs)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/core/lightning.py:1626: in optimizer_step
    optimizer.step(closure=optimizer_closure)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/core/optimizer.py:168: in step
    step_output = self._strategy.optimizer_step(self._optimizer, self._optimizer_idx, closure, **kwargs)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/strategies/strategy.py:194: in optimizer_step
    return self.precision_plugin.optimizer_step(model, optimizer, opt_idx, closure, **kwargs)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py:153: in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
../mmf_venv/lib/python3.8/site-packages/torch/optim/optimizer.py:89: in wrapper
    return func(*args, **kwargs)
../mmf_venv/lib/python3.8/site-packages/transformers/optimization.py:322: in step
    loss = closure()
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py:138: in _wrap_closure
    closure_result = closure()
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:148: in __call__
    self._result = self.closure(*args, **kwargs)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:134: in closure
    step_output = self._step_fn()
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:422: in _training_step
    training_step_output = self.trainer._call_strategy_hook("training_step", *step_kwargs.values())
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1721: in _call_strategy_hook
    output = fn(*args, **kwargs)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/strategies/strategy.py:334: in training_step
    return self.model.training_step(*args, **kwargs)
mmf/models/base_model.py:234: in training_step
    output = self._forward_lightning_step(batch, batch_idx)
mmf/models/base_model.py:276: in _forward_lightning_step
    output = self(batch)
mmf/models/base_model.py:311: in __call__
    model_output = super().__call__(sample_list, *args, **kwargs)
../mmf_venv/lib/python3.8/site-packages/torch/nn/modules/module.py:889: in _call_impl
    result = self.forward(*input, **kwargs)
tests/test_utils.py:209: in forward
    output = self.classifier(batch)
../mmf_venv/lib/python3.8/site-packages/torch/nn/modules/module.py:889: in _call_impl
    result = self.forward(*input, **kwargs)
../mmf_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py:94: in forward
    return F.linear(input, self.weight, self.bias)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = tensor([[0.]], device='cuda:0')
weight = Parameter containing:
tensor([[0.2294]], requires_grad=True)
bias = Parameter containing:
tensor([-0.2380], requires_grad=True)

    def linear(input: Tensor, weight: Tensor, bias: Optional[Tensor] = None) -> Tensor:
        r"""
        Applies a linear transformation to the incoming data: :math:`y = xA^T + b`.
    
        This operator supports :ref:`TensorFloat32<tf32_on_ampere>`.
    
        Shape:
    
            - Input: :math:`(N, *, in\_features)` N is the batch size, `*` means any number of
              additional dimensions
            - Weight: :math:`(out\_features, in\_features)`
            - Bias: :math:`(out\_features)`
            - Output: :math:`(N, *, out\_features)`
        """
        if has_torch_function_variadic(input, weight):
            return handle_torch_function(linear, (input, weight), input, weight, bias=bias)
>       return torch._C._nn.linear(input, weight, bias)
E       RuntimeError: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#cublasApi_reproducibility

../mmf_venv/lib/python3.8/site-packages/torch/nn/functional.py:1753: RuntimeError
----------------------------- Captured stdout call -----------------------------
[5m[31mWARNING[0m [32m2022-11-15T10:08:55 | mmf: [0mKey config is not present in registry, returning default value of None
[5m[31mWARNING[0m [32m2022-11-15T10:08:55 | mmf: [0mKey config is not present in registry, returning default value of None
[5m[31mWARNING[0m [32m2022-11-15T10:08:55 | mmf: [0mKey config is not present in registry, returning default value of None
[5m[31mWARNING[0m [32m2022-11-15T10:08:55 | mmf: [0mKey config is not present in registry, returning default value of None
[32m2022-11-15T10:08:55 | mmf.trainers.lightning_trainer: [0mLoading metrics
[32m2022-11-15T10:08:55 | mmf.trainers.lightning_trainer: [0mLoading metrics
[32m2022-11-15T10:08:55 | mmf.trainers.lightning_trainer: [0mLoading metrics
[32m2022-11-15T10:08:55 | mmf.trainers.lightning_trainer: [0mLoading metrics
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
----------------------------- Captured stderr call -----------------------------
ModelCheckpoint(save_last=True, save_top_k=-1, monitor=None) will duplicate the last checkpoint saved.
[W Context.cpp:70] Warning: torch.use_deterministic_algorithms is in beta, and its design and functionality may change in the future. (function operator())
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name       | Type   | Params
--------------------------------------
0 | classifier | Linear | 2     
--------------------------------------
2         Trainable params
0         Non-trainable params
2         Total params
0.000     Total estimated model params size (MB)
_________ TestLightningCheckpoint.test_lightning_checkpoint_structure __________

self = <tests.trainers.lightning.test_checkpoint.TestLightningCheckpoint testMethod=test_lightning_checkpoint_structure>

    def test_lightning_checkpoint_structure(self):
        with mock_env_with_temp("mmf.trainers.lightning_trainer.get_mmf_env") as tmp_d:
            lightning = self._get_lightning_trainer()
>           lightning.trainer.fit(
                lightning.model, train_dataloaders=lightning.train_loader
            )

tests/trainers/lightning/test_checkpoint.py:419: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:771: in fit
    self._call_and_handle_interrupt(
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:724: in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:812: in _fit_impl
    results = self._run(model, ckpt_path=self.ckpt_path)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1236: in _run
    results = self._run_stage()
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1323: in _run_stage
    return self._run_train()
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1353: in _run_train
    self.fit_loop.run()
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/loops/base.py:204: in run
    self.advance(*args, **kwargs)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/loops/fit_loop.py:281: in advance
    self._outputs = self.epoch_loop.run(self._data_fetcher)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/loops/base.py:204: in run
    self.advance(*args, **kwargs)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py:200: in advance
    batch_output = self.batch_loop.run(batch, batch_idx)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/loops/base.py:204: in run
    self.advance(*args, **kwargs)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py:88: in advance
    outputs = self.optimizer_loop.run(split_batch, optimizers, batch_idx)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/loops/base.py:204: in run
    self.advance(*args, **kwargs)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:203: in advance
    result = self._run_optimization(
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:256: in _run_optimization
    self._optimizer_step(optimizer, opt_idx, batch_idx, closure)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:365: in _optimizer_step
    self.trainer._call_lightning_module_hook(
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1595: in _call_lightning_module_hook
    output = fn(*args, **kwargs)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/core/lightning.py:1626: in optimizer_step
    optimizer.step(closure=optimizer_closure)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/core/optimizer.py:168: in step
    step_output = self._strategy.optimizer_step(self._optimizer, self._optimizer_idx, closure, **kwargs)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/strategies/strategy.py:194: in optimizer_step
    return self.precision_plugin.optimizer_step(model, optimizer, opt_idx, closure, **kwargs)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py:153: in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
../mmf_venv/lib/python3.8/site-packages/torch/optim/optimizer.py:89: in wrapper
    return func(*args, **kwargs)
../mmf_venv/lib/python3.8/site-packages/transformers/optimization.py:322: in step
    loss = closure()
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py:138: in _wrap_closure
    closure_result = closure()
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:148: in __call__
    self._result = self.closure(*args, **kwargs)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:134: in closure
    step_output = self._step_fn()
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:422: in _training_step
    training_step_output = self.trainer._call_strategy_hook("training_step", *step_kwargs.values())
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1721: in _call_strategy_hook
    output = fn(*args, **kwargs)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/strategies/strategy.py:334: in training_step
    return self.model.training_step(*args, **kwargs)
mmf/models/base_model.py:234: in training_step
    output = self._forward_lightning_step(batch, batch_idx)
mmf/models/base_model.py:276: in _forward_lightning_step
    output = self(batch)
mmf/models/base_model.py:311: in __call__
    model_output = super().__call__(sample_list, *args, **kwargs)
../mmf_venv/lib/python3.8/site-packages/torch/nn/modules/module.py:889: in _call_impl
    result = self.forward(*input, **kwargs)
tests/test_utils.py:209: in forward
    output = self.classifier(batch)
../mmf_venv/lib/python3.8/site-packages/torch/nn/modules/module.py:889: in _call_impl
    result = self.forward(*input, **kwargs)
../mmf_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py:94: in forward
    return F.linear(input, self.weight, self.bias)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = tensor([[0.]], device='cuda:0')
weight = Parameter containing:
tensor([[0.2294]], requires_grad=True)
bias = Parameter containing:
tensor([-0.2380], requires_grad=True)

    def linear(input: Tensor, weight: Tensor, bias: Optional[Tensor] = None) -> Tensor:
        r"""
        Applies a linear transformation to the incoming data: :math:`y = xA^T + b`.
    
        This operator supports :ref:`TensorFloat32<tf32_on_ampere>`.
    
        Shape:
    
            - Input: :math:`(N, *, in\_features)` N is the batch size, `*` means any number of
              additional dimensions
            - Weight: :math:`(out\_features, in\_features)`
            - Bias: :math:`(out\_features)`
            - Output: :math:`(N, *, out\_features)`
        """
        if has_torch_function_variadic(input, weight):
            return handle_torch_function(linear, (input, weight), input, weight, bias=bias)
>       return torch._C._nn.linear(input, weight, bias)
E       RuntimeError: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#cublasApi_reproducibility

../mmf_venv/lib/python3.8/site-packages/torch/nn/functional.py:1753: RuntimeError
----------------------------- Captured stdout call -----------------------------
[32m2022-11-15T10:09:16 | mmf.trainers.lightning_trainer: [0mLoading metrics
[32m2022-11-15T10:09:16 | mmf.trainers.lightning_trainer: [0mLoading metrics
[32m2022-11-15T10:09:16 | mmf.trainers.lightning_trainer: [0mLoading metrics
[32m2022-11-15T10:09:16 | mmf.trainers.lightning_trainer: [0mLoading metrics
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
----------------------------- Captured stderr call -----------------------------
ModelCheckpoint(save_last=True, save_top_k=-1, monitor=None) will duplicate the last checkpoint saved.
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name       | Type   | Params
--------------------------------------
0 | classifier | Linear | 2     
--------------------------------------
2         Trainable params
0         Non-trainable params
2         Total params
0.000     Total estimated model params size (MB)
____ TestLightningCheckpoint.test_load_mmf_trainer_checkpoint_in_lightning _____

self = <tests.trainers.lightning.test_checkpoint.TestLightningCheckpoint testMethod=test_load_mmf_trainer_checkpoint_in_lightning>

    def test_load_mmf_trainer_checkpoint_in_lightning(self):
        # specifying an mmf .ckpt as the trainer resume_from_checkpoint
        # for lightning trainer
        with mock_env_with_temp(
            "mmf.utils.checkpoint.get_mmf_env"
        ) as tmp_d, mock_env_with_temp("mmf.common.test_reporter.get_mmf_env") as _:
            # generate checkpoint
>           self._get_mmf_trainer(max_updates=6).training_loop()

tests/trainers/lightning/test_checkpoint.py:312: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
mmf/trainers/core/training_loop.py:33: in training_loop
    self.run_training_epoch()
mmf/trainers/core/training_loop.py:91: in run_training_epoch
    report = self.run_training_batch(batch, num_batches_for_this_update)
mmf/trainers/core/training_loop.py:166: in run_training_batch
    report = self._forward(batch)
mmf/trainers/core/training_loop.py:200: in _forward
    model_output = self.model(prepared_batch)
mmf/models/base_model.py:311: in __call__
    model_output = super().__call__(sample_list, *args, **kwargs)
../mmf_venv/lib/python3.8/site-packages/torch/nn/modules/module.py:889: in _call_impl
    result = self.forward(*input, **kwargs)
tests/test_utils.py:209: in forward
    output = self.classifier(batch)
../mmf_venv/lib/python3.8/site-packages/torch/nn/modules/module.py:889: in _call_impl
    result = self.forward(*input, **kwargs)
../mmf_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py:94: in forward
    return F.linear(input, self.weight, self.bias)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = tensor([[0.]], device='cuda:0')
weight = Parameter containing:
tensor([[0.2294]], device='cuda:0', requires_grad=True)
bias = Parameter containing:
tensor([-0.2380], device='cuda:0', requires_grad=True)

    def linear(input: Tensor, weight: Tensor, bias: Optional[Tensor] = None) -> Tensor:
        r"""
        Applies a linear transformation to the incoming data: :math:`y = xA^T + b`.
    
        This operator supports :ref:`TensorFloat32<tf32_on_ampere>`.
    
        Shape:
    
            - Input: :math:`(N, *, in\_features)` N is the batch size, `*` means any number of
              additional dimensions
            - Weight: :math:`(out\_features, in\_features)`
            - Bias: :math:`(out\_features)`
            - Output: :math:`(N, *, out\_features)`
        """
        if has_torch_function_variadic(input, weight):
            return handle_torch_function(linear, (input, weight), input, weight, bias=bias)
>       return torch._C._nn.linear(input, weight, bias)
E       RuntimeError: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#cublasApi_reproducibility

../mmf_venv/lib/python3.8/site-packages/torch/nn/functional.py:1753: RuntimeError
----------------------------- Captured stdout call -----------------------------
[32m2022-11-15T10:09:21 | mmf.trainers.mmf_trainer: [0mLoading metrics
[32m2022-11-15T10:09:21 | mmf.trainers.mmf_trainer: [0mLoading metrics
[32m2022-11-15T10:09:21 | mmf.trainers.mmf_trainer: [0mLoading metrics
[32m2022-11-15T10:09:21 | mmf.trainers.mmf_trainer: [0mLoading metrics
[32m2022-11-15T10:09:21 | mmf.trainers.core.training_loop: [0mStarting training...
[32m2022-11-15T10:09:21 | mmf.trainers.core.training_loop: [0mStarting training...
[32m2022-11-15T10:09:21 | mmf.trainers.core.training_loop: [0mStarting training...
[32m2022-11-15T10:09:21 | mmf.trainers.core.training_loop: [0mStarting training...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
________ TestLightningCheckpoint.test_load_resume_best_parity_with_mmf _________

self = <tests.trainers.lightning.test_checkpoint.TestLightningCheckpoint testMethod=test_load_resume_best_parity_with_mmf>

    def test_load_resume_best_parity_with_mmf(self):
        # with checkpoint.resume = True and checkpoint.resume_best = True
        # by default it loads best.ckpt. It should load the "best.ckpt"
>       self._load_checkpoint_and_test(
            "best.ckpt", ckpt_config={"resume": True, "resume_best": True}
        )

tests/trainers/lightning/test_checkpoint.py:207: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/trainers/lightning/test_checkpoint.py:480: in _load_checkpoint_and_test
    mmf_ckpt = self._get_mmf_ckpt(filename, ckpt_config=ckpt_config)
tests/trainers/lightning/test_checkpoint.py:463: in _get_mmf_ckpt
    self._get_mmf_trainer(max_updates=6).training_loop()
mmf/trainers/core/training_loop.py:33: in training_loop
    self.run_training_epoch()
mmf/trainers/core/training_loop.py:91: in run_training_epoch
    report = self.run_training_batch(batch, num_batches_for_this_update)
mmf/trainers/core/training_loop.py:166: in run_training_batch
    report = self._forward(batch)
mmf/trainers/core/training_loop.py:200: in _forward
    model_output = self.model(prepared_batch)
mmf/models/base_model.py:311: in __call__
    model_output = super().__call__(sample_list, *args, **kwargs)
../mmf_venv/lib/python3.8/site-packages/torch/nn/modules/module.py:889: in _call_impl
    result = self.forward(*input, **kwargs)
tests/test_utils.py:209: in forward
    output = self.classifier(batch)
../mmf_venv/lib/python3.8/site-packages/torch/nn/modules/module.py:889: in _call_impl
    result = self.forward(*input, **kwargs)
../mmf_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py:94: in forward
    return F.linear(input, self.weight, self.bias)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = tensor([[0.]], device='cuda:0')
weight = Parameter containing:
tensor([[0.2294]], device='cuda:0', requires_grad=True)
bias = Parameter containing:
tensor([-0.2380], device='cuda:0', requires_grad=True)

    def linear(input: Tensor, weight: Tensor, bias: Optional[Tensor] = None) -> Tensor:
        r"""
        Applies a linear transformation to the incoming data: :math:`y = xA^T + b`.
    
        This operator supports :ref:`TensorFloat32<tf32_on_ampere>`.
    
        Shape:
    
            - Input: :math:`(N, *, in\_features)` N is the batch size, `*` means any number of
              additional dimensions
            - Weight: :math:`(out\_features, in\_features)`
            - Bias: :math:`(out\_features)`
            - Output: :math:`(N, *, out\_features)`
        """
        if has_torch_function_variadic(input, weight):
            return handle_torch_function(linear, (input, weight), input, weight, bias=bias)
>       return torch._C._nn.linear(input, weight, bias)
E       RuntimeError: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#cublasApi_reproducibility

../mmf_venv/lib/python3.8/site-packages/torch/nn/functional.py:1753: RuntimeError
----------------------------- Captured stdout call -----------------------------
[32m2022-11-15T10:09:27 | mmf.trainers.mmf_trainer: [0mLoading metrics
[32m2022-11-15T10:09:27 | mmf.trainers.mmf_trainer: [0mLoading metrics
[32m2022-11-15T10:09:27 | mmf.trainers.mmf_trainer: [0mLoading metrics
[32m2022-11-15T10:09:27 | mmf.trainers.mmf_trainer: [0mLoading metrics
[32m2022-11-15T10:09:27 | mmf.trainers.core.training_loop: [0mStarting training...
[32m2022-11-15T10:09:27 | mmf.trainers.core.training_loop: [0mStarting training...
[32m2022-11-15T10:09:27 | mmf.trainers.core.training_loop: [0mStarting training...
[32m2022-11-15T10:09:27 | mmf.trainers.core.training_loop: [0mStarting training...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
__________ TestLightningCheckpoint.test_load_resume_ignore_resume_zoo __________

self = <tests.trainers.lightning.test_checkpoint.TestLightningCheckpoint testMethod=test_load_resume_ignore_resume_zoo>

    def test_load_resume_ignore_resume_zoo(self):
        # specifying both checkpoint.resume = True and resume_zoo
        # resume zoo should be ignored. It should load the "current.ckpt"
>       self._load_checkpoint_and_test(
            "current.ckpt",
            ckpt_config={"resume": True, "resume_zoo": "visual_bert.pretrained.coco"},
        )

tests/trainers/lightning/test_checkpoint.py:214: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/trainers/lightning/test_checkpoint.py:480: in _load_checkpoint_and_test
    mmf_ckpt = self._get_mmf_ckpt(filename, ckpt_config=ckpt_config)
tests/trainers/lightning/test_checkpoint.py:463: in _get_mmf_ckpt
    self._get_mmf_trainer(max_updates=6).training_loop()
mmf/trainers/core/training_loop.py:33: in training_loop
    self.run_training_epoch()
mmf/trainers/core/training_loop.py:91: in run_training_epoch
    report = self.run_training_batch(batch, num_batches_for_this_update)
mmf/trainers/core/training_loop.py:166: in run_training_batch
    report = self._forward(batch)
mmf/trainers/core/training_loop.py:200: in _forward
    model_output = self.model(prepared_batch)
mmf/models/base_model.py:311: in __call__
    model_output = super().__call__(sample_list, *args, **kwargs)
../mmf_venv/lib/python3.8/site-packages/torch/nn/modules/module.py:889: in _call_impl
    result = self.forward(*input, **kwargs)
tests/test_utils.py:209: in forward
    output = self.classifier(batch)
../mmf_venv/lib/python3.8/site-packages/torch/nn/modules/module.py:889: in _call_impl
    result = self.forward(*input, **kwargs)
../mmf_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py:94: in forward
    return F.linear(input, self.weight, self.bias)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = tensor([[0.]], device='cuda:0')
weight = Parameter containing:
tensor([[0.2294]], device='cuda:0', requires_grad=True)
bias = Parameter containing:
tensor([-0.2380], device='cuda:0', requires_grad=True)

    def linear(input: Tensor, weight: Tensor, bias: Optional[Tensor] = None) -> Tensor:
        r"""
        Applies a linear transformation to the incoming data: :math:`y = xA^T + b`.
    
        This operator supports :ref:`TensorFloat32<tf32_on_ampere>`.
    
        Shape:
    
            - Input: :math:`(N, *, in\_features)` N is the batch size, `*` means any number of
              additional dimensions
            - Weight: :math:`(out\_features, in\_features)`
            - Bias: :math:`(out\_features)`
            - Output: :math:`(N, *, out\_features)`
        """
        if has_torch_function_variadic(input, weight):
            return handle_torch_function(linear, (input, weight), input, weight, bias=bias)
>       return torch._C._nn.linear(input, weight, bias)
E       RuntimeError: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#cublasApi_reproducibility

../mmf_venv/lib/python3.8/site-packages/torch/nn/functional.py:1753: RuntimeError
----------------------------- Captured stdout call -----------------------------
[32m2022-11-15T10:09:27 | mmf.trainers.mmf_trainer: [0mLoading metrics
[32m2022-11-15T10:09:27 | mmf.trainers.mmf_trainer: [0mLoading metrics
[32m2022-11-15T10:09:27 | mmf.trainers.mmf_trainer: [0mLoading metrics
[32m2022-11-15T10:09:27 | mmf.trainers.mmf_trainer: [0mLoading metrics
[32m2022-11-15T10:09:27 | mmf.trainers.core.training_loop: [0mStarting training...
[32m2022-11-15T10:09:27 | mmf.trainers.core.training_loop: [0mStarting training...
[32m2022-11-15T10:09:27 | mmf.trainers.core.training_loop: [0mStarting training...
[32m2022-11-15T10:09:27 | mmf.trainers.core.training_loop: [0mStarting training...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
___________ TestLightningCheckpoint.test_load_resume_parity_with_mmf ___________

self = <tests.trainers.lightning.test_checkpoint.TestLightningCheckpoint testMethod=test_load_resume_parity_with_mmf>

    def test_load_resume_parity_with_mmf(self):
        # with checkpoint.resume = True, by default it loads "current.ckpt"
>       self._load_checkpoint_and_test("current.ckpt", ckpt_config={"resume": True})

tests/trainers/lightning/test_checkpoint.py:202: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/trainers/lightning/test_checkpoint.py:480: in _load_checkpoint_and_test
    mmf_ckpt = self._get_mmf_ckpt(filename, ckpt_config=ckpt_config)
tests/trainers/lightning/test_checkpoint.py:463: in _get_mmf_ckpt
    self._get_mmf_trainer(max_updates=6).training_loop()
mmf/trainers/core/training_loop.py:33: in training_loop
    self.run_training_epoch()
mmf/trainers/core/training_loop.py:91: in run_training_epoch
    report = self.run_training_batch(batch, num_batches_for_this_update)
mmf/trainers/core/training_loop.py:166: in run_training_batch
    report = self._forward(batch)
mmf/trainers/core/training_loop.py:200: in _forward
    model_output = self.model(prepared_batch)
mmf/models/base_model.py:311: in __call__
    model_output = super().__call__(sample_list, *args, **kwargs)
../mmf_venv/lib/python3.8/site-packages/torch/nn/modules/module.py:889: in _call_impl
    result = self.forward(*input, **kwargs)
tests/test_utils.py:209: in forward
    output = self.classifier(batch)
../mmf_venv/lib/python3.8/site-packages/torch/nn/modules/module.py:889: in _call_impl
    result = self.forward(*input, **kwargs)
../mmf_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py:94: in forward
    return F.linear(input, self.weight, self.bias)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = tensor([[0.]], device='cuda:0')
weight = Parameter containing:
tensor([[0.2294]], device='cuda:0', requires_grad=True)
bias = Parameter containing:
tensor([-0.2380], device='cuda:0', requires_grad=True)

    def linear(input: Tensor, weight: Tensor, bias: Optional[Tensor] = None) -> Tensor:
        r"""
        Applies a linear transformation to the incoming data: :math:`y = xA^T + b`.
    
        This operator supports :ref:`TensorFloat32<tf32_on_ampere>`.
    
        Shape:
    
            - Input: :math:`(N, *, in\_features)` N is the batch size, `*` means any number of
              additional dimensions
            - Weight: :math:`(out\_features, in\_features)`
            - Bias: :math:`(out\_features)`
            - Output: :math:`(N, *, out\_features)`
        """
        if has_torch_function_variadic(input, weight):
            return handle_torch_function(linear, (input, weight), input, weight, bias=bias)
>       return torch._C._nn.linear(input, weight, bias)
E       RuntimeError: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#cublasApi_reproducibility

../mmf_venv/lib/python3.8/site-packages/torch/nn/functional.py:1753: RuntimeError
----------------------------- Captured stdout call -----------------------------
[32m2022-11-15T10:09:27 | mmf.trainers.mmf_trainer: [0mLoading metrics
[32m2022-11-15T10:09:27 | mmf.trainers.mmf_trainer: [0mLoading metrics
[32m2022-11-15T10:09:27 | mmf.trainers.mmf_trainer: [0mLoading metrics
[32m2022-11-15T10:09:27 | mmf.trainers.mmf_trainer: [0mLoading metrics
[32m2022-11-15T10:09:27 | mmf.trainers.core.training_loop: [0mStarting training...
[32m2022-11-15T10:09:27 | mmf.trainers.core.training_loop: [0mStarting training...
[32m2022-11-15T10:09:27 | mmf.trainers.core.training_loop: [0mStarting training...
[32m2022-11-15T10:09:27 | mmf.trainers.core.training_loop: [0mStarting training...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
________ TestLightningCheckpoint.test_load_trainer_ckpt_number_of_steps ________

self = <tests.trainers.lightning.test_checkpoint.TestLightningCheckpoint testMethod=test_load_trainer_ckpt_number_of_steps>

    def test_load_trainer_ckpt_number_of_steps(self):
        with mock_env_with_temp("mmf.trainers.lightning_trainer.get_mmf_env") as tmp_d:
            # to generate ckpt file, max_steps is saved as 6
            lightning_gen = self._get_lightning_trainer(max_steps=6)
>           lightning_gen.trainer.fit(
                lightning_gen.model,
                train_dataloaders=lightning_gen.train_loader,
                val_dataloaders=lightning_gen.val_loader,
            )

tests/trainers/lightning/test_checkpoint.py:371: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:771: in fit
    self._call_and_handle_interrupt(
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:724: in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:812: in _fit_impl
    results = self._run(model, ckpt_path=self.ckpt_path)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1236: in _run
    results = self._run_stage()
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1323: in _run_stage
    return self._run_train()
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1353: in _run_train
    self.fit_loop.run()
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/loops/base.py:204: in run
    self.advance(*args, **kwargs)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/loops/fit_loop.py:281: in advance
    self._outputs = self.epoch_loop.run(self._data_fetcher)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/loops/base.py:204: in run
    self.advance(*args, **kwargs)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py:200: in advance
    batch_output = self.batch_loop.run(batch, batch_idx)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/loops/base.py:204: in run
    self.advance(*args, **kwargs)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py:88: in advance
    outputs = self.optimizer_loop.run(split_batch, optimizers, batch_idx)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/loops/base.py:204: in run
    self.advance(*args, **kwargs)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:203: in advance
    result = self._run_optimization(
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:256: in _run_optimization
    self._optimizer_step(optimizer, opt_idx, batch_idx, closure)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:365: in _optimizer_step
    self.trainer._call_lightning_module_hook(
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1595: in _call_lightning_module_hook
    output = fn(*args, **kwargs)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/core/lightning.py:1626: in optimizer_step
    optimizer.step(closure=optimizer_closure)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/core/optimizer.py:168: in step
    step_output = self._strategy.optimizer_step(self._optimizer, self._optimizer_idx, closure, **kwargs)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/strategies/strategy.py:194: in optimizer_step
    return self.precision_plugin.optimizer_step(model, optimizer, opt_idx, closure, **kwargs)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py:153: in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
../mmf_venv/lib/python3.8/site-packages/torch/optim/optimizer.py:89: in wrapper
    return func(*args, **kwargs)
../mmf_venv/lib/python3.8/site-packages/transformers/optimization.py:322: in step
    loss = closure()
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py:138: in _wrap_closure
    closure_result = closure()
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:148: in __call__
    self._result = self.closure(*args, **kwargs)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:134: in closure
    step_output = self._step_fn()
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:422: in _training_step
    training_step_output = self.trainer._call_strategy_hook("training_step", *step_kwargs.values())
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1721: in _call_strategy_hook
    output = fn(*args, **kwargs)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/strategies/strategy.py:334: in training_step
    return self.model.training_step(*args, **kwargs)
mmf/models/base_model.py:234: in training_step
    output = self._forward_lightning_step(batch, batch_idx)
mmf/models/base_model.py:276: in _forward_lightning_step
    output = self(batch)
mmf/models/base_model.py:311: in __call__
    model_output = super().__call__(sample_list, *args, **kwargs)
../mmf_venv/lib/python3.8/site-packages/torch/nn/modules/module.py:889: in _call_impl
    result = self.forward(*input, **kwargs)
tests/test_utils.py:209: in forward
    output = self.classifier(batch)
../mmf_venv/lib/python3.8/site-packages/torch/nn/modules/module.py:889: in _call_impl
    result = self.forward(*input, **kwargs)
../mmf_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py:94: in forward
    return F.linear(input, self.weight, self.bias)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = tensor([[0.]], device='cuda:0')
weight = Parameter containing:
tensor([[0.2294]], requires_grad=True)
bias = Parameter containing:
tensor([-0.2380], requires_grad=True)

    def linear(input: Tensor, weight: Tensor, bias: Optional[Tensor] = None) -> Tensor:
        r"""
        Applies a linear transformation to the incoming data: :math:`y = xA^T + b`.
    
        This operator supports :ref:`TensorFloat32<tf32_on_ampere>`.
    
        Shape:
    
            - Input: :math:`(N, *, in\_features)` N is the batch size, `*` means any number of
              additional dimensions
            - Weight: :math:`(out\_features, in\_features)`
            - Bias: :math:`(out\_features)`
            - Output: :math:`(N, *, out\_features)`
        """
        if has_torch_function_variadic(input, weight):
            return handle_torch_function(linear, (input, weight), input, weight, bias=bias)
>       return torch._C._nn.linear(input, weight, bias)
E       RuntimeError: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#cublasApi_reproducibility

../mmf_venv/lib/python3.8/site-packages/torch/nn/functional.py:1753: RuntimeError
----------------------------- Captured stdout call -----------------------------
[32m2022-11-15T10:09:58 | mmf.trainers.lightning_trainer: [0mLoading metrics
[32m2022-11-15T10:09:58 | mmf.trainers.lightning_trainer: [0mLoading metrics
[32m2022-11-15T10:09:58 | mmf.trainers.lightning_trainer: [0mLoading metrics
[32m2022-11-15T10:09:58 | mmf.trainers.lightning_trainer: [0mLoading metrics
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
----------------------------- Captured stderr call -----------------------------
ModelCheckpoint(save_last=True, save_top_k=-1, monitor=None) will duplicate the last checkpoint saved.
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name       | Type   | Params
--------------------------------------
0 | classifier | Linear | 2     
--------------------------------------
2         Trainable params
0         Non-trainable params
2         Total params
0.000     Total estimated model params size (MB)
_______ TestLightningCheckpoint.test_load_trainer_resume_parity_with_mmf _______

self = <tests.trainers.lightning.test_checkpoint.TestLightningCheckpoint testMethod=test_load_trainer_resume_parity_with_mmf>

    def test_load_trainer_resume_parity_with_mmf(self):
        # directly setting lightning's trainer param: resume_from_checkpoint
        filename = "current.ckpt"
>       mmf_ckpt_current = self._get_mmf_ckpt(filename, ckpt_config={"resume": True})

tests/trainers/lightning/test_checkpoint.py:343: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/trainers/lightning/test_checkpoint.py:463: in _get_mmf_ckpt
    self._get_mmf_trainer(max_updates=6).training_loop()
mmf/trainers/core/training_loop.py:33: in training_loop
    self.run_training_epoch()
mmf/trainers/core/training_loop.py:91: in run_training_epoch
    report = self.run_training_batch(batch, num_batches_for_this_update)
mmf/trainers/core/training_loop.py:166: in run_training_batch
    report = self._forward(batch)
mmf/trainers/core/training_loop.py:200: in _forward
    model_output = self.model(prepared_batch)
mmf/models/base_model.py:311: in __call__
    model_output = super().__call__(sample_list, *args, **kwargs)
../mmf_venv/lib/python3.8/site-packages/torch/nn/modules/module.py:889: in _call_impl
    result = self.forward(*input, **kwargs)
tests/test_utils.py:209: in forward
    output = self.classifier(batch)
../mmf_venv/lib/python3.8/site-packages/torch/nn/modules/module.py:889: in _call_impl
    result = self.forward(*input, **kwargs)
../mmf_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py:94: in forward
    return F.linear(input, self.weight, self.bias)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = tensor([[0.]], device='cuda:0')
weight = Parameter containing:
tensor([[0.2294]], device='cuda:0', requires_grad=True)
bias = Parameter containing:
tensor([-0.2380], device='cuda:0', requires_grad=True)

    def linear(input: Tensor, weight: Tensor, bias: Optional[Tensor] = None) -> Tensor:
        r"""
        Applies a linear transformation to the incoming data: :math:`y = xA^T + b`.
    
        This operator supports :ref:`TensorFloat32<tf32_on_ampere>`.
    
        Shape:
    
            - Input: :math:`(N, *, in\_features)` N is the batch size, `*` means any number of
              additional dimensions
            - Weight: :math:`(out\_features, in\_features)`
            - Bias: :math:`(out\_features)`
            - Output: :math:`(N, *, out\_features)`
        """
        if has_torch_function_variadic(input, weight):
            return handle_torch_function(linear, (input, weight), input, weight, bias=bias)
>       return torch._C._nn.linear(input, weight, bias)
E       RuntimeError: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#cublasApi_reproducibility

../mmf_venv/lib/python3.8/site-packages/torch/nn/functional.py:1753: RuntimeError
----------------------------- Captured stdout call -----------------------------
[32m2022-11-15T10:09:58 | mmf.trainers.mmf_trainer: [0mLoading metrics
[32m2022-11-15T10:09:58 | mmf.trainers.mmf_trainer: [0mLoading metrics
[32m2022-11-15T10:09:58 | mmf.trainers.mmf_trainer: [0mLoading metrics
[32m2022-11-15T10:09:58 | mmf.trainers.mmf_trainer: [0mLoading metrics
[32m2022-11-15T10:09:58 | mmf.trainers.core.training_loop: [0mStarting training...
[32m2022-11-15T10:09:58 | mmf.trainers.core.training_loop: [0mStarting training...
[32m2022-11-15T10:09:58 | mmf.trainers.core.training_loop: [0mStarting training...
[32m2022-11-15T10:09:58 | mmf.trainers.core.training_loop: [0mStarting training...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
______ TestLightningCheckpoint.test_trainer_save_current_parity_with_mmf _______

self = <tests.trainers.lightning.test_checkpoint.TestLightningCheckpoint testMethod=test_trainer_save_current_parity_with_mmf>

    def test_trainer_save_current_parity_with_mmf(self):
        with mock_env_with_temp(
            "mmf.utils.checkpoint.get_mmf_env"
        ) as tmp_d, mock_env_with_temp("mmf.common.test_reporter.get_mmf_env") as _:
            mmf_trainer = self._get_mmf_trainer()
>           mmf_trainer.training_loop()

tests/trainers/lightning/test_checkpoint.py:402: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
mmf/trainers/core/training_loop.py:33: in training_loop
    self.run_training_epoch()
mmf/trainers/core/training_loop.py:91: in run_training_epoch
    report = self.run_training_batch(batch, num_batches_for_this_update)
mmf/trainers/core/training_loop.py:166: in run_training_batch
    report = self._forward(batch)
mmf/trainers/core/training_loop.py:200: in _forward
    model_output = self.model(prepared_batch)
mmf/models/base_model.py:311: in __call__
    model_output = super().__call__(sample_list, *args, **kwargs)
../mmf_venv/lib/python3.8/site-packages/torch/nn/modules/module.py:889: in _call_impl
    result = self.forward(*input, **kwargs)
tests/test_utils.py:209: in forward
    output = self.classifier(batch)
../mmf_venv/lib/python3.8/site-packages/torch/nn/modules/module.py:889: in _call_impl
    result = self.forward(*input, **kwargs)
../mmf_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py:94: in forward
    return F.linear(input, self.weight, self.bias)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = tensor([[0.]], device='cuda:0')
weight = Parameter containing:
tensor([[0.2294]], device='cuda:0', requires_grad=True)
bias = Parameter containing:
tensor([-0.2380], device='cuda:0', requires_grad=True)

    def linear(input: Tensor, weight: Tensor, bias: Optional[Tensor] = None) -> Tensor:
        r"""
        Applies a linear transformation to the incoming data: :math:`y = xA^T + b`.
    
        This operator supports :ref:`TensorFloat32<tf32_on_ampere>`.
    
        Shape:
    
            - Input: :math:`(N, *, in\_features)` N is the batch size, `*` means any number of
              additional dimensions
            - Weight: :math:`(out\_features, in\_features)`
            - Bias: :math:`(out\_features)`
            - Output: :math:`(N, *, out\_features)`
        """
        if has_torch_function_variadic(input, weight):
            return handle_torch_function(linear, (input, weight), input, weight, bias=bias)
>       return torch._C._nn.linear(input, weight, bias)
E       RuntimeError: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#cublasApi_reproducibility

../mmf_venv/lib/python3.8/site-packages/torch/nn/functional.py:1753: RuntimeError
----------------------------- Captured stdout call -----------------------------
[32m2022-11-15T10:10:10 | mmf.trainers.mmf_trainer: [0mLoading metrics
[32m2022-11-15T10:10:10 | mmf.trainers.mmf_trainer: [0mLoading metrics
[32m2022-11-15T10:10:10 | mmf.trainers.mmf_trainer: [0mLoading metrics
[32m2022-11-15T10:10:10 | mmf.trainers.mmf_trainer: [0mLoading metrics
[32m2022-11-15T10:10:10 | mmf.trainers.core.training_loop: [0mStarting training...
[32m2022-11-15T10:10:10 | mmf.trainers.core.training_loop: [0mStarting training...
[32m2022-11-15T10:10:10 | mmf.trainers.core.training_loop: [0mStarting training...
[32m2022-11-15T10:10:10 | mmf.trainers.core.training_loop: [0mStarting training...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
___________ TestLightningTrainerGradAccumulate.test_grad_accumulate ____________

self = <tests.trainers.lightning.test_grad_accumulate.TestLightningTrainerGradAccumulate testMethod=test_grad_accumulate>

    def test_grad_accumulate(self):
        with patch("mmf.trainers.lightning_trainer.get_mmf_env", return_value=""):
            config = self._get_config(
                accumulate_grad_batches=2, max_steps=2, batch_size=3
            )
            trainer1 = get_lightning_trainer(config=config)
>           trainer1.trainer.fit(trainer1.model, trainer1.data_module.train_loader)

tests/trainers/lightning/test_grad_accumulate.py:17: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:771: in fit
    self._call_and_handle_interrupt(
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:724: in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:812: in _fit_impl
    results = self._run(model, ckpt_path=self.ckpt_path)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1236: in _run
    results = self._run_stage()
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1323: in _run_stage
    return self._run_train()
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1353: in _run_train
    self.fit_loop.run()
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/loops/base.py:204: in run
    self.advance(*args, **kwargs)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/loops/fit_loop.py:281: in advance
    self._outputs = self.epoch_loop.run(self._data_fetcher)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/loops/base.py:204: in run
    self.advance(*args, **kwargs)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py:200: in advance
    batch_output = self.batch_loop.run(batch, batch_idx)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/loops/base.py:204: in run
    self.advance(*args, **kwargs)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py:88: in advance
    outputs = self.optimizer_loop.run(split_batch, optimizers, batch_idx)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/loops/base.py:204: in run
    self.advance(*args, **kwargs)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:203: in advance
    result = self._run_optimization(
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:249: in _run_optimization
    closure()
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:148: in __call__
    self._result = self.closure(*args, **kwargs)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:134: in closure
    step_output = self._step_fn()
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:422: in _training_step
    training_step_output = self.trainer._call_strategy_hook("training_step", *step_kwargs.values())
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1721: in _call_strategy_hook
    output = fn(*args, **kwargs)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/strategies/strategy.py:334: in training_step
    return self.model.training_step(*args, **kwargs)
mmf/models/base_model.py:234: in training_step
    output = self._forward_lightning_step(batch, batch_idx)
mmf/models/base_model.py:276: in _forward_lightning_step
    output = self(batch)
mmf/models/base_model.py:311: in __call__
    model_output = super().__call__(sample_list, *args, **kwargs)
../mmf_venv/lib/python3.8/site-packages/torch/nn/modules/module.py:889: in _call_impl
    result = self.forward(*input, **kwargs)
tests/test_utils.py:209: in forward
    output = self.classifier(batch)
../mmf_venv/lib/python3.8/site-packages/torch/nn/modules/module.py:889: in _call_impl
    result = self.forward(*input, **kwargs)
../mmf_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py:94: in forward
    return F.linear(input, self.weight, self.bias)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = tensor([[0.],
        [1.],
        [2.]], device='cuda:0')
weight = Parameter containing:
tensor([[0.2294]], requires_grad=True)
bias = Parameter containing:
tensor([-0.2380], requires_grad=True)

    def linear(input: Tensor, weight: Tensor, bias: Optional[Tensor] = None) -> Tensor:
        r"""
        Applies a linear transformation to the incoming data: :math:`y = xA^T + b`.
    
        This operator supports :ref:`TensorFloat32<tf32_on_ampere>`.
    
        Shape:
    
            - Input: :math:`(N, *, in\_features)` N is the batch size, `*` means any number of
              additional dimensions
            - Weight: :math:`(out\_features, in\_features)`
            - Bias: :math:`(out\_features)`
            - Output: :math:`(N, *, out\_features)`
        """
        if has_torch_function_variadic(input, weight):
            return handle_torch_function(linear, (input, weight), input, weight, bias=bias)
>       return torch._C._nn.linear(input, weight, bias)
E       RuntimeError: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#cublasApi_reproducibility

../mmf_venv/lib/python3.8/site-packages/torch/nn/functional.py:1753: RuntimeError
----------------------------- Captured stdout call -----------------------------
[32m2022-11-15T10:10:10 | mmf.trainers.lightning_trainer: [0mLoading metrics
[32m2022-11-15T10:10:10 | mmf.trainers.lightning_trainer: [0mLoading metrics
[32m2022-11-15T10:10:10 | mmf.trainers.lightning_trainer: [0mLoading metrics
[32m2022-11-15T10:10:10 | mmf.trainers.lightning_trainer: [0mLoading metrics
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
----------------------------- Captured stderr call -----------------------------
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name       | Type   | Params
--------------------------------------
0 | classifier | Linear | 2     
--------------------------------------
2         Trainable params
0         Non-trainable params
2         Total params
0.000     Total estimated model params size (MB)
____ TestLightningTrainerGradClipping.test_grad_clipping_and_parity_to_mmf _____

self = <tests.trainers.lightning.test_grad_clipping.TestLightningTrainerGradClipping testMethod=test_grad_clipping_and_parity_to_mmf>

    def test_grad_clipping_and_parity_to_mmf(self):
        config = self._get_mmf_config(
            max_updates=5,
            max_epochs=None,
            max_grad_l2_norm=self.grad_clip_magnitude,
            clip_norm_mode="all",
        )
        mmf_trainer = get_mmf_trainer(config=config)
        mmf_trainer.evaluation_loop = MagicMock(return_value=(None, None))
    
        def _finish_update():
            clip_gradients(
                mmf_trainer.model,
                mmf_trainer.optimizer,
                mmf_trainer.num_updates,
                None,
                mmf_trainer.config,
            )
            for param in mmf_trainer.model.parameters():
                mmf_grad = torch.clone(param.grad).detach().item()
                self.mmf_grads.append(mmf_grad)
    
            mmf_trainer.scaler.step(mmf_trainer.optimizer)
            mmf_trainer.scaler.update()
            mmf_trainer.num_updates += 1
    
        mmf_trainer._finish_update = _finish_update
>       mmf_trainer.training_loop()

tests/trainers/lightning/test_grad_clipping.py:49: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
mmf/trainers/core/training_loop.py:33: in training_loop
    self.run_training_epoch()
mmf/trainers/core/training_loop.py:91: in run_training_epoch
    report = self.run_training_batch(batch, num_batches_for_this_update)
mmf/trainers/core/training_loop.py:166: in run_training_batch
    report = self._forward(batch)
mmf/trainers/core/training_loop.py:200: in _forward
    model_output = self.model(prepared_batch)
mmf/models/base_model.py:311: in __call__
    model_output = super().__call__(sample_list, *args, **kwargs)
../mmf_venv/lib/python3.8/site-packages/torch/nn/modules/module.py:889: in _call_impl
    result = self.forward(*input, **kwargs)
tests/test_utils.py:209: in forward
    output = self.classifier(batch)
../mmf_venv/lib/python3.8/site-packages/torch/nn/modules/module.py:889: in _call_impl
    result = self.forward(*input, **kwargs)
../mmf_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py:94: in forward
    return F.linear(input, self.weight, self.bias)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = tensor([[0.]], device='cuda:0')
weight = Parameter containing:
tensor([[0.2294]], device='cuda:0', requires_grad=True)
bias = Parameter containing:
tensor([-0.2380], device='cuda:0', requires_grad=True)

    def linear(input: Tensor, weight: Tensor, bias: Optional[Tensor] = None) -> Tensor:
        r"""
        Applies a linear transformation to the incoming data: :math:`y = xA^T + b`.
    
        This operator supports :ref:`TensorFloat32<tf32_on_ampere>`.
    
        Shape:
    
            - Input: :math:`(N, *, in\_features)` N is the batch size, `*` means any number of
              additional dimensions
            - Weight: :math:`(out\_features, in\_features)`
            - Bias: :math:`(out\_features)`
            - Output: :math:`(N, *, out\_features)`
        """
        if has_torch_function_variadic(input, weight):
            return handle_torch_function(linear, (input, weight), input, weight, bias=bias)
>       return torch._C._nn.linear(input, weight, bias)
E       RuntimeError: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#cublasApi_reproducibility

../mmf_venv/lib/python3.8/site-packages/torch/nn/functional.py:1753: RuntimeError
----------------------------- Captured stdout call -----------------------------
[32m2022-11-15T10:10:11 | mmf.trainers.core.training_loop: [0mStarting training...
[32m2022-11-15T10:10:11 | mmf.trainers.core.training_loop: [0mStarting training...
[32m2022-11-15T10:10:11 | mmf.trainers.core.training_loop: [0mStarting training...
[32m2022-11-15T10:10:11 | mmf.trainers.core.training_loop: [0mStarting training...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
_________ TestLightningTrainerLogging.test_tensorboard_logging_parity __________

self = <tests.trainers.lightning.test_logging.TestLightningTrainerLogging testMethod=test_tensorboard_logging_parity>
summary_writer = <MagicMock name='get_mmf_env' id='140113651170272'>
mmf = <MagicMock name='get_mmf_env' id='140113647050560'>
lightning = <MagicMock name='get_mmf_env' id='140113650638272'>
logistics = <MagicMock name='SummaryWriter' id='140113650127968'>
logistics_logs = <MagicMock name='setup_output_folder' id='140113653977440'>
report_logs = <MagicMock name='setup_output_folder' id='140113647494528'>
trainer_logs = <MagicMock name='setup_output_folder' id='140113651491648'>
mkdirs = <MagicMock name='mkdirs' id='140113649309296'>

    @patch("mmf.common.test_reporter.PathManager.mkdirs")
    @patch("mmf.trainers.callbacks.logistics.setup_output_folder", return_value="logs")
    @patch("mmf.trainers.lightning_trainer.setup_output_folder", return_value="logs")
    @patch("mmf.utils.logger.setup_output_folder", return_value="logs")
    @patch("torch.utils.tensorboard.SummaryWriter")
    @patch("mmf.trainers.callbacks.logistics.get_mmf_env", return_value="logs")
    @patch("mmf.common.test_reporter.get_mmf_env", return_value="logs")
    @patch("mmf.trainers.lightning_trainer.get_mmf_env", return_value="logs")
    def test_tensorboard_logging_parity(
        self,
        summary_writer,
        mmf,
        lightning,
        logistics,
        logistics_logs,
        report_logs,
        trainer_logs,
        mkdirs,
    ):
        # mmf trainer
        config = self._get_mmf_config(
            max_updates=8,
            batch_size=2,
            max_epochs=None,
            log_interval=3,
            evaluation_interval=9,
            tensorboard=True,
        )
        mmf_trainer = get_mmf_trainer(config=config)
    
        def _add_scalars_mmf(log_dict, iteration):
            self.mmf_tensorboard_logs.append({iteration: log_dict})
    
        mmf_trainer.load_metrics()
        logistics_callback = LogisticsCallback(mmf_trainer.config, mmf_trainer)
        logistics_callback.snapshot_timer = MagicMock(return_value=None)
        logistics_callback.train_timer = Timer()
        logistics_callback.tb_writer.add_scalars = _add_scalars_mmf
        mmf_trainer.logistics_callback = logistics_callback
        mmf_trainer.on_validation_end = logistics_callback.on_validation_end
        mmf_trainer.callbacks = [logistics_callback]
        mmf_trainer.early_stop_callback = MagicMock(return_value=None)
        mmf_trainer.on_update_end = logistics_callback.on_update_end
>       mmf_trainer.training_loop()

tests/trainers/lightning/test_logging.py:65: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
mmf/trainers/core/training_loop.py:33: in training_loop
    self.run_training_epoch()
mmf/trainers/core/training_loop.py:91: in run_training_epoch
    report = self.run_training_batch(batch, num_batches_for_this_update)
mmf/trainers/core/training_loop.py:166: in run_training_batch
    report = self._forward(batch)
mmf/trainers/core/training_loop.py:200: in _forward
    model_output = self.model(prepared_batch)
mmf/models/base_model.py:311: in __call__
    model_output = super().__call__(sample_list, *args, **kwargs)
../mmf_venv/lib/python3.8/site-packages/torch/nn/modules/module.py:889: in _call_impl
    result = self.forward(*input, **kwargs)
tests/test_utils.py:209: in forward
    output = self.classifier(batch)
../mmf_venv/lib/python3.8/site-packages/torch/nn/modules/module.py:889: in _call_impl
    result = self.forward(*input, **kwargs)
../mmf_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py:94: in forward
    return F.linear(input, self.weight, self.bias)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = tensor([[0.],
        [1.]], device='cuda:0')
weight = Parameter containing:
tensor([[0.2294]], device='cuda:0', requires_grad=True)
bias = Parameter containing:
tensor([-0.2380], device='cuda:0', requires_grad=True)

    def linear(input: Tensor, weight: Tensor, bias: Optional[Tensor] = None) -> Tensor:
        r"""
        Applies a linear transformation to the incoming data: :math:`y = xA^T + b`.
    
        This operator supports :ref:`TensorFloat32<tf32_on_ampere>`.
    
        Shape:
    
            - Input: :math:`(N, *, in\_features)` N is the batch size, `*` means any number of
              additional dimensions
            - Weight: :math:`(out\_features, in\_features)`
            - Bias: :math:`(out\_features)`
            - Output: :math:`(N, *, out\_features)`
        """
        if has_torch_function_variadic(input, weight):
            return handle_torch_function(linear, (input, weight), input, weight, bias=bias)
>       return torch._C._nn.linear(input, weight, bias)
E       RuntimeError: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#cublasApi_reproducibility

../mmf_venv/lib/python3.8/site-packages/torch/nn/functional.py:1753: RuntimeError
----------------------------- Captured stdout call -----------------------------
[32m2022-11-15T10:10:16 | mmf.trainers.mmf_trainer: [0mLoading metrics
[32m2022-11-15T10:10:16 | mmf.trainers.mmf_trainer: [0mLoading metrics
[32m2022-11-15T10:10:16 | mmf.trainers.mmf_trainer: [0mLoading metrics
[32m2022-11-15T10:10:16 | mmf.trainers.mmf_trainer: [0mLoading metrics
[32m2022-11-15T10:10:16 | mmf.trainers.core.training_loop: [0mStarting training...
[32m2022-11-15T10:10:16 | mmf.trainers.core.training_loop: [0mStarting training...
[32m2022-11-15T10:10:16 | mmf.trainers.core.training_loop: [0mStarting training...
[32m2022-11-15T10:10:16 | mmf.trainers.core.training_loop: [0mStarting training...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
_________________ TestLightningTrainer.test_epoch_over_updates _________________

self = <tests.trainers.lightning.test_loop_conditions.TestLightningTrainer testMethod=test_epoch_over_updates>

    def test_epoch_over_updates(self):
        with patch("mmf.trainers.lightning_trainer.get_mmf_env", return_value=""):
            config = self._get_config(max_steps=2, max_epochs=0.04)
            trainer = get_lightning_trainer(config=config)
            self.assertEqual(trainer._max_updates, 4)
    
            self._check_values(trainer, 0, 0)
>           trainer.trainer.fit(trainer.model, trainer.data_module.train_loader)

tests/trainers/lightning/test_loop_conditions.py:17: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:771: in fit
    self._call_and_handle_interrupt(
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:724: in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:812: in _fit_impl
    results = self._run(model, ckpt_path=self.ckpt_path)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1236: in _run
    results = self._run_stage()
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1323: in _run_stage
    return self._run_train()
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1353: in _run_train
    self.fit_loop.run()
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/loops/base.py:204: in run
    self.advance(*args, **kwargs)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/loops/fit_loop.py:281: in advance
    self._outputs = self.epoch_loop.run(self._data_fetcher)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/loops/base.py:204: in run
    self.advance(*args, **kwargs)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py:200: in advance
    batch_output = self.batch_loop.run(batch, batch_idx)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/loops/base.py:204: in run
    self.advance(*args, **kwargs)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py:88: in advance
    outputs = self.optimizer_loop.run(split_batch, optimizers, batch_idx)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/loops/base.py:204: in run
    self.advance(*args, **kwargs)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:203: in advance
    result = self._run_optimization(
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:256: in _run_optimization
    self._optimizer_step(optimizer, opt_idx, batch_idx, closure)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:365: in _optimizer_step
    self.trainer._call_lightning_module_hook(
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1595: in _call_lightning_module_hook
    output = fn(*args, **kwargs)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/core/lightning.py:1626: in optimizer_step
    optimizer.step(closure=optimizer_closure)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/core/optimizer.py:168: in step
    step_output = self._strategy.optimizer_step(self._optimizer, self._optimizer_idx, closure, **kwargs)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/strategies/strategy.py:194: in optimizer_step
    return self.precision_plugin.optimizer_step(model, optimizer, opt_idx, closure, **kwargs)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py:153: in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
../mmf_venv/lib/python3.8/site-packages/torch/optim/optimizer.py:89: in wrapper
    return func(*args, **kwargs)
../mmf_venv/lib/python3.8/site-packages/transformers/optimization.py:322: in step
    loss = closure()
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py:138: in _wrap_closure
    closure_result = closure()
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:148: in __call__
    self._result = self.closure(*args, **kwargs)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:134: in closure
    step_output = self._step_fn()
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:422: in _training_step
    training_step_output = self.trainer._call_strategy_hook("training_step", *step_kwargs.values())
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1721: in _call_strategy_hook
    output = fn(*args, **kwargs)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/strategies/strategy.py:334: in training_step
    return self.model.training_step(*args, **kwargs)
mmf/models/base_model.py:234: in training_step
    output = self._forward_lightning_step(batch, batch_idx)
mmf/models/base_model.py:276: in _forward_lightning_step
    output = self(batch)
mmf/models/base_model.py:311: in __call__
    model_output = super().__call__(sample_list, *args, **kwargs)
../mmf_venv/lib/python3.8/site-packages/torch/nn/modules/module.py:889: in _call_impl
    result = self.forward(*input, **kwargs)
tests/test_utils.py:209: in forward
    output = self.classifier(batch)
../mmf_venv/lib/python3.8/site-packages/torch/nn/modules/module.py:889: in _call_impl
    result = self.forward(*input, **kwargs)
../mmf_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py:94: in forward
    return F.linear(input, self.weight, self.bias)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = tensor([[0.]], device='cuda:0')
weight = Parameter containing:
tensor([[0.2294]], requires_grad=True)
bias = Parameter containing:
tensor([-0.2380], requires_grad=True)

    def linear(input: Tensor, weight: Tensor, bias: Optional[Tensor] = None) -> Tensor:
        r"""
        Applies a linear transformation to the incoming data: :math:`y = xA^T + b`.
    
        This operator supports :ref:`TensorFloat32<tf32_on_ampere>`.
    
        Shape:
    
            - Input: :math:`(N, *, in\_features)` N is the batch size, `*` means any number of
              additional dimensions
            - Weight: :math:`(out\_features, in\_features)`
            - Bias: :math:`(out\_features)`
            - Output: :math:`(N, *, out\_features)`
        """
        if has_torch_function_variadic(input, weight):
            return handle_torch_function(linear, (input, weight), input, weight, bias=bias)
>       return torch._C._nn.linear(input, weight, bias)
E       RuntimeError: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#cublasApi_reproducibility

../mmf_venv/lib/python3.8/site-packages/torch/nn/functional.py:1753: RuntimeError
----------------------------- Captured stdout call -----------------------------
[32m2022-11-15T10:10:16 | mmf.trainers.lightning_trainer: [0mLoading metrics
[32m2022-11-15T10:10:16 | mmf.trainers.lightning_trainer: [0mLoading metrics
[32m2022-11-15T10:10:16 | mmf.trainers.lightning_trainer: [0mLoading metrics
[32m2022-11-15T10:10:16 | mmf.trainers.lightning_trainer: [0mLoading metrics
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
----------------------------- Captured stderr call -----------------------------
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name       | Type   | Params
--------------------------------------
0 | classifier | Linear | 2     
--------------------------------------
2         Trainable params
0         Non-trainable params
2         Total params
0.000     Total estimated model params size (MB)
__________________ TestLightningTrainer.test_fractional_epoch __________________

self = <tests.trainers.lightning.test_loop_conditions.TestLightningTrainer testMethod=test_fractional_epoch>

    def test_fractional_epoch(self):
        with patch("mmf.trainers.lightning_trainer.get_mmf_env", return_value=""):
            config = self._get_config(max_steps=-1, max_epochs=0.04)
            trainer = get_lightning_trainer(config=config)
            self.assertEqual(trainer._max_updates, 4)
    
            self._check_values(trainer, 0, 0)
>           trainer.trainer.fit(trainer.model, trainer.data_module.train_loader)

tests/trainers/lightning/test_loop_conditions.py:27: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:771: in fit
    self._call_and_handle_interrupt(
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:724: in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:812: in _fit_impl
    results = self._run(model, ckpt_path=self.ckpt_path)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1236: in _run
    results = self._run_stage()
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1323: in _run_stage
    return self._run_train()
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1353: in _run_train
    self.fit_loop.run()
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/loops/base.py:204: in run
    self.advance(*args, **kwargs)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/loops/fit_loop.py:281: in advance
    self._outputs = self.epoch_loop.run(self._data_fetcher)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/loops/base.py:204: in run
    self.advance(*args, **kwargs)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py:200: in advance
    batch_output = self.batch_loop.run(batch, batch_idx)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/loops/base.py:204: in run
    self.advance(*args, **kwargs)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py:88: in advance
    outputs = self.optimizer_loop.run(split_batch, optimizers, batch_idx)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/loops/base.py:204: in run
    self.advance(*args, **kwargs)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:203: in advance
    result = self._run_optimization(
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:256: in _run_optimization
    self._optimizer_step(optimizer, opt_idx, batch_idx, closure)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:365: in _optimizer_step
    self.trainer._call_lightning_module_hook(
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1595: in _call_lightning_module_hook
    output = fn(*args, **kwargs)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/core/lightning.py:1626: in optimizer_step
    optimizer.step(closure=optimizer_closure)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/core/optimizer.py:168: in step
    step_output = self._strategy.optimizer_step(self._optimizer, self._optimizer_idx, closure, **kwargs)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/strategies/strategy.py:194: in optimizer_step
    return self.precision_plugin.optimizer_step(model, optimizer, opt_idx, closure, **kwargs)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py:153: in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
../mmf_venv/lib/python3.8/site-packages/torch/optim/optimizer.py:89: in wrapper
    return func(*args, **kwargs)
../mmf_venv/lib/python3.8/site-packages/transformers/optimization.py:322: in step
    loss = closure()
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py:138: in _wrap_closure
    closure_result = closure()
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:148: in __call__
    self._result = self.closure(*args, **kwargs)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:134: in closure
    step_output = self._step_fn()
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:422: in _training_step
    training_step_output = self.trainer._call_strategy_hook("training_step", *step_kwargs.values())
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1721: in _call_strategy_hook
    output = fn(*args, **kwargs)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/strategies/strategy.py:334: in training_step
    return self.model.training_step(*args, **kwargs)
mmf/models/base_model.py:234: in training_step
    output = self._forward_lightning_step(batch, batch_idx)
mmf/models/base_model.py:276: in _forward_lightning_step
    output = self(batch)
mmf/models/base_model.py:311: in __call__
    model_output = super().__call__(sample_list, *args, **kwargs)
../mmf_venv/lib/python3.8/site-packages/torch/nn/modules/module.py:889: in _call_impl
    result = self.forward(*input, **kwargs)
tests/test_utils.py:209: in forward
    output = self.classifier(batch)
../mmf_venv/lib/python3.8/site-packages/torch/nn/modules/module.py:889: in _call_impl
    result = self.forward(*input, **kwargs)
../mmf_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py:94: in forward
    return F.linear(input, self.weight, self.bias)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = tensor([[0.]], device='cuda:0')
weight = Parameter containing:
tensor([[0.2294]], requires_grad=True)
bias = Parameter containing:
tensor([-0.2380], requires_grad=True)

    def linear(input: Tensor, weight: Tensor, bias: Optional[Tensor] = None) -> Tensor:
        r"""
        Applies a linear transformation to the incoming data: :math:`y = xA^T + b`.
    
        This operator supports :ref:`TensorFloat32<tf32_on_ampere>`.
    
        Shape:
    
            - Input: :math:`(N, *, in\_features)` N is the batch size, `*` means any number of
              additional dimensions
            - Weight: :math:`(out\_features, in\_features)`
            - Bias: :math:`(out\_features)`
            - Output: :math:`(N, *, out\_features)`
        """
        if has_torch_function_variadic(input, weight):
            return handle_torch_function(linear, (input, weight), input, weight, bias=bias)
>       return torch._C._nn.linear(input, weight, bias)
E       RuntimeError: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#cublasApi_reproducibility

../mmf_venv/lib/python3.8/site-packages/torch/nn/functional.py:1753: RuntimeError
----------------------------- Captured stdout call -----------------------------
[32m2022-11-15T10:10:27 | mmf.trainers.lightning_trainer: [0mLoading metrics
[32m2022-11-15T10:10:27 | mmf.trainers.lightning_trainer: [0mLoading metrics
[32m2022-11-15T10:10:27 | mmf.trainers.lightning_trainer: [0mLoading metrics
[32m2022-11-15T10:10:27 | mmf.trainers.lightning_trainer: [0mLoading metrics
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
----------------------------- Captured stderr call -----------------------------
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name       | Type   | Params
--------------------------------------
0 | classifier | Linear | 2     
--------------------------------------
2         Trainable params
0         Non-trainable params
2         Total params
0.000     Total estimated model params size (MB)
______________________ TestLightningTrainer.test_updates _______________________

self = <tests.trainers.lightning.test_loop_conditions.TestLightningTrainer testMethod=test_updates>

    def test_updates(self):
        with patch("mmf.trainers.lightning_trainer.get_mmf_env", return_value=""):
            config = self._get_config(max_steps=2, max_epochs=None)
            trainer = get_lightning_trainer(config=config)
            self.assertEqual(trainer._max_updates, 2)
    
            self._check_values(trainer, 0, 0)
>           trainer.trainer.fit(trainer.model, trainer.data_module.train_loader)

tests/trainers/lightning/test_loop_conditions.py:37: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:771: in fit
    self._call_and_handle_interrupt(
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:724: in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:812: in _fit_impl
    results = self._run(model, ckpt_path=self.ckpt_path)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1236: in _run
    results = self._run_stage()
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1323: in _run_stage
    return self._run_train()
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1353: in _run_train
    self.fit_loop.run()
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/loops/base.py:204: in run
    self.advance(*args, **kwargs)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/loops/fit_loop.py:281: in advance
    self._outputs = self.epoch_loop.run(self._data_fetcher)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/loops/base.py:204: in run
    self.advance(*args, **kwargs)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py:200: in advance
    batch_output = self.batch_loop.run(batch, batch_idx)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/loops/base.py:204: in run
    self.advance(*args, **kwargs)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py:88: in advance
    outputs = self.optimizer_loop.run(split_batch, optimizers, batch_idx)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/loops/base.py:204: in run
    self.advance(*args, **kwargs)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:203: in advance
    result = self._run_optimization(
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:256: in _run_optimization
    self._optimizer_step(optimizer, opt_idx, batch_idx, closure)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:365: in _optimizer_step
    self.trainer._call_lightning_module_hook(
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1595: in _call_lightning_module_hook
    output = fn(*args, **kwargs)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/core/lightning.py:1626: in optimizer_step
    optimizer.step(closure=optimizer_closure)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/core/optimizer.py:168: in step
    step_output = self._strategy.optimizer_step(self._optimizer, self._optimizer_idx, closure, **kwargs)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/strategies/strategy.py:194: in optimizer_step
    return self.precision_plugin.optimizer_step(model, optimizer, opt_idx, closure, **kwargs)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py:153: in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
../mmf_venv/lib/python3.8/site-packages/torch/optim/optimizer.py:89: in wrapper
    return func(*args, **kwargs)
../mmf_venv/lib/python3.8/site-packages/transformers/optimization.py:322: in step
    loss = closure()
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py:138: in _wrap_closure
    closure_result = closure()
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:148: in __call__
    self._result = self.closure(*args, **kwargs)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:134: in closure
    step_output = self._step_fn()
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:422: in _training_step
    training_step_output = self.trainer._call_strategy_hook("training_step", *step_kwargs.values())
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1721: in _call_strategy_hook
    output = fn(*args, **kwargs)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/strategies/strategy.py:334: in training_step
    return self.model.training_step(*args, **kwargs)
mmf/models/base_model.py:234: in training_step
    output = self._forward_lightning_step(batch, batch_idx)
mmf/models/base_model.py:276: in _forward_lightning_step
    output = self(batch)
mmf/models/base_model.py:311: in __call__
    model_output = super().__call__(sample_list, *args, **kwargs)
../mmf_venv/lib/python3.8/site-packages/torch/nn/modules/module.py:889: in _call_impl
    result = self.forward(*input, **kwargs)
tests/test_utils.py:209: in forward
    output = self.classifier(batch)
../mmf_venv/lib/python3.8/site-packages/torch/nn/modules/module.py:889: in _call_impl
    result = self.forward(*input, **kwargs)
../mmf_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py:94: in forward
    return F.linear(input, self.weight, self.bias)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = tensor([[0.]], device='cuda:0')
weight = Parameter containing:
tensor([[0.2294]], requires_grad=True)
bias = Parameter containing:
tensor([-0.2380], requires_grad=True)

    def linear(input: Tensor, weight: Tensor, bias: Optional[Tensor] = None) -> Tensor:
        r"""
        Applies a linear transformation to the incoming data: :math:`y = xA^T + b`.
    
        This operator supports :ref:`TensorFloat32<tf32_on_ampere>`.
    
        Shape:
    
            - Input: :math:`(N, *, in\_features)` N is the batch size, `*` means any number of
              additional dimensions
            - Weight: :math:`(out\_features, in\_features)`
            - Bias: :math:`(out\_features)`
            - Output: :math:`(N, *, out\_features)`
        """
        if has_torch_function_variadic(input, weight):
            return handle_torch_function(linear, (input, weight), input, weight, bias=bias)
>       return torch._C._nn.linear(input, weight, bias)
E       RuntimeError: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#cublasApi_reproducibility

../mmf_venv/lib/python3.8/site-packages/torch/nn/functional.py:1753: RuntimeError
----------------------------- Captured stdout call -----------------------------
[32m2022-11-15T10:10:32 | mmf.trainers.lightning_trainer: [0mLoading metrics
[32m2022-11-15T10:10:32 | mmf.trainers.lightning_trainer: [0mLoading metrics
[32m2022-11-15T10:10:32 | mmf.trainers.lightning_trainer: [0mLoading metrics
[32m2022-11-15T10:10:32 | mmf.trainers.lightning_trainer: [0mLoading metrics
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
----------------------------- Captured stderr call -----------------------------
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name       | Type   | Params
--------------------------------------
0 | classifier | Linear | 2     
--------------------------------------
2         Trainable params
0         Non-trainable params
2         Total params
0.000     Total estimated model params size (MB)
____ TestLightningTrainerLoss.test_loss_computation_parity_with_mmf_trainer ____

self = <tests.trainers.lightning.test_loss.TestLightningTrainerLoss testMethod=test_loss_computation_parity_with_mmf_trainer>

    def test_loss_computation_parity_with_mmf_trainer(self):
        # compute mmf_trainer training losses
        def _on_update_end(report, meter, should_log):
            self.mmf_losses.append(report["losses"]["loss"].item())
    
        config = get_config_with_defaults(
            {"training": {"max_updates": 5, "max_epochs": None}}
        )
        mmf_trainer = get_mmf_trainer(config=config)
        mmf_trainer.on_update_end = _on_update_end
        mmf_trainer.evaluation_loop = MagicMock(return_value=(None, None))
>       mmf_trainer.training_loop()

tests/trainers/lightning/test_loss.py:31: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
mmf/trainers/core/training_loop.py:33: in training_loop
    self.run_training_epoch()
mmf/trainers/core/training_loop.py:91: in run_training_epoch
    report = self.run_training_batch(batch, num_batches_for_this_update)
mmf/trainers/core/training_loop.py:166: in run_training_batch
    report = self._forward(batch)
mmf/trainers/core/training_loop.py:200: in _forward
    model_output = self.model(prepared_batch)
mmf/models/base_model.py:311: in __call__
    model_output = super().__call__(sample_list, *args, **kwargs)
../mmf_venv/lib/python3.8/site-packages/torch/nn/modules/module.py:889: in _call_impl
    result = self.forward(*input, **kwargs)
tests/test_utils.py:209: in forward
    output = self.classifier(batch)
../mmf_venv/lib/python3.8/site-packages/torch/nn/modules/module.py:889: in _call_impl
    result = self.forward(*input, **kwargs)
../mmf_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py:94: in forward
    return F.linear(input, self.weight, self.bias)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = tensor([[0.]], device='cuda:0')
weight = Parameter containing:
tensor([[0.2294]], device='cuda:0', requires_grad=True)
bias = Parameter containing:
tensor([-0.2380], device='cuda:0', requires_grad=True)

    def linear(input: Tensor, weight: Tensor, bias: Optional[Tensor] = None) -> Tensor:
        r"""
        Applies a linear transformation to the incoming data: :math:`y = xA^T + b`.
    
        This operator supports :ref:`TensorFloat32<tf32_on_ampere>`.
    
        Shape:
    
            - Input: :math:`(N, *, in\_features)` N is the batch size, `*` means any number of
              additional dimensions
            - Weight: :math:`(out\_features, in\_features)`
            - Bias: :math:`(out\_features)`
            - Output: :math:`(N, *, out\_features)`
        """
        if has_torch_function_variadic(input, weight):
            return handle_torch_function(linear, (input, weight), input, weight, bias=bias)
>       return torch._C._nn.linear(input, weight, bias)
E       RuntimeError: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#cublasApi_reproducibility

../mmf_venv/lib/python3.8/site-packages/torch/nn/functional.py:1753: RuntimeError
----------------------------- Captured stdout call -----------------------------
[32m2022-11-15T10:10:38 | mmf.trainers.core.training_loop: [0mStarting training...
[32m2022-11-15T10:10:38 | mmf.trainers.core.training_loop: [0mStarting training...
[32m2022-11-15T10:10:38 | mmf.trainers.core.training_loop: [0mStarting training...
[32m2022-11-15T10:10:38 | mmf.trainers.core.training_loop: [0mStarting training...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
_______________ TestLightningTrainerLRSchedule.test_lr_schedule ________________

self = <tests.trainers.lightning.test_lr_schedule.TestLightningTrainerLRSchedule testMethod=test_lr_schedule>

    def test_lr_schedule(self):
        with patch("mmf.trainers.lightning_trainer.get_mmf_env", return_value=""):
            # note, be aware some of the logic also is in the SimpleLightningModel
            config = self._get_config(max_steps=8, lr_scheduler=True)
            trainer1 = get_lightning_trainer(config=config)
>           trainer1.trainer.fit(trainer1.model, trainer1.data_module.train_loader)

tests/trainers/lightning/test_lr_schedule.py:21: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:771: in fit
    self._call_and_handle_interrupt(
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:724: in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:812: in _fit_impl
    results = self._run(model, ckpt_path=self.ckpt_path)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1236: in _run
    results = self._run_stage()
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1323: in _run_stage
    return self._run_train()
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1353: in _run_train
    self.fit_loop.run()
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/loops/base.py:204: in run
    self.advance(*args, **kwargs)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/loops/fit_loop.py:281: in advance
    self._outputs = self.epoch_loop.run(self._data_fetcher)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/loops/base.py:204: in run
    self.advance(*args, **kwargs)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py:200: in advance
    batch_output = self.batch_loop.run(batch, batch_idx)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/loops/base.py:204: in run
    self.advance(*args, **kwargs)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py:88: in advance
    outputs = self.optimizer_loop.run(split_batch, optimizers, batch_idx)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/loops/base.py:204: in run
    self.advance(*args, **kwargs)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:203: in advance
    result = self._run_optimization(
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:256: in _run_optimization
    self._optimizer_step(optimizer, opt_idx, batch_idx, closure)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:365: in _optimizer_step
    self.trainer._call_lightning_module_hook(
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1595: in _call_lightning_module_hook
    output = fn(*args, **kwargs)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/core/lightning.py:1626: in optimizer_step
    optimizer.step(closure=optimizer_closure)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/core/optimizer.py:168: in step
    step_output = self._strategy.optimizer_step(self._optimizer, self._optimizer_idx, closure, **kwargs)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/strategies/strategy.py:194: in optimizer_step
    return self.precision_plugin.optimizer_step(model, optimizer, opt_idx, closure, **kwargs)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py:153: in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
../mmf_venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:65: in wrapper
    return wrapped(*args, **kwargs)
../mmf_venv/lib/python3.8/site-packages/torch/optim/optimizer.py:89: in wrapper
    return func(*args, **kwargs)
../mmf_venv/lib/python3.8/site-packages/transformers/optimization.py:322: in step
    loss = closure()
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py:138: in _wrap_closure
    closure_result = closure()
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:148: in __call__
    self._result = self.closure(*args, **kwargs)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:134: in closure
    step_output = self._step_fn()
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:422: in _training_step
    training_step_output = self.trainer._call_strategy_hook("training_step", *step_kwargs.values())
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1721: in _call_strategy_hook
    output = fn(*args, **kwargs)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/strategies/strategy.py:334: in training_step
    return self.model.training_step(*args, **kwargs)
mmf/models/base_model.py:234: in training_step
    output = self._forward_lightning_step(batch, batch_idx)
mmf/models/base_model.py:276: in _forward_lightning_step
    output = self(batch)
mmf/models/base_model.py:311: in __call__
    model_output = super().__call__(sample_list, *args, **kwargs)
../mmf_venv/lib/python3.8/site-packages/torch/nn/modules/module.py:889: in _call_impl
    result = self.forward(*input, **kwargs)
tests/test_utils.py:209: in forward
    output = self.classifier(batch)
../mmf_venv/lib/python3.8/site-packages/torch/nn/modules/module.py:889: in _call_impl
    result = self.forward(*input, **kwargs)
../mmf_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py:94: in forward
    return F.linear(input, self.weight, self.bias)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = tensor([[0.]], device='cuda:0')
weight = Parameter containing:
tensor([[0.2294]], requires_grad=True)
bias = Parameter containing:
tensor([-0.2380], requires_grad=True)

    def linear(input: Tensor, weight: Tensor, bias: Optional[Tensor] = None) -> Tensor:
        r"""
        Applies a linear transformation to the incoming data: :math:`y = xA^T + b`.
    
        This operator supports :ref:`TensorFloat32<tf32_on_ampere>`.
    
        Shape:
    
            - Input: :math:`(N, *, in\_features)` N is the batch size, `*` means any number of
              additional dimensions
            - Weight: :math:`(out\_features, in\_features)`
            - Bias: :math:`(out\_features)`
            - Output: :math:`(N, *, out\_features)`
        """
        if has_torch_function_variadic(input, weight):
            return handle_torch_function(linear, (input, weight), input, weight, bias=bias)
>       return torch._C._nn.linear(input, weight, bias)
E       RuntimeError: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#cublasApi_reproducibility

../mmf_venv/lib/python3.8/site-packages/torch/nn/functional.py:1753: RuntimeError
----------------------------- Captured stdout call -----------------------------
[32m2022-11-15T10:10:38 | mmf.trainers.lightning_trainer: [0mLoading metrics
[32m2022-11-15T10:10:38 | mmf.trainers.lightning_trainer: [0mLoading metrics
[32m2022-11-15T10:10:38 | mmf.trainers.lightning_trainer: [0mLoading metrics
[32m2022-11-15T10:10:38 | mmf.trainers.lightning_trainer: [0mLoading metrics
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
----------------------------- Captured stderr call -----------------------------
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name       | Type   | Params
--------------------------------------
0 | classifier | Linear | 2     
--------------------------------------
2         Trainable params
0         Non-trainable params
2         Total params
0.000     Total estimated model params size (MB)
___ TestLightningTrainerLRSchedule.test_lr_schedule_compared_to_mmf_is_same ____

self = <tests.trainers.lightning.test_lr_schedule.TestLightningTrainerLRSchedule testMethod=test_lr_schedule_compared_to_mmf_is_same>

    def test_lr_schedule_compared_to_mmf_is_same(self):
        config = get_config_with_defaults(
            {"training": {"max_updates": 8, "max_epochs": None, "lr_scheduler": True}}
        )
    
        mmf_trainer = get_mmf_trainer(config=config)
        mmf_trainer.lr_scheduler_callback = LRSchedulerCallback(config, mmf_trainer)
        mmf_trainer.callbacks.append(mmf_trainer.lr_scheduler_callback)
        mmf_trainer.on_update_end = mmf_trainer.lr_scheduler_callback.on_update_end
        mmf_trainer.evaluation_loop = MagicMock(return_value=(None, None))
>       mmf_trainer.training_loop()

tests/trainers/lightning/test_lr_schedule.py:41: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
mmf/trainers/core/training_loop.py:33: in training_loop
    self.run_training_epoch()
mmf/trainers/core/training_loop.py:91: in run_training_epoch
    report = self.run_training_batch(batch, num_batches_for_this_update)
mmf/trainers/core/training_loop.py:166: in run_training_batch
    report = self._forward(batch)
mmf/trainers/core/training_loop.py:200: in _forward
    model_output = self.model(prepared_batch)
mmf/models/base_model.py:311: in __call__
    model_output = super().__call__(sample_list, *args, **kwargs)
../mmf_venv/lib/python3.8/site-packages/torch/nn/modules/module.py:889: in _call_impl
    result = self.forward(*input, **kwargs)
tests/test_utils.py:209: in forward
    output = self.classifier(batch)
../mmf_venv/lib/python3.8/site-packages/torch/nn/modules/module.py:889: in _call_impl
    result = self.forward(*input, **kwargs)
../mmf_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py:94: in forward
    return F.linear(input, self.weight, self.bias)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = tensor([[0.]], device='cuda:0')
weight = Parameter containing:
tensor([[0.2294]], device='cuda:0', requires_grad=True)
bias = Parameter containing:
tensor([-0.2380], device='cuda:0', requires_grad=True)

    def linear(input: Tensor, weight: Tensor, bias: Optional[Tensor] = None) -> Tensor:
        r"""
        Applies a linear transformation to the incoming data: :math:`y = xA^T + b`.
    
        This operator supports :ref:`TensorFloat32<tf32_on_ampere>`.
    
        Shape:
    
            - Input: :math:`(N, *, in\_features)` N is the batch size, `*` means any number of
              additional dimensions
            - Weight: :math:`(out\_features, in\_features)`
            - Bias: :math:`(out\_features)`
            - Output: :math:`(N, *, out\_features)`
        """
        if has_torch_function_variadic(input, weight):
            return handle_torch_function(linear, (input, weight), input, weight, bias=bias)
>       return torch._C._nn.linear(input, weight, bias)
E       RuntimeError: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#cublasApi_reproducibility

../mmf_venv/lib/python3.8/site-packages/torch/nn/functional.py:1753: RuntimeError
----------------------------- Captured stdout call -----------------------------
[32m2022-11-15T10:10:49 | mmf.trainers.core.training_loop: [0mStarting training...
[32m2022-11-15T10:10:49 | mmf.trainers.core.training_loop: [0mStarting training...
[32m2022-11-15T10:10:49 | mmf.trainers.core.training_loop: [0mStarting training...
[32m2022-11-15T10:10:49 | mmf.trainers.core.training_loop: [0mStarting training...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
________________ TestLightningTrainerValidation.test_validation ________________

self = <tests.trainers.lightning.test_validation.TestLightningTrainerValidation testMethod=test_validation>
log_dir = <MagicMock name='get_mmf_env' id='140113646753200'>
mkdirs = <MagicMock name='mkdirs' id='140111773200640'>

    @patch("mmf.common.test_reporter.PathManager.mkdirs")
    @patch("mmf.trainers.lightning_trainer.get_mmf_env", return_value="")
    def test_validation(self, log_dir, mkdirs):
        config = self._get_config(
            max_steps=8,
            batch_size=2,
            val_check_interval=3,
            log_every_n_steps=9,  # turn it off
            limit_val_batches=1.0,
        )
        trainer = get_lightning_trainer(config=config, prepare_trainer=False)
        callback = LightningLoopCallback(trainer)
        trainer.callbacks.append(callback)
        lightning_values = []
    
        def log_values(
            current_iteration: int,
            num_updates: int,
            max_updates: int,
            meter: Meter,
            extra: Dict[str, Any],
            tb_writer: TensorboardLogger,
        ):
            lightning_values.append(
                {
                    "current_iteration": current_iteration,
                    "num_updates": num_updates,
                    "max_updates": max_updates,
                    "avg_loss": meter.loss.avg,
                }
            )
    
        with patch(
            "mmf.trainers.lightning_core.loop_callback.summarize_report",
            side_effect=log_values,
        ):
>           run_lightning_trainer(trainer)

tests/trainers/lightning/test_validation.py:88: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/trainers/test_utils.py:139: in run_lightning_trainer
    trainer.trainer.fit(
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:771: in fit
    self._call_and_handle_interrupt(
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:724: in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:812: in _fit_impl
    results = self._run(model, ckpt_path=self.ckpt_path)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1236: in _run
    results = self._run_stage()
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1323: in _run_stage
    return self._run_train()
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1353: in _run_train
    self.fit_loop.run()
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/loops/base.py:204: in run
    self.advance(*args, **kwargs)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/loops/fit_loop.py:281: in advance
    self._outputs = self.epoch_loop.run(self._data_fetcher)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/loops/base.py:204: in run
    self.advance(*args, **kwargs)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py:200: in advance
    batch_output = self.batch_loop.run(batch, batch_idx)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/loops/base.py:204: in run
    self.advance(*args, **kwargs)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py:88: in advance
    outputs = self.optimizer_loop.run(split_batch, optimizers, batch_idx)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/loops/base.py:204: in run
    self.advance(*args, **kwargs)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:203: in advance
    result = self._run_optimization(
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:256: in _run_optimization
    self._optimizer_step(optimizer, opt_idx, batch_idx, closure)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:365: in _optimizer_step
    self.trainer._call_lightning_module_hook(
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1595: in _call_lightning_module_hook
    output = fn(*args, **kwargs)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/core/lightning.py:1626: in optimizer_step
    optimizer.step(closure=optimizer_closure)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/core/optimizer.py:168: in step
    step_output = self._strategy.optimizer_step(self._optimizer, self._optimizer_idx, closure, **kwargs)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/strategies/strategy.py:194: in optimizer_step
    return self.precision_plugin.optimizer_step(model, optimizer, opt_idx, closure, **kwargs)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py:153: in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
../mmf_venv/lib/python3.8/site-packages/torch/optim/optimizer.py:89: in wrapper
    return func(*args, **kwargs)
../mmf_venv/lib/python3.8/site-packages/transformers/optimization.py:322: in step
    loss = closure()
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py:138: in _wrap_closure
    closure_result = closure()
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:148: in __call__
    self._result = self.closure(*args, **kwargs)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:134: in closure
    step_output = self._step_fn()
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:422: in _training_step
    training_step_output = self.trainer._call_strategy_hook("training_step", *step_kwargs.values())
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1721: in _call_strategy_hook
    output = fn(*args, **kwargs)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/strategies/strategy.py:334: in training_step
    return self.model.training_step(*args, **kwargs)
mmf/models/base_model.py:234: in training_step
    output = self._forward_lightning_step(batch, batch_idx)
mmf/models/base_model.py:276: in _forward_lightning_step
    output = self(batch)
mmf/models/base_model.py:311: in __call__
    model_output = super().__call__(sample_list, *args, **kwargs)
../mmf_venv/lib/python3.8/site-packages/torch/nn/modules/module.py:889: in _call_impl
    result = self.forward(*input, **kwargs)
tests/test_utils.py:209: in forward
    output = self.classifier(batch)
../mmf_venv/lib/python3.8/site-packages/torch/nn/modules/module.py:889: in _call_impl
    result = self.forward(*input, **kwargs)
../mmf_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py:94: in forward
    return F.linear(input, self.weight, self.bias)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = tensor([[0.],
        [1.]], device='cuda:0')
weight = Parameter containing:
tensor([[0.2294]], requires_grad=True)
bias = Parameter containing:
tensor([-0.2380], requires_grad=True)

    def linear(input: Tensor, weight: Tensor, bias: Optional[Tensor] = None) -> Tensor:
        r"""
        Applies a linear transformation to the incoming data: :math:`y = xA^T + b`.
    
        This operator supports :ref:`TensorFloat32<tf32_on_ampere>`.
    
        Shape:
    
            - Input: :math:`(N, *, in\_features)` N is the batch size, `*` means any number of
              additional dimensions
            - Weight: :math:`(out\_features, in\_features)`
            - Bias: :math:`(out\_features)`
            - Output: :math:`(N, *, out\_features)`
        """
        if has_torch_function_variadic(input, weight):
            return handle_torch_function(linear, (input, weight), input, weight, bias=bias)
>       return torch._C._nn.linear(input, weight, bias)
E       RuntimeError: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#cublasApi_reproducibility

../mmf_venv/lib/python3.8/site-packages/torch/nn/functional.py:1753: RuntimeError
----------------------------- Captured stdout call -----------------------------
[32m2022-11-15T10:10:49 | mmf.trainers.lightning_trainer: [0mLoading metrics
[32m2022-11-15T10:10:49 | mmf.trainers.lightning_trainer: [0mLoading metrics
[32m2022-11-15T10:10:49 | mmf.trainers.lightning_trainer: [0mLoading metrics
[32m2022-11-15T10:10:49 | mmf.trainers.lightning_trainer: [0mLoading metrics
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
----------------------------- Captured stderr call -----------------------------
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name       | Type   | Params
--------------------------------------
0 | classifier | Linear | 2     
--------------------------------------
2         Trainable params
0         Non-trainable params
2         Total params
0.000     Total estimated model params size (MB)
____________ TestLightningTrainerValidation.test_validation_parity _____________

self = <tests.trainers.lightning.test_validation.TestLightningTrainerValidation testMethod=test_validation_parity>
summarize_report_fn = <MagicMock name='summarize_report' id='140111771461088'>
test_reporter = <MagicMock name='get_mmf_env' id='140113108709488'>
sw = <MagicMock name='SummaryWriter' id='140113110052768'>
mkdirs = <MagicMock name='mkdirs' id='140113112453952'>

    @patch("mmf.common.test_reporter.PathManager.mkdirs")
    @patch("torch.utils.tensorboard.SummaryWriter")
    @patch("mmf.common.test_reporter.get_mmf_env", return_value="")
    @patch("mmf.trainers.callbacks.logistics.summarize_report")
    def test_validation_parity(self, summarize_report_fn, test_reporter, sw, mkdirs):
        config = self._get_mmf_config(
            max_updates=8, max_epochs=None, batch_size=2, evaluation_interval=3
        )
        mmf_trainer = get_mmf_trainer(config=config)
        mmf_trainer.load_metrics()
        logistics_callback = LogisticsCallback(mmf_trainer.config, mmf_trainer)
        logistics_callback.snapshot_timer = Timer()
        logistics_callback.train_timer = Timer()
        mmf_trainer.logistics_callback = logistics_callback
        mmf_trainer.callbacks.append(logistics_callback)
        mmf_trainer.early_stop_callback = MagicMock(return_value=None)
        mmf_trainer.on_validation_end = logistics_callback.on_validation_end
>       mmf_trainer.training_loop()

tests/trainers/lightning/test_validation.py:172: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
mmf/trainers/core/training_loop.py:33: in training_loop
    self.run_training_epoch()
mmf/trainers/core/training_loop.py:91: in run_training_epoch
    report = self.run_training_batch(batch, num_batches_for_this_update)
mmf/trainers/core/training_loop.py:166: in run_training_batch
    report = self._forward(batch)
mmf/trainers/core/training_loop.py:200: in _forward
    model_output = self.model(prepared_batch)
mmf/models/base_model.py:311: in __call__
    model_output = super().__call__(sample_list, *args, **kwargs)
../mmf_venv/lib/python3.8/site-packages/torch/nn/modules/module.py:889: in _call_impl
    result = self.forward(*input, **kwargs)
tests/test_utils.py:209: in forward
    output = self.classifier(batch)
../mmf_venv/lib/python3.8/site-packages/torch/nn/modules/module.py:889: in _call_impl
    result = self.forward(*input, **kwargs)
../mmf_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py:94: in forward
    return F.linear(input, self.weight, self.bias)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = tensor([[0.],
        [1.]], device='cuda:0')
weight = Parameter containing:
tensor([[0.2294]], device='cuda:0', requires_grad=True)
bias = Parameter containing:
tensor([-0.2380], device='cuda:0', requires_grad=True)

    def linear(input: Tensor, weight: Tensor, bias: Optional[Tensor] = None) -> Tensor:
        r"""
        Applies a linear transformation to the incoming data: :math:`y = xA^T + b`.
    
        This operator supports :ref:`TensorFloat32<tf32_on_ampere>`.
    
        Shape:
    
            - Input: :math:`(N, *, in\_features)` N is the batch size, `*` means any number of
              additional dimensions
            - Weight: :math:`(out\_features, in\_features)`
            - Bias: :math:`(out\_features)`
            - Output: :math:`(N, *, out\_features)`
        """
        if has_torch_function_variadic(input, weight):
            return handle_torch_function(linear, (input, weight), input, weight, bias=bias)
>       return torch._C._nn.linear(input, weight, bias)
E       RuntimeError: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#cublasApi_reproducibility

../mmf_venv/lib/python3.8/site-packages/torch/nn/functional.py:1753: RuntimeError
----------------------------- Captured stdout call -----------------------------
[32m2022-11-15T10:10:54 | mmf.trainers.mmf_trainer: [0mLoading metrics
[32m2022-11-15T10:10:54 | mmf.trainers.mmf_trainer: [0mLoading metrics
[32m2022-11-15T10:10:54 | mmf.trainers.mmf_trainer: [0mLoading metrics
[32m2022-11-15T10:10:54 | mmf.trainers.mmf_trainer: [0mLoading metrics
[32m2022-11-15T10:10:54 | mmf.trainers.core.training_loop: [0mStarting training...
[32m2022-11-15T10:10:54 | mmf.trainers.core.training_loop: [0mStarting training...
[32m2022-11-15T10:10:54 | mmf.trainers.core.training_loop: [0mStarting training...
[32m2022-11-15T10:10:54 | mmf.trainers.core.training_loop: [0mStarting training...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
_________ TestLightningTrainerValidation.test_validation_torchmetrics __________

self = <tests.trainers.lightning.test_validation.TestLightningTrainerValidation testMethod=test_validation_torchmetrics>
log_dir = <MagicMock name='get_mmf_env' id='140113650106960'>
mkdirs = <MagicMock name='mkdirs' id='140111772300000'>

    @patch("mmf.common.test_reporter.PathManager.mkdirs")
    @patch("mmf.trainers.lightning_trainer.get_mmf_env", return_value="")
    def test_validation_torchmetrics(self, log_dir, mkdirs):
        config = self._get_config(
            max_steps=8,
            batch_size=2,
            val_check_interval=3,
            log_every_n_steps=9,  # turn it off
            limit_val_batches=1.0,
        )
        trainer = get_lightning_trainer(config=config, prepare_trainer=False)
        trainer.torchmetrics = LightningTorchMetrics([])
        callback = LightningTorchMetricsCallback(trainer)
        trainer.callbacks.append(callback)
        lightning_values = []
    
        def log_values(
            extra: Optional[Dict],
            num_updates: int,
            max_updates: int,
            log_type: str = "train",
        ):
            lightning_values.append(
                {"num_updates": num_updates, "max_updates": max_updates}
            )
    
        with patch(
            "mmf.trainers.lightning_core.loop_callback_with_torchmetrics"
            + ".LightningTorchMetricsCallback._log_metrics_and_extra",
            side_effect=log_values,
        ):
>           run_lightning_trainer(trainer)

tests/trainers/lightning/test_validation.py:143: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/trainers/test_utils.py:139: in run_lightning_trainer
    trainer.trainer.fit(
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:771: in fit
    self._call_and_handle_interrupt(
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:724: in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:812: in _fit_impl
    results = self._run(model, ckpt_path=self.ckpt_path)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1236: in _run
    results = self._run_stage()
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1323: in _run_stage
    return self._run_train()
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1353: in _run_train
    self.fit_loop.run()
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/loops/base.py:204: in run
    self.advance(*args, **kwargs)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/loops/fit_loop.py:281: in advance
    self._outputs = self.epoch_loop.run(self._data_fetcher)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/loops/base.py:204: in run
    self.advance(*args, **kwargs)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py:200: in advance
    batch_output = self.batch_loop.run(batch, batch_idx)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/loops/base.py:204: in run
    self.advance(*args, **kwargs)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py:88: in advance
    outputs = self.optimizer_loop.run(split_batch, optimizers, batch_idx)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/loops/base.py:204: in run
    self.advance(*args, **kwargs)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:203: in advance
    result = self._run_optimization(
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:256: in _run_optimization
    self._optimizer_step(optimizer, opt_idx, batch_idx, closure)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:365: in _optimizer_step
    self.trainer._call_lightning_module_hook(
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1595: in _call_lightning_module_hook
    output = fn(*args, **kwargs)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/core/lightning.py:1626: in optimizer_step
    optimizer.step(closure=optimizer_closure)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/core/optimizer.py:168: in step
    step_output = self._strategy.optimizer_step(self._optimizer, self._optimizer_idx, closure, **kwargs)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/strategies/strategy.py:194: in optimizer_step
    return self.precision_plugin.optimizer_step(model, optimizer, opt_idx, closure, **kwargs)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py:153: in optimizer_step
    return optimizer.step(closure=closure, **kwargs)
../mmf_venv/lib/python3.8/site-packages/torch/optim/optimizer.py:89: in wrapper
    return func(*args, **kwargs)
../mmf_venv/lib/python3.8/site-packages/transformers/optimization.py:322: in step
    loss = closure()
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/plugins/precision/precision_plugin.py:138: in _wrap_closure
    closure_result = closure()
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:148: in __call__
    self._result = self.closure(*args, **kwargs)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:134: in closure
    step_output = self._step_fn()
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:422: in _training_step
    training_step_output = self.trainer._call_strategy_hook("training_step", *step_kwargs.values())
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1721: in _call_strategy_hook
    output = fn(*args, **kwargs)
../mmf_venv/lib/python3.8/site-packages/pytorch_lightning/strategies/strategy.py:334: in training_step
    return self.model.training_step(*args, **kwargs)
mmf/models/base_model.py:234: in training_step
    output = self._forward_lightning_step(batch, batch_idx)
mmf/models/base_model.py:276: in _forward_lightning_step
    output = self(batch)
mmf/models/base_model.py:311: in __call__
    model_output = super().__call__(sample_list, *args, **kwargs)
../mmf_venv/lib/python3.8/site-packages/torch/nn/modules/module.py:889: in _call_impl
    result = self.forward(*input, **kwargs)
tests/test_utils.py:209: in forward
    output = self.classifier(batch)
../mmf_venv/lib/python3.8/site-packages/torch/nn/modules/module.py:889: in _call_impl
    result = self.forward(*input, **kwargs)
../mmf_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py:94: in forward
    return F.linear(input, self.weight, self.bias)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = tensor([[0.],
        [1.]], device='cuda:0')
weight = Parameter containing:
tensor([[0.2294]], requires_grad=True)
bias = Parameter containing:
tensor([-0.2380], requires_grad=True)

    def linear(input: Tensor, weight: Tensor, bias: Optional[Tensor] = None) -> Tensor:
        r"""
        Applies a linear transformation to the incoming data: :math:`y = xA^T + b`.
    
        This operator supports :ref:`TensorFloat32<tf32_on_ampere>`.
    
        Shape:
    
            - Input: :math:`(N, *, in\_features)` N is the batch size, `*` means any number of
              additional dimensions
            - Weight: :math:`(out\_features, in\_features)`
            - Bias: :math:`(out\_features)`
            - Output: :math:`(N, *, out\_features)`
        """
        if has_torch_function_variadic(input, weight):
            return handle_torch_function(linear, (input, weight), input, weight, bias=bias)
>       return torch._C._nn.linear(input, weight, bias)
E       RuntimeError: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#cublasApi_reproducibility

../mmf_venv/lib/python3.8/site-packages/torch/nn/functional.py:1753: RuntimeError
----------------------------- Captured stdout call -----------------------------
[32m2022-11-15T10:11:00 | mmf.trainers.lightning_trainer: [0mLoading metrics
[32m2022-11-15T10:11:00 | mmf.trainers.lightning_trainer: [0mLoading metrics
[32m2022-11-15T10:11:00 | mmf.trainers.lightning_trainer: [0mLoading metrics
[32m2022-11-15T10:11:00 | mmf.trainers.lightning_trainer: [0mLoading metrics
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
----------------------------- Captured stderr call -----------------------------
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name       | Type   | Params
--------------------------------------
0 | classifier | Linear | 2     
--------------------------------------
2         Trainable params
0         Non-trainable params
2         Total params
0.000     Total estimated model params size (MB)
______________ TestUtilsCheckpoint.test_checkpoint_scaler_loading ______________

self = <tests.utils.test_checkpoint.TestUtilsCheckpoint testMethod=test_checkpoint_scaler_loading>

    @skip_if_no_cuda
    def test_checkpoint_scaler_loading(self):
        with mock_env_with_temp():
            original_scaler = deepcopy(self.trainer.scaler)
    
            checkpoint = Checkpoint(self.trainer)
            self._init_early_stopping(checkpoint)
    
>           self._do_a_fp16_pass()

tests/utils/test_checkpoint.py:416: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/utils/test_checkpoint.py:590: in _do_a_fp16_pass
    loss = self.trainer.model(
mmf/models/base_model.py:311: in __call__
    model_output = super().__call__(sample_list, *args, **kwargs)
../mmf_venv/lib/python3.8/site-packages/torch/nn/modules/module.py:889: in _call_impl
    result = self.forward(*input, **kwargs)
tests/utils/test_checkpoint.py:48: in forward
    x = self.classifier(self.base(x))
../mmf_venv/lib/python3.8/site-packages/torch/nn/modules/module.py:889: in _call_impl
    result = self.forward(*input, **kwargs)
../mmf_venv/lib/python3.8/site-packages/torch/nn/modules/container.py:119: in forward
    input = module(input)
../mmf_venv/lib/python3.8/site-packages/torch/nn/modules/module.py:889: in _call_impl
    result = self.forward(*input, **kwargs)
../mmf_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py:94: in forward
    return F.linear(input, self.weight, self.bias)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = tensor([[0.7595, 0.5311, 0.6449, 0.7224, 0.4416],
        [0.3634, 0.8818, 0.9874, 0.7316, 0.2814],
        [0.0651, 0...9, 0.4533, 0.3499],
        [0.7428, 0.4601, 0.0242, 0.6630, 0.9787]], device='cuda:0',
       grad_fn=<CopyBackwards>)
weight = Parameter containing:
tensor([[-0.4213, -0.0877, -0.2148, -0.1193, -0.3951],
        [ 0.1795, -0.4009, -0.0285,  0.15...  0.2869, -0.1974],
        [ 0.1625, -0.1935,  0.1402, -0.2337,  0.2069]], device='cuda:0',
       requires_grad=True)
bias = Parameter containing:
tensor([ 0.0905, -0.1750, -0.2194,  0.1157], device='cuda:0',
       requires_grad=True)

    def linear(input: Tensor, weight: Tensor, bias: Optional[Tensor] = None) -> Tensor:
        r"""
        Applies a linear transformation to the incoming data: :math:`y = xA^T + b`.
    
        This operator supports :ref:`TensorFloat32<tf32_on_ampere>`.
    
        Shape:
    
            - Input: :math:`(N, *, in\_features)` N is the batch size, `*` means any number of
              additional dimensions
            - Weight: :math:`(out\_features, in\_features)`
            - Bias: :math:`(out\_features)`
            - Output: :math:`(N, *, out\_features)`
        """
        if has_torch_function_variadic(input, weight):
            return handle_torch_function(linear, (input, weight), input, weight, bias=bias)
>       return torch._C._nn.linear(input, weight, bias)
E       RuntimeError: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#cublasApi_reproducibility

../mmf_venv/lib/python3.8/site-packages/torch/nn/functional.py:1753: RuntimeError
___________________ TestUtilsEnvE2E.test_cpu_evaluation_e2e ____________________

self = <tests.utils.test_env.TestUtilsEnvE2E testMethod=test_cpu_evaluation_e2e>

    def test_cpu_evaluation_e2e(self):
>       self._test_user_import_e2e(extra_opts=["evaluation.use_cpu=True"])

tests/utils/test_env.py:88: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/utils/test_env.py:64: in _test_user_import_e2e
    run(opts)
mmf_cli/run.py:133: in run
    main(configuration, predict=predict)
mmf_cli/run.py:56: in main
    trainer.train()
mmf/trainers/mmf_trainer.py:145: in train
    self.training_loop()
mmf/trainers/core/training_loop.py:33: in training_loop
    self.run_training_epoch()
mmf/trainers/core/training_loop.py:91: in run_training_epoch
    report = self.run_training_batch(batch, num_batches_for_this_update)
mmf/trainers/core/training_loop.py:166: in run_training_batch
    report = self._forward(batch)
mmf/trainers/core/training_loop.py:200: in _forward
    model_output = self.model(prepared_batch)
mmf/models/base_model.py:311: in __call__
    model_output = super().__call__(sample_list, *args, **kwargs)
../mmf_venv/lib/python3.8/site-packages/torch/nn/modules/module.py:889: in _call_impl
    result = self.forward(*input, **kwargs)
tests/data/user_dir/models/simple.py:14: in forward
    return {"scores": self.classifier(sample_list.input)}
../mmf_venv/lib/python3.8/site-packages/torch/nn/modules/module.py:889: in _call_impl
    result = self.forward(*input, **kwargs)
../mmf_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py:94: in forward
    return F.linear(input, self.weight, self.bias)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = tensor([[11.],
        [19.],
        [ 1.],
        [ 8.],
        [ 4.],
        [14.],
        [18.],
        [ 6.]], device='cuda:0')
weight = Parameter containing:
tensor([[0.5153]], device='cuda:0', requires_grad=True)
bias = Parameter containing:
tensor([-0.4414], device='cuda:0', requires_grad=True)

    def linear(input: Tensor, weight: Tensor, bias: Optional[Tensor] = None) -> Tensor:
        r"""
        Applies a linear transformation to the incoming data: :math:`y = xA^T + b`.
    
        This operator supports :ref:`TensorFloat32<tf32_on_ampere>`.
    
        Shape:
    
            - Input: :math:`(N, *, in\_features)` N is the batch size, `*` means any number of
              additional dimensions
            - Weight: :math:`(out\_features, in\_features)`
            - Bias: :math:`(out\_features)`
            - Output: :math:`(N, *, out\_features)`
        """
        if has_torch_function_variadic(input, weight):
            return handle_torch_function(linear, (input, weight), input, weight, bias=bias)
>       return torch._C._nn.linear(input, weight, bias)
E       RuntimeError: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#cublasApi_reproducibility

../mmf_venv/lib/python3.8/site-packages/torch/nn/functional.py:1753: RuntimeError
----------------------------- Captured stdout call -----------------------------
[32m2022-11-15T10:11:23 | mmf.utils.env: [0mImporting from /code/mmf/mmf/../tests/data/user_dir
[32m2022-11-15T10:11:23 | mmf.utils.env: [0mImporting from /code/mmf/mmf/../tests/data/user_dir
[32m2022-11-15T10:11:23 | mmf.utils.env: [0mImporting from /code/mmf/mmf/../tests/data/user_dir
[32m2022-11-15T10:11:23 | mmf.utils.env: [0mImporting from /code/mmf/mmf/../tests/data/user_dir
[32m2022-11-15T10:11:23 | mmf.utils.configuration: [0mOverriding option model to simple
[32m2022-11-15T10:11:23 | mmf.utils.configuration: [0mOverriding option model to simple
[32m2022-11-15T10:11:23 | mmf.utils.configuration: [0mOverriding option model to simple
[32m2022-11-15T10:11:23 | mmf.utils.configuration: [0mOverriding option model to simple
[32m2022-11-15T10:11:23 | mmf.utils.configuration: [0mOverriding option run_type to train_val_test
[32m2022-11-15T10:11:23 | mmf.utils.configuration: [0mOverriding option run_type to train_val_test
[32m2022-11-15T10:11:23 | mmf.utils.configuration: [0mOverriding option run_type to train_val_test
[32m2022-11-15T10:11:23 | mmf.utils.configuration: [0mOverriding option run_type to train_val_test
[32m2022-11-15T10:11:23 | mmf.utils.configuration: [0mOverriding option datasets to always_one
[32m2022-11-15T10:11:23 | mmf.utils.configuration: [0mOverriding option datasets to always_one
[32m2022-11-15T10:11:23 | mmf.utils.configuration: [0mOverriding option datasets to always_one
[32m2022-11-15T10:11:23 | mmf.utils.configuration: [0mOverriding option datasets to always_one
[32m2022-11-15T10:11:23 | mmf.utils.configuration: [0mOverriding option config to configs/experiment.yaml
[32m2022-11-15T10:11:23 | mmf.utils.configuration: [0mOverriding option config to configs/experiment.yaml
[32m2022-11-15T10:11:23 | mmf.utils.configuration: [0mOverriding option config to configs/experiment.yaml
[32m2022-11-15T10:11:23 | mmf.utils.configuration: [0mOverriding option config to configs/experiment.yaml
[32m2022-11-15T10:11:23 | mmf.utils.configuration: [0mOverriding option env.user_dir to /code/mmf/mmf/../tests/data/user_dir
[32m2022-11-15T10:11:23 | mmf.utils.configuration: [0mOverriding option env.user_dir to /code/mmf/mmf/../tests/data/user_dir
[32m2022-11-15T10:11:23 | mmf.utils.configuration: [0mOverriding option env.user_dir to /code/mmf/mmf/../tests/data/user_dir
[32m2022-11-15T10:11:23 | mmf.utils.configuration: [0mOverriding option env.user_dir to /code/mmf/mmf/../tests/data/user_dir
[32m2022-11-15T10:11:23 | mmf.utils.configuration: [0mOverriding option training.seed to 1
[32m2022-11-15T10:11:23 | mmf.utils.configuration: [0mOverriding option training.seed to 1
[32m2022-11-15T10:11:23 | mmf.utils.configuration: [0mOverriding option training.seed to 1
[32m2022-11-15T10:11:23 | mmf.utils.configuration: [0mOverriding option training.seed to 1
[32m2022-11-15T10:11:23 | mmf.utils.configuration: [0mOverriding option training.num_workers to 3
[32m2022-11-15T10:11:23 | mmf.utils.configuration: [0mOverriding option training.num_workers to 3
[32m2022-11-15T10:11:23 | mmf.utils.configuration: [0mOverriding option training.num_workers to 3
[32m2022-11-15T10:11:23 | mmf.utils.configuration: [0mOverriding option training.num_workers to 3
[32m2022-11-15T10:11:23 | mmf.utils.configuration: [0mOverriding option training.max_updates to 50
[32m2022-11-15T10:11:23 | mmf.utils.configuration: [0mOverriding option training.max_updates to 50
[32m2022-11-15T10:11:23 | mmf.utils.configuration: [0mOverriding option training.max_updates to 50
[32m2022-11-15T10:11:23 | mmf.utils.configuration: [0mOverriding option training.max_updates to 50
[32m2022-11-15T10:11:23 | mmf.utils.configuration: [0mOverriding option env.save_dir to /tmp/tmpjmrt4tvn
[32m2022-11-15T10:11:23 | mmf.utils.configuration: [0mOverriding option env.save_dir to /tmp/tmpjmrt4tvn
[32m2022-11-15T10:11:23 | mmf.utils.configuration: [0mOverriding option env.save_dir to /tmp/tmpjmrt4tvn
[32m2022-11-15T10:11:23 | mmf.utils.configuration: [0mOverriding option env.save_dir to /tmp/tmpjmrt4tvn
[32m2022-11-15T10:11:23 | mmf.utils.configuration: [0mOverriding option evaluation.use_cpu to True
[32m2022-11-15T10:11:23 | mmf.utils.configuration: [0mOverriding option evaluation.use_cpu to True
[32m2022-11-15T10:11:23 | mmf.utils.configuration: [0mOverriding option evaluation.use_cpu to True
[32m2022-11-15T10:11:23 | mmf.utils.configuration: [0mOverriding option evaluation.use_cpu to True
[32m2022-11-15T10:11:23 | mmf.utils.env: [0mUser dir /code/mmf/mmf/../tests/data/user_dir already imported. Skipping.
[32m2022-11-15T10:11:23 | mmf.utils.env: [0mUser dir /code/mmf/mmf/../tests/data/user_dir already imported. Skipping.
[32m2022-11-15T10:11:23 | mmf.utils.env: [0mUser dir /code/mmf/mmf/../tests/data/user_dir already imported. Skipping.
[32m2022-11-15T10:11:23 | mmf.utils.env: [0mUser dir /code/mmf/mmf/../tests/data/user_dir already imported. Skipping.
[32m2022-11-15T10:11:23 | mmf: [0mLogging to: /tmp/tmpjmrt4tvn/train.log
[32m2022-11-15T10:11:23 | mmf: [0mLogging to: /tmp/tmpjmrt4tvn/train.log
[32m2022-11-15T10:11:23 | mmf: [0mLogging to: /tmp/tmpjmrt4tvn/train.log
[32m2022-11-15T10:11:23 | mmf: [0mLogging to: /tmp/tmpjmrt4tvn/train.log
[32m2022-11-15T10:11:23 | mmf.utils.general: [0mCUDA Device 0 is: NVIDIA GeForce RTX 3090
[32m2022-11-15T10:11:23 | mmf.utils.general: [0mCUDA Device 0 is: NVIDIA GeForce RTX 3090
[32m2022-11-15T10:11:23 | mmf.utils.general: [0mCUDA Device 0 is: NVIDIA GeForce RTX 3090
[32m2022-11-15T10:11:23 | mmf.utils.general: [0mCUDA Device 0 is: NVIDIA GeForce RTX 3090
[32m2022-11-15T10:11:23 | mmf.trainers.mmf_trainer: [0mLoading datasets
[32m2022-11-15T10:11:23 | mmf.trainers.mmf_trainer: [0mLoading datasets
[32m2022-11-15T10:11:23 | mmf.trainers.mmf_trainer: [0mLoading datasets
[32m2022-11-15T10:11:23 | mmf.trainers.mmf_trainer: [0mLoading datasets
[32m2022-11-15T10:11:23 | mmf.datasets.multi_datamodule: [0mMultitasking disabled by default for single dataset training
[32m2022-11-15T10:11:23 | mmf.datasets.multi_datamodule: [0mMultitasking disabled by default for single dataset training
[32m2022-11-15T10:11:23 | mmf.datasets.multi_datamodule: [0mMultitasking disabled by default for single dataset training
[32m2022-11-15T10:11:23 | mmf.datasets.multi_datamodule: [0mMultitasking disabled by default for single dataset training
[32m2022-11-15T10:11:23 | mmf.datasets.multi_datamodule: [0mMultitasking disabled by default for single dataset training
[32m2022-11-15T10:11:23 | mmf.datasets.multi_datamodule: [0mMultitasking disabled by default for single dataset training
[32m2022-11-15T10:11:23 | mmf.datasets.multi_datamodule: [0mMultitasking disabled by default for single dataset training
[32m2022-11-15T10:11:23 | mmf.datasets.multi_datamodule: [0mMultitasking disabled by default for single dataset training
[32m2022-11-15T10:11:23 | mmf.datasets.multi_datamodule: [0mMultitasking disabled by default for single dataset training
[32m2022-11-15T10:11:23 | mmf.datasets.multi_datamodule: [0mMultitasking disabled by default for single dataset training
[32m2022-11-15T10:11:23 | mmf.datasets.multi_datamodule: [0mMultitasking disabled by default for single dataset training
[32m2022-11-15T10:11:23 | mmf.datasets.multi_datamodule: [0mMultitasking disabled by default for single dataset training
[32m2022-11-15T10:11:23 | mmf.trainers.mmf_trainer: [0mLoading model
[32m2022-11-15T10:11:23 | mmf.trainers.mmf_trainer: [0mLoading model
[32m2022-11-15T10:11:23 | mmf.trainers.mmf_trainer: [0mLoading model
[32m2022-11-15T10:11:23 | mmf.trainers.mmf_trainer: [0mLoading model
[32m2022-11-15T10:11:23 | mmf.trainers.mmf_trainer: [0mLoading optimizer
[32m2022-11-15T10:11:23 | mmf.trainers.mmf_trainer: [0mLoading optimizer
[32m2022-11-15T10:11:23 | mmf.trainers.mmf_trainer: [0mLoading optimizer
[32m2022-11-15T10:11:23 | mmf.trainers.mmf_trainer: [0mLoading optimizer
[32m2022-11-15T10:11:23 | mmf.trainers.mmf_trainer: [0mLoading metrics
[32m2022-11-15T10:11:23 | mmf.trainers.mmf_trainer: [0mLoading metrics
[32m2022-11-15T10:11:23 | mmf.trainers.mmf_trainer: [0mLoading metrics
[32m2022-11-15T10:11:23 | mmf.trainers.mmf_trainer: [0mLoading metrics
[32m2022-11-15T10:11:23 | mmf.trainers.mmf_trainer: [0m===== Model =====
[32m2022-11-15T10:11:23 | mmf.trainers.mmf_trainer: [0m===== Model =====
[32m2022-11-15T10:11:23 | mmf.trainers.mmf_trainer: [0m===== Model =====
[32m2022-11-15T10:11:23 | mmf.trainers.mmf_trainer: [0m===== Model =====
[32m2022-11-15T10:11:23 | mmf.trainers.mmf_trainer: [0mCustomSimpleModel(
  (classifier): Linear(in_features=1, out_features=1, bias=True)
  (losses): Losses(
    (losses): ModuleList(
      (0): MMFLoss(
        (loss_criterion): CrossEntropyLoss(
          (loss_fn): CrossEntropyLoss()
        )
      )
    )
  )
)
[32m2022-11-15T10:11:23 | mmf.trainers.mmf_trainer: [0mCustomSimpleModel(
  (classifier): Linear(in_features=1, out_features=1, bias=True)
  (losses): Losses(
    (losses): ModuleList(
      (0): MMFLoss(
        (loss_criterion): CrossEntropyLoss(
          (loss_fn): CrossEntropyLoss()
        )
      )
    )
  )
)
[32m2022-11-15T10:11:23 | mmf.trainers.mmf_trainer: [0mCustomSimpleModel(
  (classifier): Linear(in_features=1, out_features=1, bias=True)
  (losses): Losses(
    (losses): ModuleList(
      (0): MMFLoss(
        (loss_criterion): CrossEntropyLoss(
          (loss_fn): CrossEntropyLoss()
        )
      )
    )
  )
)
[32m2022-11-15T10:11:23 | mmf.trainers.mmf_trainer: [0mCustomSimpleModel(
  (classifier): Linear(in_features=1, out_features=1, bias=True)
  (losses): Losses(
    (losses): ModuleList(
      (0): MMFLoss(
        (loss_criterion): CrossEntropyLoss(
          (loss_fn): CrossEntropyLoss()
        )
      )
    )
  )
)
[32m2022-11-15T10:11:23 | mmf.utils.general: [0mTotal Parameters: 2. Trained Parameters: 2
[32m2022-11-15T10:11:23 | mmf.utils.general: [0mTotal Parameters: 2. Trained Parameters: 2
[32m2022-11-15T10:11:23 | mmf.utils.general: [0mTotal Parameters: 2. Trained Parameters: 2
[32m2022-11-15T10:11:23 | mmf.utils.general: [0mTotal Parameters: 2. Trained Parameters: 2
[32m2022-11-15T10:11:23 | mmf.trainers.core.training_loop: [0mStarting training...
[32m2022-11-15T10:11:23 | mmf.trainers.core.training_loop: [0mStarting training...
[32m2022-11-15T10:11:23 | mmf.trainers.core.training_loop: [0mStarting training...
[32m2022-11-15T10:11:23 | mmf.trainers.core.training_loop: [0mStarting training...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
_____________________ TestUtilsEnvE2E.test_user_import_e2e _____________________

self = <tests.utils.test_env.TestUtilsEnvE2E testMethod=test_user_import_e2e>

    def test_user_import_e2e(self):
>       self._test_user_import_e2e()

tests/utils/test_env.py:85: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
tests/utils/test_env.py:64: in _test_user_import_e2e
    run(opts)
mmf_cli/run.py:133: in run
    main(configuration, predict=predict)
mmf_cli/run.py:56: in main
    trainer.train()
mmf/trainers/mmf_trainer.py:145: in train
    self.training_loop()
mmf/trainers/core/training_loop.py:33: in training_loop
    self.run_training_epoch()
mmf/trainers/core/training_loop.py:91: in run_training_epoch
    report = self.run_training_batch(batch, num_batches_for_this_update)
mmf/trainers/core/training_loop.py:166: in run_training_batch
    report = self._forward(batch)
mmf/trainers/core/training_loop.py:200: in _forward
    model_output = self.model(prepared_batch)
mmf/models/base_model.py:311: in __call__
    model_output = super().__call__(sample_list, *args, **kwargs)
../mmf_venv/lib/python3.8/site-packages/torch/nn/modules/module.py:889: in _call_impl
    result = self.forward(*input, **kwargs)
tests/data/user_dir/models/simple.py:14: in forward
    return {"scores": self.classifier(sample_list.input)}
../mmf_venv/lib/python3.8/site-packages/torch/nn/modules/module.py:889: in _call_impl
    result = self.forward(*input, **kwargs)
../mmf_venv/lib/python3.8/site-packages/torch/nn/modules/linear.py:94: in forward
    return F.linear(input, self.weight, self.bias)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

input = tensor([[11.],
        [19.],
        [ 1.],
        [ 8.],
        [ 4.],
        [14.],
        [18.],
        [ 6.]], device='cuda:0')
weight = Parameter containing:
tensor([[0.5153]], device='cuda:0', requires_grad=True)
bias = Parameter containing:
tensor([-0.4414], device='cuda:0', requires_grad=True)

    def linear(input: Tensor, weight: Tensor, bias: Optional[Tensor] = None) -> Tensor:
        r"""
        Applies a linear transformation to the incoming data: :math:`y = xA^T + b`.
    
        This operator supports :ref:`TensorFloat32<tf32_on_ampere>`.
    
        Shape:
    
            - Input: :math:`(N, *, in\_features)` N is the batch size, `*` means any number of
              additional dimensions
            - Weight: :math:`(out\_features, in\_features)`
            - Bias: :math:`(out\_features)`
            - Output: :math:`(N, *, out\_features)`
        """
        if has_torch_function_variadic(input, weight):
            return handle_torch_function(linear, (input, weight), input, weight, bias=bias)
>       return torch._C._nn.linear(input, weight, bias)
E       RuntimeError: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#cublasApi_reproducibility

../mmf_venv/lib/python3.8/site-packages/torch/nn/functional.py:1753: RuntimeError
----------------------------- Captured stdout call -----------------------------
[32m2022-11-15T10:11:23 | mmf.utils.env: [0mImporting from /code/mmf/mmf/../tests/data/user_dir
[32m2022-11-15T10:11:23 | mmf.utils.env: [0mImporting from /code/mmf/mmf/../tests/data/user_dir
[32m2022-11-15T10:11:23 | mmf.utils.env: [0mImporting from /code/mmf/mmf/../tests/data/user_dir
[32m2022-11-15T10:11:23 | mmf.utils.env: [0mImporting from /code/mmf/mmf/../tests/data/user_dir
[32m2022-11-15T10:11:23 | mmf.utils.configuration: [0mOverriding option model to simple
[32m2022-11-15T10:11:23 | mmf.utils.configuration: [0mOverriding option model to simple
[32m2022-11-15T10:11:23 | mmf.utils.configuration: [0mOverriding option model to simple
[32m2022-11-15T10:11:23 | mmf.utils.configuration: [0mOverriding option model to simple
[32m2022-11-15T10:11:23 | mmf.utils.configuration: [0mOverriding option run_type to train_val_test
[32m2022-11-15T10:11:23 | mmf.utils.configuration: [0mOverriding option run_type to train_val_test
[32m2022-11-15T10:11:23 | mmf.utils.configuration: [0mOverriding option run_type to train_val_test
[32m2022-11-15T10:11:23 | mmf.utils.configuration: [0mOverriding option run_type to train_val_test
[32m2022-11-15T10:11:23 | mmf.utils.configuration: [0mOverriding option datasets to always_one
[32m2022-11-15T10:11:23 | mmf.utils.configuration: [0mOverriding option datasets to always_one
[32m2022-11-15T10:11:23 | mmf.utils.configuration: [0mOverriding option datasets to always_one
[32m2022-11-15T10:11:23 | mmf.utils.configuration: [0mOverriding option datasets to always_one
[32m2022-11-15T10:11:23 | mmf.utils.configuration: [0mOverriding option config to configs/experiment.yaml
[32m2022-11-15T10:11:23 | mmf.utils.configuration: [0mOverriding option config to configs/experiment.yaml
[32m2022-11-15T10:11:23 | mmf.utils.configuration: [0mOverriding option config to configs/experiment.yaml
[32m2022-11-15T10:11:23 | mmf.utils.configuration: [0mOverriding option config to configs/experiment.yaml
[32m2022-11-15T10:11:23 | mmf.utils.configuration: [0mOverriding option env.user_dir to /code/mmf/mmf/../tests/data/user_dir
[32m2022-11-15T10:11:23 | mmf.utils.configuration: [0mOverriding option env.user_dir to /code/mmf/mmf/../tests/data/user_dir
[32m2022-11-15T10:11:23 | mmf.utils.configuration: [0mOverriding option env.user_dir to /code/mmf/mmf/../tests/data/user_dir
[32m2022-11-15T10:11:23 | mmf.utils.configuration: [0mOverriding option env.user_dir to /code/mmf/mmf/../tests/data/user_dir
[32m2022-11-15T10:11:23 | mmf.utils.configuration: [0mOverriding option training.seed to 1
[32m2022-11-15T10:11:23 | mmf.utils.configuration: [0mOverriding option training.seed to 1
[32m2022-11-15T10:11:23 | mmf.utils.configuration: [0mOverriding option training.seed to 1
[32m2022-11-15T10:11:23 | mmf.utils.configuration: [0mOverriding option training.seed to 1
[32m2022-11-15T10:11:23 | mmf.utils.configuration: [0mOverriding option training.num_workers to 3
[32m2022-11-15T10:11:23 | mmf.utils.configuration: [0mOverriding option training.num_workers to 3
[32m2022-11-15T10:11:23 | mmf.utils.configuration: [0mOverriding option training.num_workers to 3
[32m2022-11-15T10:11:23 | mmf.utils.configuration: [0mOverriding option training.num_workers to 3
[32m2022-11-15T10:11:23 | mmf.utils.configuration: [0mOverriding option training.max_updates to 50
[32m2022-11-15T10:11:23 | mmf.utils.configuration: [0mOverriding option training.max_updates to 50
[32m2022-11-15T10:11:23 | mmf.utils.configuration: [0mOverriding option training.max_updates to 50
[32m2022-11-15T10:11:23 | mmf.utils.configuration: [0mOverriding option training.max_updates to 50
[32m2022-11-15T10:11:23 | mmf.utils.configuration: [0mOverriding option env.save_dir to /tmp/tmprraf_bjl
[32m2022-11-15T10:11:23 | mmf.utils.configuration: [0mOverriding option env.save_dir to /tmp/tmprraf_bjl
[32m2022-11-15T10:11:23 | mmf.utils.configuration: [0mOverriding option env.save_dir to /tmp/tmprraf_bjl
[32m2022-11-15T10:11:23 | mmf.utils.configuration: [0mOverriding option env.save_dir to /tmp/tmprraf_bjl
[32m2022-11-15T10:11:23 | mmf.utils.env: [0mUser dir /code/mmf/mmf/../tests/data/user_dir already imported. Skipping.
[32m2022-11-15T10:11:23 | mmf.utils.env: [0mUser dir /code/mmf/mmf/../tests/data/user_dir already imported. Skipping.
[32m2022-11-15T10:11:23 | mmf.utils.env: [0mUser dir /code/mmf/mmf/../tests/data/user_dir already imported. Skipping.
[32m2022-11-15T10:11:23 | mmf.utils.env: [0mUser dir /code/mmf/mmf/../tests/data/user_dir already imported. Skipping.
[32m2022-11-15T10:11:23 | mmf: [0mLogging to: /tmp/tmprraf_bjl/train.log
[32m2022-11-15T10:11:23 | mmf: [0mLogging to: /tmp/tmprraf_bjl/train.log
[32m2022-11-15T10:11:23 | mmf: [0mLogging to: /tmp/tmprraf_bjl/train.log
[32m2022-11-15T10:11:23 | mmf: [0mLogging to: /tmp/tmprraf_bjl/train.log
[32m2022-11-15T10:11:23 | mmf.utils.general: [0mCUDA Device 0 is: NVIDIA GeForce RTX 3090
[32m2022-11-15T10:11:23 | mmf.utils.general: [0mCUDA Device 0 is: NVIDIA GeForce RTX 3090
[32m2022-11-15T10:11:23 | mmf.utils.general: [0mCUDA Device 0 is: NVIDIA GeForce RTX 3090
[32m2022-11-15T10:11:23 | mmf.utils.general: [0mCUDA Device 0 is: NVIDIA GeForce RTX 3090
[32m2022-11-15T10:11:23 | mmf.trainers.mmf_trainer: [0mLoading datasets
[32m2022-11-15T10:11:23 | mmf.trainers.mmf_trainer: [0mLoading datasets
[32m2022-11-15T10:11:23 | mmf.trainers.mmf_trainer: [0mLoading datasets
[32m2022-11-15T10:11:23 | mmf.trainers.mmf_trainer: [0mLoading datasets
[32m2022-11-15T10:11:23 | mmf.datasets.multi_datamodule: [0mMultitasking disabled by default for single dataset training
[32m2022-11-15T10:11:23 | mmf.datasets.multi_datamodule: [0mMultitasking disabled by default for single dataset training
[32m2022-11-15T10:11:23 | mmf.datasets.multi_datamodule: [0mMultitasking disabled by default for single dataset training
[32m2022-11-15T10:11:23 | mmf.datasets.multi_datamodule: [0mMultitasking disabled by default for single dataset training
[32m2022-11-15T10:11:23 | mmf.datasets.multi_datamodule: [0mMultitasking disabled by default for single dataset training
[32m2022-11-15T10:11:23 | mmf.datasets.multi_datamodule: [0mMultitasking disabled by default for single dataset training
[32m2022-11-15T10:11:23 | mmf.datasets.multi_datamodule: [0mMultitasking disabled by default for single dataset training
[32m2022-11-15T10:11:23 | mmf.datasets.multi_datamodule: [0mMultitasking disabled by default for single dataset training
[32m2022-11-15T10:11:23 | mmf.datasets.multi_datamodule: [0mMultitasking disabled by default for single dataset training
[32m2022-11-15T10:11:23 | mmf.datasets.multi_datamodule: [0mMultitasking disabled by default for single dataset training
[32m2022-11-15T10:11:23 | mmf.datasets.multi_datamodule: [0mMultitasking disabled by default for single dataset training
[32m2022-11-15T10:11:23 | mmf.datasets.multi_datamodule: [0mMultitasking disabled by default for single dataset training
[32m2022-11-15T10:11:23 | mmf.trainers.mmf_trainer: [0mLoading model
[32m2022-11-15T10:11:23 | mmf.trainers.mmf_trainer: [0mLoading model
[32m2022-11-15T10:11:23 | mmf.trainers.mmf_trainer: [0mLoading model
[32m2022-11-15T10:11:23 | mmf.trainers.mmf_trainer: [0mLoading model
[32m2022-11-15T10:11:23 | mmf.trainers.mmf_trainer: [0mLoading optimizer
[32m2022-11-15T10:11:23 | mmf.trainers.mmf_trainer: [0mLoading optimizer
[32m2022-11-15T10:11:23 | mmf.trainers.mmf_trainer: [0mLoading optimizer
[32m2022-11-15T10:11:23 | mmf.trainers.mmf_trainer: [0mLoading optimizer
[32m2022-11-15T10:11:23 | mmf.trainers.mmf_trainer: [0mLoading metrics
[32m2022-11-15T10:11:23 | mmf.trainers.mmf_trainer: [0mLoading metrics
[32m2022-11-15T10:11:23 | mmf.trainers.mmf_trainer: [0mLoading metrics
[32m2022-11-15T10:11:23 | mmf.trainers.mmf_trainer: [0mLoading metrics
[32m2022-11-15T10:11:23 | mmf.trainers.mmf_trainer: [0m===== Model =====
[32m2022-11-15T10:11:23 | mmf.trainers.mmf_trainer: [0m===== Model =====
[32m2022-11-15T10:11:23 | mmf.trainers.mmf_trainer: [0m===== Model =====
[32m2022-11-15T10:11:23 | mmf.trainers.mmf_trainer: [0m===== Model =====
[32m2022-11-15T10:11:23 | mmf.trainers.mmf_trainer: [0mCustomSimpleModel(
  (classifier): Linear(in_features=1, out_features=1, bias=True)
  (losses): Losses(
    (losses): ModuleList(
      (0): MMFLoss(
        (loss_criterion): CrossEntropyLoss(
          (loss_fn): CrossEntropyLoss()
        )
      )
    )
  )
)
[32m2022-11-15T10:11:23 | mmf.trainers.mmf_trainer: [0mCustomSimpleModel(
  (classifier): Linear(in_features=1, out_features=1, bias=True)
  (losses): Losses(
    (losses): ModuleList(
      (0): MMFLoss(
        (loss_criterion): CrossEntropyLoss(
          (loss_fn): CrossEntropyLoss()
        )
      )
    )
  )
)
[32m2022-11-15T10:11:23 | mmf.trainers.mmf_trainer: [0mCustomSimpleModel(
  (classifier): Linear(in_features=1, out_features=1, bias=True)
  (losses): Losses(
    (losses): ModuleList(
      (0): MMFLoss(
        (loss_criterion): CrossEntropyLoss(
          (loss_fn): CrossEntropyLoss()
        )
      )
    )
  )
)
[32m2022-11-15T10:11:23 | mmf.trainers.mmf_trainer: [0mCustomSimpleModel(
  (classifier): Linear(in_features=1, out_features=1, bias=True)
  (losses): Losses(
    (losses): ModuleList(
      (0): MMFLoss(
        (loss_criterion): CrossEntropyLoss(
          (loss_fn): CrossEntropyLoss()
        )
      )
    )
  )
)
[32m2022-11-15T10:11:23 | mmf.utils.general: [0mTotal Parameters: 2. Trained Parameters: 2
[32m2022-11-15T10:11:23 | mmf.utils.general: [0mTotal Parameters: 2. Trained Parameters: 2
[32m2022-11-15T10:11:23 | mmf.utils.general: [0mTotal Parameters: 2. Trained Parameters: 2
[32m2022-11-15T10:11:23 | mmf.utils.general: [0mTotal Parameters: 2. Trained Parameters: 2
[32m2022-11-15T10:11:23 | mmf.trainers.core.training_loop: [0mStarting training...
[32m2022-11-15T10:11:23 | mmf.trainers.core.training_loop: [0mStarting training...
[32m2022-11-15T10:11:23 | mmf.trainers.core.training_loop: [0mStarting training...
[32m2022-11-15T10:11:23 | mmf.trainers.core.training_loop: [0mStarting training...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
=============================== warnings summary ===============================
tests/modules/test_losses.py:34
  /code/mmf/tests/modules/test_losses.py:34: PytestCollectionWarning: cannot collect test class 'TestMSEAndMAELoss' because it has a __init__ constructor (from: tests/modules/test_losses.py)
    class TestMSEAndMAELoss(nn.Module):

tests/utils/test_model.py:7
  /code/mmf/tests/utils/test_model.py:7: PytestCollectionWarning: cannot collect test class 'TestDecoderModel' because it has a __init__ constructor (from: tests/utils/test_model.py)
    class TestDecoderModel(nn.Module):

tests/utils/test_model.py:7
  /code/mmf/tests/utils/test_model.py:7: PytestCollectionWarning: cannot collect test class 'TestDecoderModel' because it has a __init__ constructor (from: tests/utils/test_text.py)
    class TestDecoderModel(nn.Module):

tests/common/test_sample.py::TestFunctions::test_to_device
tests/utils/test_checkpoint.py::TestUtilsCheckpoint::test_checkpoint_scaler_loading
tests/utils/test_checkpoint.py::TestUtilsCheckpoint::test_finalize_and_restore_from_it
tests/utils/test_checkpoint.py::TestUtilsCheckpoint::test_finalize_and_resume_file
tests/utils/test_checkpoint.py::TestUtilsCheckpoint::test_max_to_keep
tests/utils/test_checkpoint.py::TestUtilsCheckpoint::test_pretrained_load
tests/utils/test_checkpoint.py::TestUtilsCheckpoint::test_resets
tests/utils/test_checkpoint.py::TestUtilsCheckpoint::test_save_and_load_state_dict
tests/utils/test_checkpoint.py::TestUtilsCheckpoint::test_zoo_load
  /code/mmf/mmf/common/sample.py:433: UserWarning: You are not returning SampleList/Sample from your dataset. MMF expects you to move your tensors to cuda yourself.
    warnings.warn(

tests/configs/test_configs_for_keys.py: 2 warnings
tests/datasets/test_base_dataset.py: 1 warning
tests/models/test_cnn_lstm.py: 1 warning
tests/models/test_mmbt.py: 3 warnings
tests/models/test_mmf_transformer.py: 4 warnings
tests/models/test_uniter.py: 2 warnings
tests/models/test_vilbert.py: 4 warnings
tests/models/test_vilt.py: 1 warning
tests/models/test_vinvl.py: 2 warnings
tests/models/test_visual_bert.py: 3 warnings
tests/trainers/test_fp16.py: 2 warnings
tests/trainers/test_training_loop.py: 9 warnings
tests/trainers/callbacks/test_logistics.py: 4 warnings
tests/trainers/callbacks/test_lr_scheduler.py: 1 warning
tests/trainers/callbacks/test_user_callback.py: 1 warning
tests/trainers/lightning/test_checkpoint.py: 11 warnings
tests/trainers/lightning/test_grad_accumulate.py: 1 warning
tests/trainers/lightning/test_grad_clipping.py: 1 warning
tests/trainers/lightning/test_logging.py: 1 warning
tests/trainers/lightning/test_loop_conditions.py: 3 warnings
tests/trainers/lightning/test_loss.py: 1 warning
tests/trainers/lightning/test_lr_schedule.py: 2 warnings
tests/trainers/lightning/test_validation.py: 3 warnings
tests/utils/test_checkpoint.py: 9 warnings
tests/utils/test_configuration.py: 1 warning
tests/utils/test_env.py: 3 warnings
tests/utils/test_logger.py: 1 warning
tests/utils/test_text.py: 6 warnings
  /code/mmf_venv/lib/python3.8/site-packages/omegaconf/grammar_visitor.py:253: UserWarning: In the sequence `MMF_USER_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.
    warnings.warn(

tests/configs/test_configs_for_keys.py: 2 warnings
tests/datasets/test_base_dataset.py: 1 warning
tests/models/test_cnn_lstm.py: 1 warning
tests/models/test_mmbt.py: 3 warnings
tests/models/test_mmf_transformer.py: 4 warnings
tests/models/test_uniter.py: 2 warnings
tests/models/test_vilbert.py: 4 warnings
tests/models/test_vilt.py: 1 warning
tests/models/test_vinvl.py: 2 warnings
tests/models/test_visual_bert.py: 3 warnings
tests/trainers/callbacks/test_logistics.py: 3 warnings
tests/trainers/callbacks/test_lr_scheduler.py: 1 warning
tests/trainers/callbacks/test_user_callback.py: 1 warning
tests/trainers/lightning/test_checkpoint.py: 9 warnings
tests/utils/test_checkpoint.py: 9 warnings
tests/utils/test_configuration.py: 1 warning
tests/utils/test_env.py: 3 warnings
tests/utils/test_logger.py: 1 warning
tests/utils/test_text.py: 6 warnings
  /code/mmf_venv/lib/python3.8/site-packages/omegaconf/grammar_visitor.py:253: UserWarning: In the sequence `MMF_LOG_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.
    warnings.warn(

tests/configs/test_configs_for_keys.py: 2 warnings
tests/datasets/test_base_dataset.py: 1 warning
tests/models/test_cnn_lstm.py: 1 warning
tests/models/test_mmbt.py: 3 warnings
tests/models/test_mmf_transformer.py: 4 warnings
tests/models/test_uniter.py: 2 warnings
tests/models/test_vilbert.py: 4 warnings
tests/models/test_vilt.py: 1 warning
tests/models/test_vinvl.py: 2 warnings
tests/models/test_visual_bert.py: 3 warnings
tests/trainers/test_training_loop.py: 7 warnings
tests/trainers/callbacks/test_logistics.py: 3 warnings
tests/trainers/callbacks/test_lr_scheduler.py: 1 warning
tests/trainers/callbacks/test_user_callback.py: 1 warning
tests/trainers/lightning/test_checkpoint.py: 9 warnings
tests/utils/test_checkpoint.py: 9 warnings
tests/utils/test_configuration.py: 1 warning
tests/utils/test_env.py: 3 warnings
tests/utils/test_logger.py: 1 warning
tests/utils/test_text.py: 6 warnings
  /code/mmf_venv/lib/python3.8/site-packages/omegaconf/grammar_visitor.py:253: UserWarning: In the sequence `MMF_REPORT_DIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.
    warnings.warn(

tests/configs/test_configs_for_keys.py: 2 warnings
tests/datasets/test_base_dataset.py: 1 warning
tests/models/test_cnn_lstm.py: 1 warning
tests/models/test_mmbt.py: 3 warnings
tests/models/test_mmf_transformer.py: 4 warnings
tests/models/test_uniter.py: 2 warnings
tests/models/test_vilbert.py: 4 warnings
tests/models/test_vilt.py: 1 warning
tests/models/test_vinvl.py: 2 warnings
tests/models/test_visual_bert.py: 3 warnings
tests/trainers/callbacks/test_logistics.py: 3 warnings
tests/trainers/callbacks/test_lr_scheduler.py: 1 warning
tests/trainers/callbacks/test_user_callback.py: 1 warning
tests/trainers/lightning/test_checkpoint.py: 9 warnings
tests/utils/test_checkpoint.py: 9 warnings
tests/utils/test_configuration.py: 1 warning
tests/utils/test_env.py: 3 warnings
tests/utils/test_logger.py: 1 warning
tests/utils/test_text.py: 6 warnings
  /code/mmf_venv/lib/python3.8/site-packages/omegaconf/grammar_visitor.py:253: UserWarning: In the sequence `MMF_TENSORBOARD_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.
    warnings.warn(

tests/configs/test_configs_for_keys.py: 2 warnings
tests/datasets/test_base_dataset.py: 1 warning
tests/models/test_cnn_lstm.py: 1 warning
tests/models/test_mmbt.py: 3 warnings
tests/models/test_mmf_transformer.py: 4 warnings
tests/models/test_uniter.py: 2 warnings
tests/models/test_vilbert.py: 4 warnings
tests/models/test_vilt.py: 1 warning
tests/models/test_vinvl.py: 2 warnings
tests/models/test_visual_bert.py: 3 warnings
tests/trainers/callbacks/test_logistics.py: 3 warnings
tests/trainers/callbacks/test_lr_scheduler.py: 1 warning
tests/trainers/callbacks/test_user_callback.py: 1 warning
tests/trainers/lightning/test_checkpoint.py: 9 warnings
tests/utils/test_checkpoint.py: 9 warnings
tests/utils/test_configuration.py: 1 warning
tests/utils/test_env.py: 3 warnings
tests/utils/test_logger.py: 1 warning
tests/utils/test_text.py: 6 warnings
  /code/mmf_venv/lib/python3.8/site-packages/omegaconf/grammar_visitor.py:253: UserWarning: In the sequence `MMF_WANDB_LOGDIR,` some elements are missing: please replace them with empty quoted strings. See https://github.com/omry/omegaconf/issues/572 for details.
    warnings.warn(

tests/configs/test_configs_for_keys.py::TestConfigsForKeys::test_dataset_configs_for_keys
  /code/mmf/tests/configs/test_configs_for_keys.py:56: UserWarning: Dataset retrieval has no default configuration defined. Skipping it. Make sure it is intentional
    warnings.warn(

tests/configs/test_configs_for_keys.py::TestConfigsForKeys::test_dataset_configs_for_keys
  /code/mmf/tests/configs/test_configs_for_keys.py:56: UserWarning: Dataset vqa2_ocr has no default configuration defined. Skipping it. Make sure it is intentional
    warnings.warn(

tests/configs/test_configs_for_keys.py::TestConfigsForKeys::test_model_configs_for_keys
  /code/mmf/tests/configs/test_configs_for_keys.py:27: UserWarning: Model multihead has no default configuration defined. Skipping it. Make sure it is intentional
    warnings.warn(

tests/configs/test_configs_for_keys.py::TestConfigsForKeys::test_model_configs_for_keys
  /code/mmf/tests/configs/test_configs_for_keys.py:27: UserWarning: Model top_down_bottom_up has no default configuration defined. Skipping it. Make sure it is intentional
    warnings.warn(

tests/configs/test_configs_for_keys.py::TestConfigsForKeys::test_model_configs_for_keys
  /code/mmf/tests/configs/test_configs_for_keys.py:27: UserWarning: Model simple_model has no default configuration defined. Skipping it. Make sure it is intentional
    warnings.warn(

tests/configs/test_configs_for_keys.py::TestConfigsForKeys::test_model_configs_for_keys
  /code/mmf/tests/configs/test_configs_for_keys.py:27: UserWarning: Model simple_lightning_model has no default configuration defined. Skipping it. Make sure it is intentional
    warnings.warn(

tests/datasets/test_iteration_strategies.py::TestIterationStrategies::test_ratios_strategy
  /code/mmf/mmf/datasets/iteration_strategies.py:289: UserWarning: Either 'datasets' key not in global config or is a empty list. Moving forward with dataset list same as sampling ratios
    warnings.warn(

tests/datasets/test_multi_dataset_loader.py::TestMultiDatasetLoader::test_equal_sampling
tests/datasets/test_multi_dataset_loader.py::TestMultiDatasetLoader::test_proportional_sampling
  /code/mmf/mmf/datasets/multi_dataset_loader.py:37: UserWarning: Empty loaders passed into MultiDataLoader. This can have unintended consequences.
    warnings.warn(

tests/models/test_mmbt.py: 3 warnings
tests/models/test_mmf_transformer.py: 11 warnings
tests/models/test_vilbert.py: 4 warnings
tests/models/test_visual_bert.py: 3 warnings
tests/trainers/lightning/test_checkpoint.py: 2 warnings
  /code/mmf/mmf/models/base_model.py:160: UserWarning: No losses are defined in model configuration. You are expected to return loss in your return dict from forward.
    warnings.warn(

tests/models/test_mmbt.py: 2 warnings
tests/models/test_mmf_transformer.py: 8 warnings
tests/models/test_visual_bert.py: 4 warnings
  /code/mmf_venv/lib/python3.8/site-packages/torch/jit/_recursive.py:545: LightningDeprecationWarning: The `LightningModule.model_size` property was deprecated in v1.5 and will be removed in v1.7. Please use the `pytorch_lightning.utilities.memory.get_model_size_mb`.
    item = getattr(mod, name, None)

tests/models/test_mmbt.py::TestMMBTTorchscript::test_load_save_finetune_model
tests/models/test_mmf_transformer.py::TestMMFTransformerTorchscript::test_finetune_bert_base
tests/models/test_mmf_transformer.py::TestMMFTransformerTorchscript::test_finetune_roberta_base
tests/models/test_mmf_transformer.py::TestMMFTransformerTorchscript::test_finetune_xlmr_base
tests/models/test_mmf_transformer.py::TestMMFTransformerTorchscript::test_load_save_finetune_model
tests/models/test_visual_bert.py::TestVisualBertTorchscript::test_finetune_model
tests/models/test_visual_bert.py::TestVisualBertTorchscript::test_load_save_finetune_model
  /code/mmf_venv/lib/python3.8/site-packages/torch/jit/_recursive.py:620: LightningDeprecationWarning: The `LightningModule.model_size` property was deprecated in v1.5 and will be removed in v1.7. Please use the `pytorch_lightning.utilities.memory.get_model_size_mb`.
    item = getattr(nn_module, name, None)

tests/models/test_mmbt.py::TestMMBTTorchscript::test_load_save_finetune_model
tests/models/test_mmf_transformer.py::TestMMFTransformerTorchscript::test_finetune_bert_base
tests/models/test_mmf_transformer.py::TestMMFTransformerTorchscript::test_finetune_roberta_base
tests/models/test_mmf_transformer.py::TestMMFTransformerTorchscript::test_finetune_xlmr_base
tests/models/test_mmf_transformer.py::TestMMFTransformerTorchscript::test_load_save_finetune_model
tests/models/test_visual_bert.py::TestVisualBertTorchscript::test_finetune_model
tests/models/test_visual_bert.py::TestVisualBertTorchscript::test_load_save_finetune_model
  /code/mmf_venv/lib/python3.8/site-packages/torch/jit/_recursive.py:436: LightningDeprecationWarning: The `LightningModule.model_size` property was deprecated in v1.5 and will be removed in v1.7. Please use the `pytorch_lightning.utilities.memory.get_model_size_mb`.
    item = getattr(nn_module, name, None)

tests/models/test_mmbt.py::TestMMBTTorchscript::test_load_save_finetune_model
tests/models/test_mmf_transformer.py::TestMMFTransformerTorchscript::test_finetune_bert_base
tests/models/test_mmf_transformer.py::TestMMFTransformerTorchscript::test_finetune_roberta_base
tests/models/test_mmf_transformer.py::TestMMFTransformerTorchscript::test_finetune_xlmr_base
tests/models/test_mmf_transformer.py::TestMMFTransformerTorchscript::test_load_save_finetune_model
tests/models/test_visual_bert.py::TestVisualBertTorchscript::test_finetune_model
tests/models/test_visual_bert.py::TestVisualBertTorchscript::test_load_save_finetune_model
  /code/mmf_venv/lib/python3.8/site-packages/torch/jit/_recursive.py:517: LightningDeprecationWarning: The `LightningModule.model_size` property was deprecated in v1.5 and will be removed in v1.7. Please use the `pytorch_lightning.utilities.memory.get_model_size_mb`.
    item = getattr(nn_module, name, None)

tests/models/test_mmf_transformer.py::TestMMFTransformerTorchscript::test_finetune_bert_base
tests/models/test_mmf_transformer.py::TestMMFTransformerTorchscript::test_finetune_roberta_base
tests/models/test_mmf_transformer.py::TestMMFTransformerTorchscript::test_finetune_xlmr_base
tests/models/test_visual_bert.py::TestVisualBertTorchscript::test_finetune_model
  /code/mmf/mmf/modules/losses.py:111: UserWarning: Sample list has not field 'targets', are you sure that your ImDB has labels? you may have wanted to run with evaluation.predict=true
    warnings.warn(

tests/models/test_uniter.py::TestUNITERImageEmbeddings::test_forward
  /code/mmf/tests/models/test_uniter.py:37: DeprecationWarning: Please use assertEqual instead.
    self.assertEquals(list(output.shape), [32, 100, 256])

tests/models/test_uniter.py::TestUniterModel::test_uniter_for_classification
tests/models/test_uniter.py::TestUniterModel::test_uniter_for_pretraining
  /code/mmf/mmf/models/uniter.py:680: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
    bboxs = torch.tensor(sample_list["image_info_0"]["bbox"])[:, :, :4]

tests/models/test_uniter.py: 2 warnings
tests/models/test_vilt.py: 1 warning
tests/models/test_vinvl.py: 2 warnings
tests/models/test_visual_bert.py: 1 warning
tests/trainers/test_eval_loop.py: 1 warning
tests/trainers/test_fp16.py: 2 warnings
tests/trainers/test_training_loop.py: 9 warnings
tests/utils/test_checkpoint.py: 7 warnings
  /code/mmf/mmf/models/base_model.py:324: UserWarning: 'losses' already present in model output. No calculation will be done in base model.
    warnings.warn(

tests/models/test_vilt.py::TestViltPretrained::test_pretrained_model
tests/models/test_vinvl.py::TestVinVLModel::test_vinvl_for_classification
tests/models/test_vinvl.py::TestVinVLModel::test_vinvl_for_pretraining
  /code/mmf/mmf/utils/configuration.py:453: UserWarning: No dataset named 'test' has been registered
    warnings.warn(warning)

tests/models/test_vinvl.py::TestVinVLModel::test_vinvl_for_pretraining
tests/models/transformers/test_heads.py::TestMLMHead::test_head_missing_masked_labels
  /code/mmf/mmf/models/transformers/heads/mlm.py:90: UserWarning: NaN detected in masked_lm_loss. Replacing it with 0.
    warnings.warn("NaN detected in masked_lm_loss. Replacing it with 0.")

tests/modules/test_metrics.py::TestModuleMetrics::test_caption_bleu4
  /code/mmf_venv/lib/python3.8/site-packages/nltk/decorators.py:67: DeprecationWarning: `formatargspec` is deprecated since Python 3.5. Use `signature` and the `Signature` object directly
    signature = inspect.formatargspec(

tests/modules/test_metrics.py::TestModuleMetrics::test_caption_bleu4
  /code/mmf_venv/lib/python3.8/site-packages/nltk/lm/counter.py:15: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.9 it will stop working
    from collections import Sequence, defaultdict

tests/modules/test_metrics.py::TestModuleMetrics::test_caption_bleu4
  /code/mmf_venv/lib/python3.8/site-packages/nltk/lm/vocabulary.py:13: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.9 it will stop working
    from collections import Counter, Iterable

tests/modules/test_metrics.py::TestModuleMetrics::test_macro_f1_precision_recall
tests/modules/test_metrics.py::TestModuleMetrics::test_macro_f1_precision_recall
  /code/mmf_venv/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
    _warn_prf(average, modifier, msg_start, len(result))

tests/trainers/test_device.py::TestDevice::test_current_device
  /code/mmf/mmf/trainers/core/device.py:41: UserWarning: No 'device_id' in 'config', setting to -1. This can cause issues later in training. Ensure that distributed setup is properly initialized.
    warnings.warn(

tests/trainers/test_eval_loop.py: 1 warning
tests/trainers/test_fp16.py: 2 warnings
tests/trainers/test_training_loop.py: 9 warnings
tests/trainers/lightning/test_checkpoint.py: 11 warnings
tests/trainers/lightning/test_grad_accumulate.py: 1 warning
tests/trainers/lightning/test_grad_clipping.py: 1 warning
tests/trainers/lightning/test_logging.py: 1 warning
tests/trainers/lightning/test_loop_conditions.py: 3 warnings
tests/trainers/lightning/test_loss.py: 1 warning
tests/trainers/lightning/test_lr_schedule.py: 2 warnings
tests/trainers/lightning/test_validation.py: 3 warnings
  /code/mmf/mmf/datasets/multi_datamodule.py:62: UserWarning: 'multitasking' config not defined. Disabling any form of multitasking
    warnings.warn(

tests/trainers/test_fp16.py::TestFp16::test_fp16_values
tests/trainers/test_fp16.py::TestFp16::test_fp16_works
tests/trainers/test_training_loop.py::TestTrainingLoop::test_epoch_over_updates
tests/trainers/lightning/test_loop_conditions.py::TestLightningTrainer::test_epoch_over_updates
tests/trainers/lightning/test_loop_conditions.py::TestLightningTrainer::test_fractional_epoch
  /code/mmf/mmf/utils/general.py:348: UserWarning: Both max_updates and max_epochs are specified. Favoring max_epochs: 0.04
    warnings.warn(

tests/trainers/callbacks/test_lr_scheduler.py::TestLogisticsCallback::test_on_update_end
tests/trainers/callbacks/test_user_callback.py::TestUserCallback::test_on_update_end
  /code/mmf/mmf/utils/build.py:473: UserWarning: No type for scheduler specified even though lr_scheduler is True, setting default to 'Pythia'
    warnings.warn(

tests/trainers/callbacks/test_lr_scheduler.py::TestLogisticsCallback::test_on_update_end
tests/trainers/callbacks/test_user_callback.py::TestUserCallback::test_on_update_end
  /code/mmf/mmf/utils/build.py:480: UserWarning: scheduler attributes has no params defined, defaulting to {}.
    warnings.warn("scheduler attributes has no params defined, defaulting to {}.")

tests/trainers/callbacks/test_lr_scheduler.py::TestLogisticsCallback::test_on_update_end
  /code/mmf_venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:129: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
    warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "

tests/trainers/lightning/test_checkpoint.py: 5 warnings
tests/trainers/lightning/test_grad_accumulate.py: 1 warning
tests/trainers/lightning/test_loop_conditions.py: 3 warnings
tests/trainers/lightning/test_lr_schedule.py: 1 warning
tests/trainers/lightning/test_validation.py: 2 warnings
  /code/mmf_venv/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:96: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=0)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.
    rank_zero_deprecation(

tests/trainers/lightning/test_checkpoint.py::TestLightningCheckpoint::test_lightning_checkpoint_interval
tests/trainers/lightning/test_checkpoint.py::TestLightningCheckpoint::test_lightning_checkpoint_structure
tests/trainers/lightning/test_checkpoint.py::TestLightningCheckpoint::test_load_resume_zoo_parity_with_mmf
tests/trainers/lightning/test_checkpoint.py::TestLightningCheckpoint::test_load_trainer_ckpt_number_of_steps
tests/trainers/lightning/test_checkpoint.py::TestLightningCheckpoint::test_load_zoo_with_pretrained_state_mapping_parity_with_mmf
tests/trainers/lightning/test_validation.py::TestLightningTrainerValidation::test_validation
tests/trainers/lightning/test_validation.py::TestLightningTrainerValidation::test_validation_torchmetrics
  /code/mmf_venv/lib/python3.8/site-packages/pytorch_lightning/trainer/configuration_validator.py:340: LightningDeprecationWarning: Base `Callback.on_train_batch_end` hook signature has changed in v1.5. The `dataloader_idx` argument will be removed in v1.7.
    rank_zero_deprecation(

tests/trainers/lightning/test_checkpoint.py::TestLightningCheckpoint::test_lightning_checkpoint_structure
tests/trainers/lightning/test_checkpoint.py::TestLightningCheckpoint::test_load_resume_zoo_parity_with_mmf
tests/trainers/lightning/test_checkpoint.py::TestLightningCheckpoint::test_load_zoo_with_pretrained_state_mapping_parity_with_mmf
tests/trainers/lightning/test_grad_accumulate.py::TestLightningTrainerGradAccumulate::test_grad_accumulate
tests/trainers/lightning/test_loop_conditions.py::TestLightningTrainer::test_epoch_over_updates
tests/trainers/lightning/test_loop_conditions.py::TestLightningTrainer::test_fractional_epoch
tests/trainers/lightning/test_loop_conditions.py::TestLightningTrainer::test_updates
tests/trainers/lightning/test_lr_schedule.py::TestLightningTrainerLRSchedule::test_lr_schedule
  /code/mmf_venv/lib/python3.8/site-packages/pytorch_lightning/trainer/configuration_validator.py:131: UserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.
    rank_zero_warn("You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.")

tests/trainers/lightning/test_checkpoint.py: 431 warnings
  /code/mmf/tests/trainers/lightning/test_checkpoint.py:56: DeprecationWarning: Please use assertAlmostEqual instead.
    self.assertAlmostEquals(obj1.mean().item(), obj2.mean().item(), 2)

tests/trainers/lightning/test_checkpoint.py::TestLightningCheckpoint::test_load_resume_zoo_parity_with_mmf
  /code/mmf_venv/lib/python3.8/site-packages/pytorch_lightning/core/saving.py:213: UserWarning: Found keys that are in the model state dict but not in the checkpoint: ['base.encoder.embeddings.position_ids']
    rank_zero_warn(

tests/trainers/lightning/test_checkpoint.py::TestLightningCheckpoint::test_load_resume_zoo_parity_with_mmf
  /code/mmf_venv/lib/python3.8/site-packages/pytorch_lightning/core/saving.py:217: UserWarning: Found keys that are not in the model state dict but in the checkpoint: ['hyper_parameters']
    rank_zero_warn(

tests/trainers/lightning/test_checkpoint.py::TestLightningCheckpoint::test_load_zoo_with_pretrained_state_mapping_parity_with_mmf
  /code/mmf_venv/lib/python3.8/site-packages/pytorch_lightning/core/saving.py:213: UserWarning: Found keys that are in the model state dict but not in the checkpoint: ['base.encoder.embeddings.position_ids', 'base.encoder.encoder.layer.0.attention.self.query.weight', 'base.encoder.encoder.layer.0.attention.self.query.bias', 'base.encoder.encoder.layer.0.attention.self.key.weight', 'base.encoder.encoder.layer.0.attention.self.key.bias', 'base.encoder.encoder.layer.0.attention.self.value.weight', 'base.encoder.encoder.layer.0.attention.self.value.bias', 'base.encoder.encoder.layer.0.attention.output.dense.weight', 'base.encoder.encoder.layer.0.attention.output.dense.bias', 'base.encoder.encoder.layer.0.attention.output.LayerNorm.weight', 'base.encoder.encoder.layer.0.attention.output.LayerNorm.bias', 'base.encoder.encoder.layer.0.intermediate.dense.weight', 'base.encoder.encoder.layer.0.intermediate.dense.bias', 'base.encoder.encoder.layer.0.output.dense.weight', 'base.encoder.encoder.layer.0.output.dense.bias', 'base.encoder.encoder.layer.0.output.LayerNorm.weight', 'base.encoder.encoder.layer.0.output.LayerNorm.bias', 'base.encoder.encoder.layer.1.attention.self.query.weight', 'base.encoder.encoder.layer.1.attention.self.query.bias', 'base.encoder.encoder.layer.1.attention.self.key.weight', 'base.encoder.encoder.layer.1.attention.self.key.bias', 'base.encoder.encoder.layer.1.attention.self.value.weight', 'base.encoder.encoder.layer.1.attention.self.value.bias', 'base.encoder.encoder.layer.1.attention.output.dense.weight', 'base.encoder.encoder.layer.1.attention.output.dense.bias', 'base.encoder.encoder.layer.1.attention.output.LayerNorm.weight', 'base.encoder.encoder.layer.1.attention.output.LayerNorm.bias', 'base.encoder.encoder.layer.1.intermediate.dense.weight', 'base.encoder.encoder.layer.1.intermediate.dense.bias', 'base.encoder.encoder.layer.1.output.dense.weight', 'base.encoder.encoder.layer.1.output.dense.bias', 'base.encoder.encoder.layer.1.output.LayerNorm.weight', 'base.encoder.encoder.layer.1.output.LayerNorm.bias', 'base.encoder.encoder.layer.2.attention.self.query.weight', 'base.encoder.encoder.layer.2.attention.self.query.bias', 'base.encoder.encoder.layer.2.attention.self.key.weight', 'base.encoder.encoder.layer.2.attention.self.key.bias', 'base.encoder.encoder.layer.2.attention.self.value.weight', 'base.encoder.encoder.layer.2.attention.self.value.bias', 'base.encoder.encoder.layer.2.attention.output.dense.weight', 'base.encoder.encoder.layer.2.attention.output.dense.bias', 'base.encoder.encoder.layer.2.attention.output.LayerNorm.weight', 'base.encoder.encoder.layer.2.attention.output.LayerNorm.bias', 'base.encoder.encoder.layer.2.intermediate.dense.weight', 'base.encoder.encoder.layer.2.intermediate.dense.bias', 'base.encoder.encoder.layer.2.output.dense.weight', 'base.encoder.encoder.layer.2.output.dense.bias', 'base.encoder.encoder.layer.2.output.LayerNorm.weight', 'base.encoder.encoder.layer.2.output.LayerNorm.bias', 'base.encoder.encoder.layer.3.attention.self.query.weight', 'base.encoder.encoder.layer.3.attention.self.query.bias', 'base.encoder.encoder.layer.3.attention.self.key.weight', 'base.encoder.encoder.layer.3.attention.self.key.bias', 'base.encoder.encoder.layer.3.attention.self.value.weight', 'base.encoder.encoder.layer.3.attention.self.value.bias', 'base.encoder.encoder.layer.3.attention.output.dense.weight', 'base.encoder.encoder.layer.3.attention.output.dense.bias', 'base.encoder.encoder.layer.3.attention.output.LayerNorm.weight', 'base.encoder.encoder.layer.3.attention.output.LayerNorm.bias', 'base.encoder.encoder.layer.3.intermediate.dense.weight', 'base.encoder.encoder.layer.3.intermediate.dense.bias', 'base.encoder.encoder.layer.3.output.dense.weight', 'base.encoder.encoder.layer.3.output.dense.bias', 'base.encoder.encoder.layer.3.output.LayerNorm.weight', 'base.encoder.encoder.layer.3.output.LayerNorm.bias', 'base.encoder.encoder.layer.4.attention.self.query.weight', 'base.encoder.encoder.layer.4.attention.self.query.bias', 'base.encoder.encoder.layer.4.attention.self.key.weight', 'base.encoder.encoder.layer.4.attention.self.key.bias', 'base.encoder.encoder.layer.4.attention.self.value.weight', 'base.encoder.encoder.layer.4.attention.self.value.bias', 'base.encoder.encoder.layer.4.attention.output.dense.weight', 'base.encoder.encoder.layer.4.attention.output.dense.bias', 'base.encoder.encoder.layer.4.attention.output.LayerNorm.weight', 'base.encoder.encoder.layer.4.attention.output.LayerNorm.bias', 'base.encoder.encoder.layer.4.intermediate.dense.weight', 'base.encoder.encoder.layer.4.intermediate.dense.bias', 'base.encoder.encoder.layer.4.output.dense.weight', 'base.encoder.encoder.layer.4.output.dense.bias', 'base.encoder.encoder.layer.4.output.LayerNorm.weight', 'base.encoder.encoder.layer.4.output.LayerNorm.bias', 'base.encoder.encoder.layer.5.attention.self.query.weight', 'base.encoder.encoder.layer.5.attention.self.query.bias', 'base.encoder.encoder.layer.5.attention.self.key.weight', 'base.encoder.encoder.layer.5.attention.self.key.bias', 'base.encoder.encoder.layer.5.attention.self.value.weight', 'base.encoder.encoder.layer.5.attention.self.value.bias', 'base.encoder.encoder.layer.5.attention.output.dense.weight', 'base.encoder.encoder.layer.5.attention.output.dense.bias', 'base.encoder.encoder.layer.5.attention.output.LayerNorm.weight', 'base.encoder.encoder.layer.5.attention.output.LayerNorm.bias', 'base.encoder.encoder.layer.5.intermediate.dense.weight', 'base.encoder.encoder.layer.5.intermediate.dense.bias', 'base.encoder.encoder.layer.5.output.dense.weight', 'base.encoder.encoder.layer.5.output.dense.bias', 'base.encoder.encoder.layer.5.output.LayerNorm.weight', 'base.encoder.encoder.layer.5.output.LayerNorm.bias', 'base.encoder.encoder.layer.6.attention.self.query.weight', 'base.encoder.encoder.layer.6.attention.self.query.bias', 'base.encoder.encoder.layer.6.attention.self.key.weight', 'base.encoder.encoder.layer.6.attention.self.key.bias', 'base.encoder.encoder.layer.6.attention.self.value.weight', 'base.encoder.encoder.layer.6.attention.self.value.bias', 'base.encoder.encoder.layer.6.attention.output.dense.weight', 'base.encoder.encoder.layer.6.attention.output.dense.bias', 'base.encoder.encoder.layer.6.attention.output.LayerNorm.weight', 'base.encoder.encoder.layer.6.attention.output.LayerNorm.bias', 'base.encoder.encoder.layer.6.intermediate.dense.weight', 'base.encoder.encoder.layer.6.intermediate.dense.bias', 'base.encoder.encoder.layer.6.output.dense.weight', 'base.encoder.encoder.layer.6.output.dense.bias', 'base.encoder.encoder.layer.6.output.LayerNorm.weight', 'base.encoder.encoder.layer.6.output.LayerNorm.bias', 'base.encoder.encoder.layer.7.attention.self.query.weight', 'base.encoder.encoder.layer.7.attention.self.query.bias', 'base.encoder.encoder.layer.7.attention.self.key.weight', 'base.encoder.encoder.layer.7.attention.self.key.bias', 'base.encoder.encoder.layer.7.attention.self.value.weight', 'base.encoder.encoder.layer.7.attention.self.value.bias', 'base.encoder.encoder.layer.7.attention.output.dense.weight', 'base.encoder.encoder.layer.7.attention.output.dense.bias', 'base.encoder.encoder.layer.7.attention.output.LayerNorm.weight', 'base.encoder.encoder.layer.7.attention.output.LayerNorm.bias', 'base.encoder.encoder.layer.7.intermediate.dense.weight', 'base.encoder.encoder.layer.7.intermediate.dense.bias', 'base.encoder.encoder.layer.7.output.dense.weight', 'base.encoder.encoder.layer.7.output.dense.bias', 'base.encoder.encoder.layer.7.output.LayerNorm.weight', 'base.encoder.encoder.layer.7.output.LayerNorm.bias', 'base.encoder.encoder.layer.8.attention.self.query.weight', 'base.encoder.encoder.layer.8.attention.self.query.bias', 'base.encoder.encoder.layer.8.attention.self.key.weight', 'base.encoder.encoder.layer.8.attention.self.key.bias', 'base.encoder.encoder.layer.8.attention.self.value.weight', 'base.encoder.encoder.layer.8.attention.self.value.bias', 'base.encoder.encoder.layer.8.attention.output.dense.weight', 'base.encoder.encoder.layer.8.attention.output.dense.bias', 'base.encoder.encoder.layer.8.attention.output.LayerNorm.weight', 'base.encoder.encoder.layer.8.attention.output.LayerNorm.bias', 'base.encoder.encoder.layer.8.intermediate.dense.weight', 'base.encoder.encoder.layer.8.intermediate.dense.bias', 'base.encoder.encoder.layer.8.output.dense.weight', 'base.encoder.encoder.layer.8.output.dense.bias', 'base.encoder.encoder.layer.8.output.LayerNorm.weight', 'base.encoder.encoder.layer.8.output.LayerNorm.bias', 'base.encoder.encoder.layer.9.attention.self.query.weight', 'base.encoder.encoder.layer.9.attention.self.query.bias', 'base.encoder.encoder.layer.9.attention.self.key.weight', 'base.encoder.encoder.layer.9.attention.self.key.bias', 'base.encoder.encoder.layer.9.attention.self.value.weight', 'base.encoder.encoder.layer.9.attention.self.value.bias', 'base.encoder.encoder.layer.9.attention.output.dense.weight', 'base.encoder.encoder.layer.9.attention.output.dense.bias', 'base.encoder.encoder.layer.9.attention.output.LayerNorm.weight', 'base.encoder.encoder.layer.9.attention.output.LayerNorm.bias', 'base.encoder.encoder.layer.9.intermediate.dense.weight', 'base.encoder.encoder.layer.9.intermediate.dense.bias', 'base.encoder.encoder.layer.9.output.dense.weight', 'base.encoder.encoder.layer.9.output.dense.bias', 'base.encoder.encoder.layer.9.output.LayerNorm.weight', 'base.encoder.encoder.layer.9.output.LayerNorm.bias', 'base.encoder.encoder.layer.10.attention.self.query.weight', 'base.encoder.encoder.layer.10.attention.self.query.bias', 'base.encoder.encoder.layer.10.attention.self.key.weight', 'base.encoder.encoder.layer.10.attention.self.key.bias', 'base.encoder.encoder.layer.10.attention.self.value.weight', 'base.encoder.encoder.layer.10.attention.self.value.bias', 'base.encoder.encoder.layer.10.attention.output.dense.weight', 'base.encoder.encoder.layer.10.attention.output.dense.bias', 'base.encoder.encoder.layer.10.attention.output.LayerNorm.weight', 'base.encoder.encoder.layer.10.attention.output.LayerNorm.bias', 'base.encoder.encoder.layer.10.intermediate.dense.weight', 'base.encoder.encoder.layer.10.intermediate.dense.bias', 'base.encoder.encoder.layer.10.output.dense.weight', 'base.encoder.encoder.layer.10.output.dense.bias', 'base.encoder.encoder.layer.10.output.LayerNorm.weight', 'base.encoder.encoder.layer.10.output.LayerNorm.bias', 'base.encoder.encoder.layer.11.attention.self.query.weight', 'base.encoder.encoder.layer.11.attention.self.query.bias', 'base.encoder.encoder.layer.11.attention.self.key.weight', 'base.encoder.encoder.layer.11.attention.self.key.bias', 'base.encoder.encoder.layer.11.attention.self.value.weight', 'base.encoder.encoder.layer.11.attention.self.value.bias', 'base.encoder.encoder.layer.11.attention.output.dense.weight', 'base.encoder.encoder.layer.11.attention.output.dense.bias', 'base.encoder.encoder.layer.11.attention.output.LayerNorm.weight', 'base.encoder.encoder.layer.11.attention.output.LayerNorm.bias', 'base.encoder.encoder.layer.11.intermediate.dense.weight', 'base.encoder.encoder.layer.11.intermediate.dense.bias', 'base.encoder.encoder.layer.11.output.dense.weight', 'base.encoder.encoder.layer.11.output.dense.bias', 'base.encoder.encoder.layer.11.output.LayerNorm.weight', 'base.encoder.encoder.layer.11.output.LayerNorm.bias', 'base.encoder.pooler.dense.weight', 'base.encoder.pooler.dense.bias', 'classifier.layers.0.weight', 'classifier.layers.0.bias', 'classifier.layers.1.weight', 'classifier.layers.1.bias', 'classifier.layers.1.running_mean', 'classifier.layers.1.running_var', 'classifier.layers.4.weight', 'classifier.layers.4.bias', 'classifier.layers.5.weight', 'classifier.layers.5.bias', 'classifier.layers.5.running_mean', 'classifier.layers.5.running_var', 'classifier.layers.8.weight', 'classifier.layers.8.bias']
    rank_zero_warn(

tests/utils/test_checkpoint.py::TestUtilsCheckpoint::test_finalize_and_resume_file
tests/utils/test_checkpoint.py::TestUtilsCheckpoint::test_zoo_load
  /code/mmf/mmf/utils/checkpoint.py:344: UserWarning: 'optimizer' key is not present in the checkpoint asked to be loaded. Skipping.
    warnings.warn(

tests/utils/test_checkpoint.py::TestUtilsCheckpoint::test_finalize_and_resume_file
tests/utils/test_checkpoint.py::TestUtilsCheckpoint::test_zoo_load
  /code/mmf/mmf/utils/checkpoint.py:393: UserWarning: 'lr_scheduler' key is not present in the checkpoint asked to be loaded. Setting lr_scheduler's last_epoch to current_iteration.
    warnings.warn(

tests/utils/test_patch.py::TestClass::test_function
  /code/mmf_venv/lib/python3.8/site-packages/_pytest/python.py:199: PytestReturnNotNoneWarning: Expected None, but tests/utils/test_patch.py::TestClass::test_function returned True, which will be an error in a future version of pytest.  Did you mean to use `assert` instead of `return`?
    warnings.warn(

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
LEAKED 3348.47265625MB in tests/common/test_report.py::TestReport::test_to_device
LEAKED 10.203125MB in tests/configs/test_configs_for_keys.py::TestConfigsForKeys::test_dataset_configs_for_keys
LEAKED 12.25MB in tests/datasets/test_bert_processors.py::TestBERTProcessors::test_bert_tokenizer
LEAKED 504.50390625MB in tests/models/test_albef.py::TestAlbefEncoders::test_vision_transformer
LEAKED 100.33203125MB in tests/models/test_cnn_lstm.py::TestModelCNNLSTM::test_forward
LEAKED 1044.6171875MB in tests/models/test_mmbt.py::TestMMBTTorchscript::test_finetune_model
LEAKED 52.64453125MB in tests/models/test_mmbt.py::TestMMBTTorchscript::test_load_save_finetune_model
LEAKED 254.5546875MB in tests/models/test_mmf_transformer.py::TestMMFTransformerTorchscript::test_finetune_bert_base
LEAKED 121.375MB in tests/models/test_mmf_transformer.py::TestMMFTransformerTorchscript::test_load_save_finetune_model
LEAKED 123.48828125MB in tests/models/test_mmf_transformer.py::TestMMFTransformerConfig::test_mmft_from_build_model
LEAKED 283.37890625MB in tests/models/test_uniter.py::TestUniterModel::test_uniter_for_pretraining
LEAKED 245.7890625MB in tests/models/test_vilbert.py::TestViLBertTorchscript::test_load_save_finetune_model
LEAKED 98.33984375MB in tests/models/test_vilbert.py::TestViLBertTorchscript::test_load_save_pretrain_model
LEAKED 1205.25390625MB in tests/models/interfaces/test_interfaces.py::TestModelInterfaces::test_mmbt_hm_interface
LEAKED 14.703125MB in tests/modules/test_encoders.py::TestEncoders::test_r2plus1d18_video_encoder
LEAKED 2732.921875MB in tests/modules/test_vit.py::TestViT::test_model_static_constructor_from_config
=========================== short test summary info ============================
FAILED tests/models/test_cnn_lstm.py::TestModelCNNLSTM::test_forward - Assert...
FAILED tests/models/test_uniter.py::TestUniterWithHeads::test_uniter_for_classification
FAILED tests/models/test_uniter.py::TestUniterWithHeads::test_uniter_for_pretraining
FAILED tests/models/test_vinvl.py::TestVinVLBase::test_forward - RuntimeError...
FAILED tests/models/test_vinvl.py::TestVinVLForClassificationAndPretraining::test_classification_forward
FAILED tests/models/test_vinvl.py::TestVinVLForClassificationAndPretraining::test_pretraining_forward
FAILED tests/modules/test_layers.py::TestModuleLayers::test_bert_classifier_head
FAILED tests/modules/test_layers.py::TestModuleLayers::test_mlp - AssertionEr...
FAILED tests/trainers/test_eval_loop.py::TestEvalLoop::test_eval_loop - TypeE...
FAILED tests/trainers/test_training_loop.py::TestTrainingLoop::test_update_frequency_reporting
FAILED tests/trainers/lightning/test_checkpoint.py::TestLightningCheckpoint::test_lightning_checkpoint_interval
FAILED tests/trainers/lightning/test_checkpoint.py::TestLightningCheckpoint::test_lightning_checkpoint_structure
FAILED tests/trainers/lightning/test_checkpoint.py::TestLightningCheckpoint::test_load_mmf_trainer_checkpoint_in_lightning
FAILED tests/trainers/lightning/test_checkpoint.py::TestLightningCheckpoint::test_load_resume_best_parity_with_mmf
FAILED tests/trainers/lightning/test_checkpoint.py::TestLightningCheckpoint::test_load_resume_ignore_resume_zoo
FAILED tests/trainers/lightning/test_checkpoint.py::TestLightningCheckpoint::test_load_resume_parity_with_mmf
FAILED tests/trainers/lightning/test_checkpoint.py::TestLightningCheckpoint::test_load_trainer_ckpt_number_of_steps
FAILED tests/trainers/lightning/test_checkpoint.py::TestLightningCheckpoint::test_load_trainer_resume_parity_with_mmf
FAILED tests/trainers/lightning/test_checkpoint.py::TestLightningCheckpoint::test_trainer_save_current_parity_with_mmf
FAILED tests/trainers/lightning/test_grad_accumulate.py::TestLightningTrainerGradAccumulate::test_grad_accumulate
FAILED tests/trainers/lightning/test_grad_clipping.py::TestLightningTrainerGradClipping::test_grad_clipping_and_parity_to_mmf
FAILED tests/trainers/lightning/test_logging.py::TestLightningTrainerLogging::test_tensorboard_logging_parity
FAILED tests/trainers/lightning/test_loop_conditions.py::TestLightningTrainer::test_epoch_over_updates
FAILED tests/trainers/lightning/test_loop_conditions.py::TestLightningTrainer::test_fractional_epoch
FAILED tests/trainers/lightning/test_loop_conditions.py::TestLightningTrainer::test_updates
FAILED tests/trainers/lightning/test_loss.py::TestLightningTrainerLoss::test_loss_computation_parity_with_mmf_trainer
FAILED tests/trainers/lightning/test_lr_schedule.py::TestLightningTrainerLRSchedule::test_lr_schedule
FAILED tests/trainers/lightning/test_lr_schedule.py::TestLightningTrainerLRSchedule::test_lr_schedule_compared_to_mmf_is_same
FAILED tests/trainers/lightning/test_validation.py::TestLightningTrainerValidation::test_validation
FAILED tests/trainers/lightning/test_validation.py::TestLightningTrainerValidation::test_validation_parity
FAILED tests/trainers/lightning/test_validation.py::TestLightningTrainerValidation::test_validation_torchmetrics
FAILED tests/utils/test_checkpoint.py::TestUtilsCheckpoint::test_checkpoint_scaler_loading
FAILED tests/utils/test_env.py::TestUtilsEnvE2E::test_cpu_evaluation_e2e - Ru...
FAILED tests/utils/test_env.py::TestUtilsEnvE2E::test_user_import_e2e - Runti...
===== 34 failed, 224 passed, 6 skipped, 951 warnings in 688.09s (0:11:28) ======
