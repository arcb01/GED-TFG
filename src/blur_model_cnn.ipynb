{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18877da6-7bff-4734-bbe5-6eb67a074511",
   "metadata": {},
   "source": [
    "# Basline blur classifier model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa07d25-767a-497b-a249-f9d633a234f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function \n",
    "from __future__ import division\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import io\n",
    "print(\"PyTorch Version: \",torch.__version__)\n",
    "print(\"Torchvision Version: \",torchvision.__version__)\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Detect if we have a GPU available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1d567f-49ce-4545-bfcd-632d934a88b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNet input size\n",
    "input_size = (224,224)\n",
    "\n",
    "# Just normalization\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize(input_size),\n",
    "        transforms.RandomCrop(input_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.4853, 0.4290, 0.3761], [0.1348, 0.1184, 0.1311])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(input_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.4275, 0.4184, 0.4037], [0.1419, 0.1179, 0.1273])\n",
    "    ]),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada87a0d-5a2a-4ec0-8209-f0f21bf27f7a",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e585d6d-8e90-4bc2-abd8-c48efb656e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlurDataset(Dataset):\n",
    "\n",
    "    def __init__(self, root_dir, imgs_list, transform=None):\n",
    "\n",
    "        self.root_dir = root_dir\n",
    "        self.imgs_list = imgs_list\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        # Blur kaggle dataset case\n",
    "        if type(self.imgs_list[idx]) == str:\n",
    "            img_name = self.imgs_list[idx]\n",
    "            img_tag = self.imgs_list[idx].split(\"/\")[-1].split(\"_\")[-1].split(\".\")[0]\n",
    "            if img_tag == \"S\":\n",
    "                img_label = 0\n",
    "            else:\n",
    "                img_label = 1\n",
    "         # VizWiz dataset case\n",
    "        else:\n",
    "            img_name = os.path.join(self.root_dir, self.imgs_list[idx][0])\n",
    "            img_label = self.imgs_list[idx][1]\n",
    "\n",
    "        image = Image.open(img_name)\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(image)\n",
    "\n",
    "        return sample, img_label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6055f965-ea7c-492e-957b-26f10f31816f",
   "metadata": {},
   "source": [
    "Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91956686-0457-47c1-b551-d541a818486c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/mixed_vwbk_dataset.json', encoding='UTF-8') as m_json_file:\n",
    "    data = json.load(m_json_file)\n",
    "    m_train_data = data[\"train\"]\n",
    "    m_val_data = data[\"val\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bfdfe6-e6b3-4d14-a2c5-019aac7d046c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_dataset = BlurDataset('/media/arnau/PEN/TFG/train/', m_train_data, data_transforms[\"train\"])\n",
    "val_dataset = BlurDataset('/media/arnau/PEN/TFG/val/', m_val_data, data_transforms[\"val\"])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "dataloaders_dict = {\"train\": train_loader, \"val\": val_loader}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa5cbfd-e767-4aab-8628-d119b64b60ae",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d925ced-1b26-4197-b51e-a2603e2aa8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, criterion, optimizer, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    acc_history = {\"train\": [], \"val\": []}\n",
    "    losses = {\"train\": [], \"val\": []}\n",
    "\n",
    "    # we will keep a copy of the best weights so far according to validation accuracy\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # Get model outputs and calculate loss\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    losses[phase].append(loss.cpu().detach().numpy())\n",
    "\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            \n",
    "            acc_history[phase].append(epoch_acc)\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, acc_history, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea85417-9f5e-4793-9baf-386032cf0bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model(num_classes):\n",
    "    # vgg16\n",
    "    #model = models.vgg16(pretrained=True)\n",
    "    model = models.convnext_tiny(pretrained=True)\n",
    "    \n",
    "    model.fc = nn.Linear(512,num_classes)# YOUR CODE HERE!\n",
    "    \n",
    "    input_size = 224\n",
    "        \n",
    "    return model, input_size\n",
    "\n",
    "\n",
    "# Number of classes in the dataset\n",
    "num_classes = 2\n",
    "\n",
    "# Initialize the model\n",
    "model, input_size = initialize_model(num_classes)\n",
    "\n",
    "# Print the model we just instantiated\n",
    "print(model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a2a47b-310d-4fa7-b643-dd3ab44dc49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send the model to GPU\n",
    "model = model.to(device)\n",
    "\n",
    "# Setup the loss fxn\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Number of epochs to train for \n",
    "num_epochs = 5\n",
    "\n",
    "lr = 2.114132886588899e-05\n",
    "wd = 2.2379504272114866e-10\n",
    "optimizer_ft = optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "\n",
    "# Train and evaluate\n",
    "model, hist, losses = train_model(model, dataloaders_dict, criterion, optimizer_ft, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba446e8c-87e8-4d65-88ad-a1a6c02ce7d1",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89920a9-5b5e-4361-b01b-a9f26c2b2961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the losses and accuracies\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "ax1.set_title(\"Loss\")\n",
    "ax1.plot(losses[\"train\"], label=\"training loss\")\n",
    "ax1.plot(losses[\"val\"], label=\"validation loss\")\n",
    "ax1.legend()\n",
    "\n",
    "ax2.set_title(\"Accuracy\")\n",
    "ax2.plot([x.cpu().numpy() for x in hist[\"train\"]],label=\"training accuracy\")\n",
    "ax2.plot([x.cpu().numpy() for x in hist[\"val\"]],label=\"val accuracy\")\n",
    "ax2.set_ylim([0, 1])\n",
    "ax2.legend()\n",
    "\n",
    "plt.show()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb1ae39-7f3e-4212-9fe3-2d7b484f489d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "def evaluate(model, loss_fn, test_loader, device):\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    for inputs, targets in test_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        test_loss += loss.item() * inputs.size(0)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += targets.size(0)\n",
    "        correct += (predicted == targets).sum().item()\n",
    "        y_true += targets.cpu().numpy().tolist()\n",
    "        y_pred += predicted.cpu().numpy().tolist()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_acc = correct / total\n",
    "    test_f1 = f1_score(y_true, y_pred, average='macro')\n",
    "    test_precision = precision_score(y_true, y_pred, average='macro')\n",
    "    test_recall = recall_score(y_true, y_pred, average='macro')\n",
    "\n",
    "    print(f\"Test Loss: {test_loss:.4f}\")\n",
    "    print(f\"Test Accuracy: {test_acc * 100:.2f}%\")\n",
    "    print(f\"Test Precision: {test_precision * 100:.2f}%\")\n",
    "    print(f\"Test Recall: {test_recall * 100:.2f}%\")\n",
    "    print(f\"Test F1 Score: {test_f1 * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26059752-a584-4e11-b2bf-ffb2a5c58283",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "test_dataset = BlurDataset('/media/arnau/PEN/TFG/val/', data[\"test\"], data_transforms[\"val\"])\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, pin_memory=True)\n",
    "\n",
    "evaluate(model, loss_fn, test_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71eac61-f307-40e4-bbdf-8764fe628714",
   "metadata": {},
   "source": [
    "## Hyperparemeter tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1be26a2-b466-494d-9e02-30a463023172",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision.transforms import ToTensor, Normalize\n",
    "\n",
    "\n",
    "# Define the objective function to optimize\n",
    "def objective(trial):\n",
    "    # Define the hyperparameters to optimize\n",
    "    lr = trial.suggest_loguniform('lr', 1e-5, 1e-1)\n",
    "    weight_decay = trial.suggest_loguniform('weight_decay', 1e-10, 1e-3)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [8, 16, 32, 64])\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "    \n",
    "    # Define the ConvNetXt model with the hyperparameters\n",
    "    model = models.convnext_tiny(pretrained=True).to(device)\n",
    "    \n",
    "    # Define the optimizer and loss function\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    acc_history = {\"train\": [], \"val\": []}\n",
    "    losses = {\"train\": [], \"val\": []}\n",
    "\n",
    "    # we will keep a copy of the best weights so far according to validation accuracy\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    # Train the model\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders_dict[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # Get model outputs and calculate loss\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    losses[phase].append(loss.cpu().detach().numpy())\n",
    "\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders_dict[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders_dict[phase].dataset)\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                \n",
    "            acc_history[phase].append(epoch_acc)\n",
    "            losses[phase].append(epoch_loss)\n",
    "    \n",
    "    \n",
    "    return losses[\"val\"][-1]\n",
    "\n",
    "# Create the Optuna study and optimize the objective function\n",
    "study = optuna.create_study(direction='minimize', pruner=optuna.pruners.MedianPruner())\n",
    "study.optimize(objective, n_trials=50)\n",
    "print('Best trial:', study.best_trial.params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfg env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
