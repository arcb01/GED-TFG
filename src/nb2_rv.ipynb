{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d1e066d-7672-4872-a4ba-40d03c48494b",
   "metadata": {},
   "source": [
    "# TFG: Notebook 2: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92474496-a753-4bf5-bf07-892567ca4ab9",
   "metadata": {},
   "source": [
    "Questions to answer:\n",
    "   - How many images have text?\n",
    "       - Out of these, how many are blurred?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11deeb1a-bcef-4dc9-ac84-d3691a92ff12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy  as np\n",
    "import cv2\n",
    "import random\n",
    "import json\n",
    "import pprint\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea01b517-c57a-47c6-81dd-b0dab3989365",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Blurred\n",
    "Out of the images containing text, find how many of those are blurried\n",
    "\n",
    "<u>**2 approaches:**</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ad5c6f-3910-4643-abe6-457eb20f042d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Automatic approach\n",
    "Given a threshold, find which images are blurried using OpenCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c85d72c-078f-458f-8444-b6a65982f836",
   "metadata": {},
   "outputs": [],
   "source": [
    "def blur_detector(img_path, thr=100):\n",
    "    image = cv2.imread(img_path)\n",
    "    b = cv2.Laplacian(image, cv2.CV_64F).var()\n",
    "    if b <= thr: return img_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d4605e-bfc3-406c-926a-e80f413f0ba4",
   "metadata": {},
   "source": [
    "**Testing threshold manually**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f55e1b6-bdcb-43c2-854d-6650954c6220",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_thr = []\n",
    "\n",
    "for img in imgs_w_txt:\n",
    "    blrd_img = blur_detector(source + img, thr=70)\n",
    "    if blrd_img:\n",
    "        t_thr.append(blrd_img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e1d795-99c1-420d-bf70-c2caf84cc87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = random.randint(0, len(t_thr))\n",
    "random_img = t_thr[r]\n",
    "plt.figure(figsize=(4, 4))\n",
    "img = cv2.imread(random_img)\n",
    "\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33a8d83-eb90-40db-85eb-15e8760faecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "blurred_imgs_labels = []\n",
    "\n",
    "# For every img contaning text, check if it's blurred, if so append to list\n",
    "for img in imgs_w_txt:\n",
    "    blrd_img = blur_detector(source + img)\n",
    "    if blrd_img:\n",
    "        blurred_imgs_labels.append(blrd_img)\n",
    "        \n",
    "p_imtg_blrd = round((len(blurred_imgs_labels) / len(imgs_w_txt)) * 100, 2)\n",
    "\n",
    "print(f\"{len(blurred_imgs_labels)} images blurred out of {len(imgs_w_txt)} â‰ˆ {p_imtg_blrd}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1726e739-b18f-4ce0-89ce-60c49ee93595",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Dataset Assessing approach\n",
    "Framing : FRM --- Blurried : BLR --- Dark : DRK --- Bright : BRT ---\n",
    "Obstructed : OBS --- Other : OTH --- NoFlaws : NON --- Rotated : ROT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e7c6c21-f00f-4dbc-be00-fd243c69da5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 20000\n"
     ]
    }
   ],
   "source": [
    "train_json_qlty = '/media/arnau/PEN/TFG/data_assesing/'\n",
    "\n",
    "with open(train_json_qlty + \"final.json\", encoding='UTF-8') as json_file:\n",
    "    data = json.load(json_file)\n",
    "    data = data[\"train\"]\n",
    "    # {img_name : {info_img ...}}\n",
    "    \n",
    "    \n",
    "data_size = len(data)\n",
    "print(f\"Dataset size: {data_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5b35bed-50d0-4d4d-97ab-51a138d525d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VizWiz_train_00000000.jpg\n",
      "\n",
      "{'answerable': 1.0,\n",
      " 'flaws': [True, False, False, False, False, False, False, False],\n",
      " 'question': \"What's the name of this product?\",\n",
      " 'recognizable': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# Sample a data item\n",
    "key = list(data.keys())[0]\n",
    "value = list(data.values())[0]\n",
    "print(f\"{key}\\n\")\n",
    "pprint.pprint(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e868351-a5b3-49cf-a311-7afa19a50136",
   "metadata": {},
   "outputs": [],
   "source": [
    "#list_imgs_dts1 = interest_data[\"IMG\"].to_list()\n",
    "#list_imgs_dts2 = list(data.keys())\n",
    "\n",
    "#print(f\"Number of imgs: \\n\\t dataset1: {len(list_imgs_dts1)} \\n\\t dataset2: {len(list_imgs_dts2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6540eeae-4f44-4cd7-8127-413bd82869ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the 1st dataset, array containing all images with text\n",
    "arr_imgs_w_txt = np.array(imgs_w_txt, dtype=str)\n",
    "\n",
    "# From the 2nd dataset, array containing all images\n",
    "arr_imgs_dts2 = np.array(list_imgs_dts2, dtype=str)\n",
    "\n",
    "# Images present in both datasets\n",
    "common_imgs = list(set(list_imgs_dts1).intersection(list_imgs_dts2))\n",
    "print(f\"Intersection dataset size: {len(common_imgs)} imgs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7071ac-7093-4c42-880f-aca8a62e71a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many imgs from the 1st dataset (txt) are also in the 2nd one?\n",
    "total_imgs_w_txt = np.array(list(set(arr_imgs_w_txt).intersection(common_imgs)), \n",
    "                          dtype=str)\n",
    "total_imgs_w_txt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e302f78-beb2-4448-8e21-2dc5bdfa5d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_flaw(info_img, flaw):\n",
    "    \"\"\"\n",
    "    For a given flaw, checks if it is present in the image\n",
    "    \"\"\"\n",
    "    \n",
    "    flaws = {'FRM': 0, 'BLR': 1, 'DRK': 2, 'BRT': 3, \n",
    "                  'OBS': 4, 'OTH': 5, 'NON': 6, 'ROT': 7}\n",
    "    idx = flaws[flaw]\n",
    "    \n",
    "    return True if info_img[\"flaws\"][idx] == True else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04b7bf0-9503-4804-bba9-2c2aff426bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "flaw_typed_imgs = []\n",
    "flaw = \"BLR\"\n",
    "\n",
    "# Return a list with the images that have the given flaw\n",
    "for img in total_imgs_w_txt:\n",
    "    info_img = data[img] # dict with img flaws\n",
    "    if not get_flaw(info_img, \"NON\"): # if the img is flawed\n",
    "        answerable = True if info_img[\"answerable\"] == 1.0 else False\n",
    "        recognizable = True if info_img[\"recognizable\"] == 1.0 else False\n",
    "\n",
    "        flawed = get_flaw(info_img, flaw) # Specify the desired flaw \n",
    "\n",
    "        if answerable and recognizable and flawed:\n",
    "            flaw_typed_imgs.append(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa52be42-c698-43d5-9751-08201f3d77ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nImages containing text and {flaw}\\n\")\n",
    "nrow, ncol = 4, 4\n",
    "_, axs = plt.subplots(nrow, ncol, figsize=(8, 8))\n",
    "axs = axs.flatten()\n",
    "random.shuffle(flaw_typed_imgs)\n",
    "for img, ax in zip(flaw_typed_imgs, axs):\n",
    "    img = cv2.imread(source + img)\n",
    "    imgc = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    ax.imshow(imgc)\n",
    "    ax.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6795f988-f92c-46ad-a692-c4f00ab387a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stats\n",
    "n_flaw_typed_imgs = len(flaw_typed_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c44467c-5be6-45f5-8d03-a18fc8c28cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = f'{flaw}', 'Other flaws'\n",
    "other_flaws = len(common_imgs) - n_flaw_typed_imgs\n",
    "sizes = [n_flaw_typed_imgs, other_flaws]\n",
    "explode = (0.04, 0)  # for highligthing\n",
    "\n",
    "fig1, ax1 = plt.subplots(figsize=(4, 3))\n",
    "ax1.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%',\n",
    "        shadow=False, startangle=90)\n",
    "ax1.axis('equal') \n",
    "fig1.suptitle(f\"How many text-{flaw} images are in the entire dataset?\",fontsize=10)\n",
    "ax1.set_title(f\"{flaw} frequency in dataset\\n\", y=1.02, fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183ef008-e500-4143-897e-2fa9e3e2e8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing: Frequency of flaws\n",
    "\n",
    "flaws = {'FRM': 0, 'BLR': 0, 'DRK': 0, 'BRT': 0, \n",
    "        'OBS': 0, 'OTH': 0, 'ROT': 0, \"NON\" : 0}\n",
    "inv_flaws = {v: k for k, v in flaws.items()}\n",
    "\n",
    "for flaw in flaws.keys():\n",
    "    for img in total_imgs_w_txt:\n",
    "        info_img = data[img] # dict with img flaws\n",
    "        # if get_flaw(info_img, \"NON\"): # if the img is flawed\n",
    "        if get_flaw(info_img, flaw):\n",
    "            flaws[flaw] += 1\n",
    "\n",
    "            \n",
    "flaws = {flaw : freq for flaw, freq in sorted(flaws.items(),\n",
    "                                              key=lambda item : item[1],\n",
    "                                          reverse=True)}\n",
    "flaws\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da98f0d-d497-4f7c-96bd-a16a95241eab",
   "metadata": {},
   "source": [
    "NON == True & BLR == True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1ae7e1-caab-456d-a54e-25c3ea6b49d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4,4))\n",
    "img_name = \"VizWiz_train_00008063.jpg\"\n",
    "img = cv2.imread(source + img_name)\n",
    "imgc = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(imgc)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "pprint.pprint(data[img_name])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfg env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
