{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c2bee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import pprint\n",
    "import cv2\n",
    "import shutil\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5dd1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_json = '../Annotations/train.json'\n",
    "train_data = '/media/arcas/PEN/train/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09dbb110",
   "metadata": {},
   "source": [
    "Estructura basica de les dades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4838391a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(train_json, encoding='UTF-8') as json_file:\n",
    "    data = json.load(json_file)\n",
    "    pprint.pprint(data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708b6f6d",
   "metadata": {},
   "source": [
    "Percentatge d'imatges que no es poden respondre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe96a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "answerability = [d[\"answerable\"] for d in data]\n",
    "non_answerable = len(data) - sum(answerability)\n",
    "non_answble_imgs_percent = (non_answerable / len(data)) * 100\n",
    "non_answble_imgs_labels = [d[\"image\"] for d in data if d[\"answerable\"] == 0]\n",
    "\n",
    "print(f\"Non-answerable: {non_answerable} imgs = {round(non_answble_imgs_percent, 2)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c5a591",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_img = non_answble_imgs_labels[20]\n",
    "img = cv2.imread(train_data + sample_img)\n",
    "imgc = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "plt.imshow(imgc)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3e6ddd",
   "metadata": {},
   "source": [
    "Copiar les imatges NAI (Non-Answerable-Images) en una carpeta a aprt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0a71fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "destination = '../data/NAI_train/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73db727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating storing folders\n",
    "if not (os.path.isdir(destination)):\n",
    "    os.mkdir(destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e734e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(os.listdir(destination)) == 0:\n",
    "    for fs in non_answble_imgs_labels:\n",
    "        full_file = train_data + fs\n",
    "        shutil.copy(full_file, destination + fs)\n",
    "    print(\"Files moved succesfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed1438a-49ca-4ff6-a39d-08438f75eccf",
   "metadata": {},
   "source": [
    "# Detect images with text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e09b5fd-bd44-4662-8418-30868da33443",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import pytesseract\n",
    " \n",
    "def find_text(sample_img):\n",
    "    err_count = 0\n",
    "    try:\n",
    "        img = cv2.imread(sample_img)\n",
    "\n",
    "        # Preprocessing the image starts\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Performing OTSU threshold\n",
    "        ret, thresh1 = cv2.threshold(gray, 0, 255, cv2.THRESH_OTSU | cv2.THRESH_BINARY_INV)\n",
    "\n",
    "        # Specify structure shape and kernel size.\n",
    "        # Kernel size increases or decreases the area\n",
    "        # of the rectangle to be detected.\n",
    "        # A smaller value like (10, 10) will detect\n",
    "        # each word instead of a sentence.\n",
    "        rect_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (18, 18))\n",
    "\n",
    "        # Applying dilation on the threshold image\n",
    "        dilation = cv2.dilate(thresh1, rect_kernel, iterations = 1)\n",
    "\n",
    "        # Finding contours\n",
    "        contours, hierarchy = cv2.findContours(dilation, cv2.RETR_EXTERNAL,\n",
    "                                                         cv2.CHAIN_APPROX_NONE)\n",
    "        # Creating a copy of image\n",
    "        im2 = img.copy()\n",
    "\n",
    "        # A text file is created and flushed\n",
    "        file = open(\"recognized.txt\", \"w+\")\n",
    "        file.write(\"\")\n",
    "        file.close()\n",
    "\n",
    "        # Looping through the identified contours\n",
    "        # Then rectangular part is cropped and passed on\n",
    "        # to pytesseract for extracting text from it\n",
    "        # Extracted text is then written into the text file\n",
    "        for cnt in contours:\n",
    "            x, y, w, h = cv2.boundingRect(cnt)\n",
    "\n",
    "            # Drawing a rectangle on copied image\n",
    "            rect = cv2.rectangle(im2, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "            # Cropping the text block for giving input to OCR\n",
    "            cropped = im2[y:y + h, x:x + w]\n",
    "\n",
    "            # Open the file in append mode\n",
    "            file = open(\"recognized.txt\", \"a\")\n",
    "\n",
    "            # Apply OCR on the cropped image\n",
    "            text = pytesseract.image_to_string(cropped)\n",
    "\n",
    "            # Appending the text into file\n",
    "            file.write(text)\n",
    "            file.write(\"\\n\")\n",
    "\n",
    "            # Close the file\n",
    "            file.close\n",
    "\n",
    "        if os.path.getsize(\"./recognized.txt\") > 0:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    except:\n",
    "        err_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2354b7e7-d8ca-416f-8fc1-32059b29288e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Array containing bool values wheather the images had text or not\n",
    "results = np.array([find_text(destination + img) \n",
    "                    for img in non_answble_imgs_labels], dtype=bool)\n",
    "\n",
    "# Count how many True values (images containing text)\n",
    "count = np.count_nonzero(results)\n",
    "perct_of_text = round(count / results.shape[0] * 100, 2)\n",
    "\n",
    "print(f\"Total number of imgs = {results.shape[0]}\\nTotal percentage of imgs containing text = {perct_of_text}%\")\n",
    "print(err_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967d06b1-8f72-486f-8184-7e42969d2916",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfg",
   "language": "python",
   "name": "tfg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
