{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c98f3725-9c7c-4bd7-a55f-8a08a17bd18d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import cv2\n",
    "from json import JSONEncoder\n",
    "import skimage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bed86db-51fd-4c5e-ac73-00faaf7a3792",
   "metadata": {
    "tags": []
   },
   "source": [
    "# VizWiz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82195045-d984-415f-9487-577abc967546",
   "metadata": {
    "tags": []
   },
   "source": [
    "[Data](https://vizwiz.org/tasks-and-datasets/image-quality-issues/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e02fdfb-7c48-4d59-b421-e2887a98feb1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dir = '/media/arnau/SSD/VizWiz/captioning/train/'\n",
    "val_dir = '/media/arnau/SSD/VizWiz/captioning/val/'\n",
    "test_dir = '/media/arnau/SSD/VizWiz/captioning/test/'\n",
    "annots_dir = '/media/arnau/SSD/VizWiz/captioning/annotations/'\n",
    "annots_qi_dir = '/media/arnau/SSD/VizWiz/quality_issues/annotations/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11be330d-c6be-4bc3-a542-53f04dc210e5",
   "metadata": {},
   "source": [
    "Train / Val / Test splits images **annotations**. Used to retrieve if there is **text present** in images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c842096c-4a9f-402c-b9a0-552404db67c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(annots_dir + \"train.json\", encoding='UTF-8') as tr_json_file:\n",
    "    train_data = json.load(tr_json_file)\n",
    "    train_annots = train_data['images']\n",
    "\n",
    "with open(annots_dir + \"val.json\", encoding='UTF-8') as v_json_file:\n",
    "    val_data = json.load(v_json_file)\n",
    "    val_annots = val_data['images']\n",
    "\n",
    "with open(annots_dir + \"test.json\", encoding='UTF-8') as ts_json_file:\n",
    "    test_data = json.load(ts_json_file)\n",
    "    test_annots = test_data['images']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36433e0d-4a7b-46cb-bb10-aeda9d0f17bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load custom test data\n",
    "f = '/media/arnau/PEN/TFG/hf_model_test_res.json'\n",
    "#f = 'D://TFG//hf_model_test_res.json'\n",
    "with open(f) as user_file:\n",
    "    test_data = json.load(user_file)\n",
    "\n",
    "model_test_imgs = list(test_data.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c871e0-6012-4c3b-9190-8c7cac4ba6d0",
   "metadata": {
    "tags": []
   },
   "source": [
    "Images containing text for each split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91100f19-7913-4ae7-88f6-b39079c9a177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14701 training images containing text\n",
      "5018 training images containing text\n",
      "5093 training images containing text\n"
     ]
    }
   ],
   "source": [
    "train_imgs_w_text = []\n",
    "val_imgs_w_text = []\n",
    "test_imgs_w_text = []\n",
    "\n",
    "annots = {'train': train_annots, \n",
    "        'val' : val_annots,\n",
    "        'test' : test_annots}\n",
    "\n",
    "for split, data in annots.items():\n",
    "    for d in data:\n",
    "        if d[\"text_detected\"] == True and split == 'train':\n",
    "            train_imgs_w_text .append(d[\"file_name\"])\n",
    "        elif d[\"text_detected\"] == True and split == 'val':\n",
    "            val_imgs_w_text .append(d[\"file_name\"])\n",
    "        elif d[\"text_detected\"] == True and split == 'test':\n",
    "            test_imgs_w_text .append(d[\"file_name\"])\n",
    "        \n",
    "print(f\"{len(train_imgs_w_text)} training images containing text\")\n",
    "print(f\"{len(val_imgs_w_text)} training images containing text\")\n",
    "print(f\"{len(test_imgs_w_text)} training images containing text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495ff46f-25f5-4d63-92bb-8eb8e77291d0",
   "metadata": {},
   "source": [
    "Train / Val / Test splits images **quality annotations**. Used to retrieve quality flaws **(blur)** in images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13174fa6-a19c-4f23-9f5c-21b45c5d7377",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(annots_qi_dir + \"train.json\", encoding='UTF-8') as tqif:\n",
    "    train_qi_data = json.load(tqif)\n",
    "    \n",
    "with open(annots_qi_dir + \"val.json\", encoding='UTF-8') as vqif:\n",
    "    val_qi_data = json.load(vqif)\n",
    "    \n",
    "with open(annots_qi_dir + \"test.json\", encoding='UTF-8') as tsqif:\n",
    "    test_qi_data = json.load(tsqif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd5d0ee8-eb6e-426f-b02e-72d8edbe57b6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4196 training images blured with text\n",
      "4496 training images non-blured with text\n",
      "1248 val images blured with text\n",
      "1049 val images non-blured with text\n"
     ]
    }
   ],
   "source": [
    "blured_train_images_with_text = []\n",
    "non_blured_train_images_with_text = []\n",
    "\n",
    "blured_val_images_with_text = []\n",
    "non_blured_val_images_with_text = []\n",
    "\n",
    "blured_test_images_with_text = []\n",
    "non_test_train_images_with_text = []\n",
    "\n",
    "annots_qi = {'train': train_qi_data, \n",
    "            'val' : val_qi_data}\n",
    "\n",
    "for split, data in annots_qi.items():\n",
    "    for d in data:\n",
    "        if d[\"image\"] in train_imgs_w_text: # if image has text\n",
    "            if d[\"flaws\"][\"BLR\"] >= 3: # if image is blur\n",
    "                blured_train_images_with_text.append(d[\"image\"])\n",
    "            elif d[\"flaws\"][\"NON\"] >= 3: # if image is clean\n",
    "                non_blured_train_images_with_text.append(d[\"image\"])\n",
    "        \n",
    "        # Since we use a custom test set (extracted from val set) that also contains \n",
    "        # images from VizWiz, we have to check that the validation images are not in the test set. \n",
    "        # More on why we do this in vqa_hf notebook\n",
    "        elif d[\"image\"] in val_imgs_w_text and d[\"image\"] not in model_test_imgs: \n",
    "            if d[\"flaws\"][\"BLR\"] >= 3: \n",
    "                blured_val_images_with_text.append(d[\"image\"])\n",
    "            elif d[\"flaws\"][\"NON\"] >= 3: \n",
    "                non_blured_val_images_with_text.append(d[\"image\"])\n",
    "            \n",
    "\n",
    "print(f\"{len(blured_train_images_with_text)} training images blured with text\")\n",
    "print(f\"{len(non_blured_train_images_with_text)} training images non-blured with text\")\n",
    "\n",
    "print(f\"{len(blured_val_images_with_text)} val images blured with text\")\n",
    "print(f\"{len(non_blured_val_images_with_text)} val images non-blured with text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5f08b38-7613-4d21-bfe2-80483f50ef4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def balance(l):  \n",
    "    \"\"\"\n",
    "    Balance a given list to have the same proportion of elements\n",
    "    for each class\n",
    "    \"\"\"\n",
    "    \n",
    "    n = len(l) // 2\n",
    "    arr = np.array(l, dtype=object)\n",
    "    zeros = arr[arr[:,1] == 0]\n",
    "    ones = arr[arr[:,1] == 1]\n",
    "    np.random.shuffle(zeros)\n",
    "    np.random.shuffle(ones)\n",
    "    final_arr = np.concatenate((zeros[:n], ones[:n]))\n",
    "    final_list = list(map(tuple, final_arr))\n",
    "    \n",
    "    return final_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7bb006c-8fb0-4538-8a43-05407feae0c2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Train data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f499cb78-023c-4ee0-9ee5-924cbd63a280",
   "metadata": {
    "tags": []
   },
   "source": [
    "Train set containing blur and non-blur images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62fa9ffa-6c8b-4cb7-973f-0c15e247a4b6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VizWiz TRAIN set size 8542\n"
     ]
    }
   ],
   "source": [
    "blured_train_dataset = np.asarray(list(map(lambda im : (im, 1), blured_train_images_with_text)), dtype=object)\n",
    "non_blured_train_dataset = np.asarray(list(map(lambda im : (im, 0), non_blured_train_images_with_text)), dtype=object)\n",
    "\n",
    "vw_train_set = np.asarray(balance(\n",
    "                                np.vstack((blured_train_dataset, non_blured_train_dataset)\n",
    "                                         )), \n",
    "                          dtype=object)\n",
    "\n",
    "print(f\"VizWiz TRAIN set size {vw_train_set.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef6343a-3f91-4e8b-93b5-4e7311899af0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Val + Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b970f1ab-3987-4874-93f7-187fb6237dcc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "blured_val_dataset = np.asarray(list(map(lambda im : (im, 1), blured_val_images_with_text)), dtype=object)\n",
    "non_blured_val_dataset = np.asarray(list(map(lambda im : (im, 0), non_blured_val_images_with_text)), dtype=object)\n",
    "\n",
    "test_n_val_set = np.asarray(balance(\n",
    "                                np.vstack((blured_val_dataset, non_blured_val_dataset))\n",
    "                        ), \n",
    "                        dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e8f40abd-72fa-4928-9552-26c9f2eaf15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(test_n_val_set)\n",
    "# Half data for val set, half data for test set\n",
    "test_set = test_n_val_set[: len(test_n_val_set) // 2]\n",
    "val_set = test_n_val_set[len(test_n_val_set) // 2: ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b829f19a-6462-427b-970b-ff88416ce61e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VizWiz TEST set size 1082\n",
      "VizWiz VAL set size 1065\n"
     ]
    }
   ],
   "source": [
    "vw_test_set = np.asarray(balance(test_set), dtype=object)\n",
    "vw_val_set = np.asarray(balance(val_set), dtype=object)\n",
    "\n",
    "print(f\"VizWiz TEST set size {vw_test_set.shape[0]}\")\n",
    "print(f\"VizWiz VAL set size {vw_val_set.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d82450fc-8f50-482b-a2e6-3a4bd0fc186d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for img in list(vw_test_set[:, 0]):\n",
    "    assert img not in list(vw_val_set[:, 0]), \"ERROR\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8b80db7a-17d2-49b2-8db0-6fc75f7bd4cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#plt.imshow(plt.imread(train_dir + random.choice(blured_train_images_with_text)))\n",
    "#plt.axis('off')\n",
    "#plt.show()\n",
    "#plt.imshow(plt.imread(train_dir + random.choice(non_blured_train_images_with_text)))\n",
    "#plt.axis('off')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e2a4d7-e344-4c4b-8c8d-78400c38af7b",
   "metadata": {},
   "source": [
    "## Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9e03b9d4-5e5e-4685-a591-bafd1cac8a21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files saved\n"
     ]
    }
   ],
   "source": [
    "total_size = len(vw_test_set) + len(vw_val_set) + len(vw_train_set)\n",
    "\n",
    "np.random.shuffle(vw_test_set)\n",
    "np.random.shuffle(vw_train_set)\n",
    "np.random.shuffle(vw_val_set)\n",
    "\n",
    "vw_test_set = vw_test_set[: int(total_size * 0.1)]\n",
    "vw_val_set = vw_val_set[: int(total_size * 0.1)]\n",
    "vw_train_set = vw_train_set[: int(total_size * 0.8)]\n",
    "\n",
    "vw_data = {'train' : vw_train_set,\n",
    "           'val' : vw_val_set,\n",
    "           'test' : vw_test_set,\n",
    "          }\n",
    "\n",
    "class NumpyArrayEncoder(JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        return JSONEncoder.default(self, obj)\n",
    "\n",
    "with open('/home/arnau/tfg/GED-TFG/data/vw_blur_dataset.json', 'w') as outfile:\n",
    "    json.dump(vw_data, outfile, cls=NumpyArrayEncoder)\n",
    "print(\"Files saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0820ea-bae9-4980-90b8-df23e33d5af0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# TextVQA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef653c7e-1178-48bd-a88a-5847a4521d71",
   "metadata": {},
   "source": [
    "[Data](https://textvqa.org/dataset/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "997e87ec-4eff-4a02-adda-90f5d24bf96f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tvqa_imgs_dir = './/TextVQA//train_images//'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5f9cf27d-1759-4093-80eb-606b7081d7b4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './/TextVQA//train_images//'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m imgs_list \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtvqa_imgs_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m random\u001b[38;5;241m.\u001b[39mshuffle(imgs_list)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTextVQA Dataset size \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(imgs_list)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './/TextVQA//train_images//'"
     ]
    }
   ],
   "source": [
    "imgs_list = os.listdir(tvqa_imgs_dir)\n",
    "\n",
    "random.shuffle(imgs_list)\n",
    "\n",
    "print(f\"TextVQA Dataset size {len(imgs_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd422bd8-fc8a-4a33-8c3b-dd0424e8bcd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove dark images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148ca0c4-fe6e-4bad-84c3-7252efe111bf",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dece7e71-9e35-46a6-acdd-80f7e6e03f7a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tvqa_train_imgs = imgs_list[: len(imgs_list) // 2]\n",
    "print(f\"TextVQA TRAIN size: {len(tvqa_train_imgs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da69a17-112e-4cfe-8d3d-aa23545cfd35",
   "metadata": {},
   "source": [
    "## Val data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc6d98e-0b16-40cc-8786-51774229d02b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tvqa_val_imgs = imgs_list[len(imgs_list) // 2 :]\n",
    "print(f\"TextVQA VAL size: {len(tvqa_val_imgs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483c8c42-ca01-4a27-8aeb-535b9b98c351",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Blur data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9f2518-34e8-426b-8d2c-09a45bc02efe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tvqa_train_to_add_blur = tvqa_train_imgs[: len(tvqa_train_imgs) // 2 :]\n",
    "tvqa_train_non_blur = tvqa_train_imgs[len(tvqa_train_imgs) // 2 :]\n",
    "\n",
    "tvqa_val_to_add_blur = tvqa_val_imgs[: len(tvqa_val_imgs) // 2 :]\n",
    "tvqa_val_non_blur = tvqa_val_imgs[len(tvqa_val_imgs) // 2 :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "badbeee3-8d09-46f8-926e-c5e9afc31683",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_blur(img_name, typ, orient=\"h\"):\n",
    "\n",
    "    if type(img_name) == str:\n",
    "        img = cv2.imread(img_name, cv2.IMREAD_COLOR)\n",
    "\n",
    "    if typ == \"motion\":\n",
    "        kernel_size = 20\n",
    "\n",
    "        kernel_v = np.zeros((kernel_size, kernel_size))\n",
    "        kernel_h = np.copy(kernel_v)\n",
    "        kernel_v[:, int((kernel_size - 1)/2)] = np.ones(kernel_size)\n",
    "        kernel_h[int((kernel_size - 1)/2), :] = np.ones(kernel_size)\n",
    "        kernel_v /= kernel_size\n",
    "        kernel_h /= kernel_size\n",
    "\n",
    "        vertical_mb = cv2.filter2D(img, -1, kernel_v)\n",
    "        horizonal_mb = cv2.filter2D(img, -1, kernel_h)\n",
    "\n",
    "        if orient == 'h':\n",
    "            return horizonal_mb\n",
    "        elif orient == \"v\":\n",
    "            return vertical_mb\n",
    "        \n",
    "    elif typ == \"gaussian\":\n",
    "        sigma = 4.0\n",
    "\n",
    "        # apply Gaussian blur, creating a new image\n",
    "        blurred = skimage.filters.gaussian(\n",
    "            img, sigma=(sigma, sigma), truncate=3.5, channel_axis=2)\n",
    "        return blurred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9954f229-b5c6-4cff-91df-66b16b68da9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "u, d, t = 0, 0, 0\n",
    "\n",
    "for i, img in enumerate(tvqa_train_to_add_blur[:100]):\n",
    "    if i >= 0 and i <= 33:\n",
    "        img_v_mblur = add_blur(tvqa_imgs_dir + img, \"motion\", \"v\")\n",
    "        cv2.imwrite(blur_dir + img, img_v_mblur)\n",
    "        u += 1\n",
    "    elif i >= 33 and i <= 66:\n",
    "        img_h_mblur = add_blur(tvqa_imgs_dir + img, \"motion\", \"h\")\n",
    "        cv2.imwrite(blur_dir + img, img_h_mblur)\n",
    "        d += 1\n",
    "    elif i >= 66:\n",
    "        img_gsn_blur = add_blur(tvqa_imgs_dir + img, \"gaussian\")\n",
    "        cv2.imwrite(blur_dir + img, img_gsn_blur)\n",
    "        t += 1\n",
    "print(u, d, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb48ccf3-e644-40b8-8c07-bfc03beea93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "blur_dir = './/TextVQA//blur//'\n",
    "\n",
    "if len(os.listdir(blur_dir)) == 0:\n",
    "    \n",
    "    for i, img in enumerate(tvqa_train_to_add_blur):\n",
    "        if i>= 0 and i <= len(tvqa_train_to_add_blur) // 3:\n",
    "            img_v_mblur = add_blur(tvqa_imgs_dir + img, \"motion\", \"v\")\n",
    "            cv2.imwrite(blur_dir + img, img_v_mblur)\n",
    "            i >= 33 and i <= 66:\n",
    "        elif i >= len(tvqa_train_to_add_blur) // and i <= (len(tvqa_train_to_add_blur) // 3) * 2:\n",
    "            img_h_mblur = add_blur(tvqa_imgs_dir + img, \"motion\", \"h\")\n",
    "            cv2.imwrite(blur_dir + img, img_h_mblur)\n",
    "        elif i >= (len(tvqa_train_to_add_blur) // 3) * 2:\n",
    "            img_gsn_blur = add_blur(tvqa_imgs_dir + img, \"gaussian\")\n",
    "            cv2.imwrite(blur_dir + img, img_gsn_blur)\n",
    "         \n",
    "    for i, img in enumerate(tvqa_val_to_add_blur):\n",
    "        if i>= 0 and i <= len(tvqa_val_to_add_blur) // 3:\n",
    "            img_v_mblur = add_blur(plt.imread(tvqa_imgs_dir + img), \"motion\", \"v\")\n",
    "            cv2.imwrite(blur_dir + img, img_v_mblur)\n",
    "        elif i >= len(tvqa_val_to_add_blur) // 3 and i <= (len(tvqa_val_to_add_blur) // 3) * 2:\n",
    "            img_h_mblur = add_blur(plt.imread(tvqa_imgs_dir + img), \"motion\", \"h\")\n",
    "            cv2.imwrite(blur_dir + img, img_h_mblur)\n",
    "        elif i >= (len(tvqa_val_to_add_blur) // 3) * 2:\n",
    "            img_gsn_blur = add_blur(plt.imread(tvqa_imgs_dir + img), \"gaussian\")\n",
    "            cv2.imwrite(blur_dir + img, img_gsn_blur)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90dd8652-ef7a-4d14-abe5-7709030b171e",
   "metadata": {},
   "source": [
    "# VizWiz + TextVQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7229ad93-80d8-40e7-8fe1-31b69856e5d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43dfbe6e-c2b9-496e-b88e-95fa1b7eed95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e94e621-80db-4ee5-84b2-05d61cc78e70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfg env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
