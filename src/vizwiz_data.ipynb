{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c98f3725-9c7c-4bd7-a55f-8a08a17bd18d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import cv2\n",
    "from json import JSONEncoder\n",
    "import skimage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bed86db-51fd-4c5e-ac73-00faaf7a3792",
   "metadata": {
    "tags": []
   },
   "source": [
    "# VizWiz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82195045-d984-415f-9487-577abc967546",
   "metadata": {
    "tags": []
   },
   "source": [
    "[Data](https://vizwiz.org/tasks-and-datasets/image-quality-issues/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e02fdfb-7c48-4d59-b421-e2887a98feb1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dir = '/media/arnau/SSD/VizWiz/captioning/train/'\n",
    "val_dir = '/media/arnau/SSD/VizWiz/captioning/val/'\n",
    "test_dir = '/media/arnau/SSD/VizWiz/captioning/test/'\n",
    "annots_dir = '/media/arnau/SSD/VizWiz/captioning/annotations/'\n",
    "annots_qi_dir = '/media/arnau/SSD/VizWiz/quality_issues/annotations/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11be330d-c6be-4bc3-a542-53f04dc210e5",
   "metadata": {},
   "source": [
    "Train / Val / Test splits images **annotations**. Used to retrieve if there is **text present** in images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c842096c-4a9f-402c-b9a0-552404db67c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(annots_dir + \"train.json\", encoding='UTF-8') as tr_json_file:\n",
    "    train_data = json.load(tr_json_file)\n",
    "    train_annots = train_data['images']\n",
    "\n",
    "with open(annots_dir + \"val.json\", encoding='UTF-8') as v_json_file:\n",
    "    val_data = json.load(v_json_file)\n",
    "    val_annots = val_data['images']\n",
    "\n",
    "with open(annots_dir + \"test.json\", encoding='UTF-8') as ts_json_file:\n",
    "    test_data = json.load(ts_json_file)\n",
    "    test_annots = test_data['images']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36433e0d-4a7b-46cb-bb10-aeda9d0f17bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load custom test data\n",
    "f = '/media/arnau/PEN/TFG/hf_model_test_res.json'\n",
    "#f = 'D://TFG//hf_model_test_res.json'\n",
    "with open(f) as user_file:\n",
    "    test_data = json.load(user_file)\n",
    "\n",
    "model_test_imgs = list(test_data.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c871e0-6012-4c3b-9190-8c7cac4ba6d0",
   "metadata": {
    "tags": []
   },
   "source": [
    "Images containing text for each split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91100f19-7913-4ae7-88f6-b39079c9a177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14701 training images containing text\n",
      "5018 training images containing text\n",
      "5093 training images containing text\n"
     ]
    }
   ],
   "source": [
    "train_imgs_w_text = []\n",
    "val_imgs_w_text = []\n",
    "test_imgs_w_text = []\n",
    "\n",
    "annots = {'train': train_annots, \n",
    "        'val' : val_annots,\n",
    "        'test' : test_annots}\n",
    "\n",
    "for split, data in annots.items():\n",
    "    for d in data:\n",
    "        if d[\"text_detected\"] == True and split == 'train':\n",
    "            train_imgs_w_text .append(d[\"file_name\"])\n",
    "        elif d[\"text_detected\"] == True and split == 'val':\n",
    "            val_imgs_w_text .append(d[\"file_name\"])\n",
    "        elif d[\"text_detected\"] == True and split == 'test':\n",
    "            test_imgs_w_text .append(d[\"file_name\"])\n",
    "\n",
    "print(f\"{len(train_imgs_w_text)} training images containing text\")\n",
    "print(f\"{len(val_imgs_w_text)} training images containing text\")\n",
    "print(f\"{len(test_imgs_w_text)} training images containing text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495ff46f-25f5-4d63-92bb-8eb8e77291d0",
   "metadata": {},
   "source": [
    "Train / Val / Test splits images **quality annotations**. Used to retrieve quality flaws **(blur)** in images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13174fa6-a19c-4f23-9f5c-21b45c5d7377",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(annots_qi_dir + \"train.json\", encoding='UTF-8') as tqif:\n",
    "    train_qi_data = json.load(tqif)\n",
    "    \n",
    "with open(annots_qi_dir + \"val.json\", encoding='UTF-8') as vqif:\n",
    "    val_qi_data = json.load(vqif)\n",
    "    \n",
    "with open(annots_qi_dir + \"test.json\", encoding='UTF-8') as tsqif:\n",
    "    test_qi_data = json.load(tsqif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be496eff-e3cc-4271-81c6-40c4d12e69ad",
   "metadata": {},
   "source": [
    "**Specify flaw:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00855300",
   "metadata": {},
   "outputs": [],
   "source": [
    "flaw = \"FRM\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd5d0ee8-eb6e-426f-b02e-72d8edbe57b6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5591 training images blured with text\n",
      "4496 training images non-blured with text\n",
      "1622 val images blured with text\n",
      "1049 val images non-blured with text\n"
     ]
    }
   ],
   "source": [
    "blured_train_images_with_text = []\n",
    "non_blured_train_images_with_text = []\n",
    "\n",
    "blured_val_images_with_text = []\n",
    "non_blured_val_images_with_text = []\n",
    "\n",
    "blured_test_images_with_text = []\n",
    "non_test_train_images_with_text = []\n",
    "\n",
    "annots_qi = {'train': train_qi_data, \n",
    "            'val' : val_qi_data}\n",
    "\n",
    "for split, data in annots_qi.items():\n",
    "    for d in data:\n",
    "        if d[\"image\"] in train_imgs_w_text: # if image has text\n",
    "            if d[\"flaws\"][flaw] >= 3: # if image is blur\n",
    "                blured_train_images_with_text.append(d[\"image\"])\n",
    "            elif d[\"flaws\"][\"NON\"] >= 3: # if image is clean\n",
    "                non_blured_train_images_with_text.append(d[\"image\"])\n",
    "        \n",
    "        # Since we use a custom test set (extracted from val set) that also contains \n",
    "        # images from VizWiz, we have to check that the validation images are not in the test set. \n",
    "        # More on why we do this in vqa_hf notebook\n",
    "        elif d[\"image\"] in val_imgs_w_text and d[\"image\"] not in model_test_imgs: \n",
    "            if d[\"flaws\"][flaw] >= 3: \n",
    "                blured_val_images_with_text.append(d[\"image\"])\n",
    "            elif d[\"flaws\"][\"NON\"] >= 3: \n",
    "                non_blured_val_images_with_text.append(d[\"image\"])\n",
    "            \n",
    "            \n",
    "print(f\"{len(blured_train_images_with_text)} training images blured with text\")\n",
    "print(f\"{len(non_blured_train_images_with_text)} training images non-blured with text\")\n",
    "\n",
    "print(f\"{len(blured_val_images_with_text)} val images blured with text\")\n",
    "print(f\"{len(non_blured_val_images_with_text)} val images non-blured with text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5f08b38-7613-4d21-bfe2-80483f50ef4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def balance(l):  \n",
    "    \"\"\"\n",
    "    Balance a given list to have the same proportion of elements\n",
    "    for each class\n",
    "    \"\"\"\n",
    "    \n",
    "    n = len(l) // 2\n",
    "    arr = np.array(l, dtype=object)\n",
    "    zeros = arr[arr[:,1] == 0]\n",
    "    ones = arr[arr[:,1] == 1]\n",
    "    np.random.shuffle(zeros)\n",
    "    np.random.shuffle(ones)\n",
    "    final_arr = np.concatenate((zeros[:n], ones[:n]))\n",
    "    final_list = list(map(tuple, final_arr))\n",
    "    \n",
    "    return final_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7bb006c-8fb0-4538-8a43-05407feae0c2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Train data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f499cb78-023c-4ee0-9ee5-924cbd63a280",
   "metadata": {
    "tags": []
   },
   "source": [
    "Train set containing blur and non-blur images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62fa9ffa-6c8b-4cb7-973f-0c15e247a4b6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VizWiz TRAIN set size 9539\n"
     ]
    }
   ],
   "source": [
    "blured_train_dataset = np.asarray(list(map(lambda im : (im, 1), blured_train_images_with_text)), dtype=object)\n",
    "non_blured_train_dataset = np.asarray(list(map(lambda im : (im, 0), non_blured_train_images_with_text)), dtype=object)\n",
    "\n",
    "vw_train_set = np.asarray(balance(\n",
    "                                np.vstack((blured_train_dataset, non_blured_train_dataset)\n",
    "                                         )), \n",
    "                          dtype=object)\n",
    "\n",
    "print(f\"VizWiz TRAIN set size {vw_train_set.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef6343a-3f91-4e8b-93b5-4e7311899af0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Val + Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b970f1ab-3987-4874-93f7-187fb6237dcc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "blured_val_dataset = np.asarray(list(map(lambda im : (im, 1), blured_val_images_with_text)), dtype=object)\n",
    "non_blured_val_dataset = np.asarray(list(map(lambda im : (im, 0), non_blured_val_images_with_text)), dtype=object)\n",
    "\n",
    "test_n_val_set = np.asarray(balance(\n",
    "                                np.vstack((blured_val_dataset, non_blured_val_dataset))\n",
    "                        ), \n",
    "                        dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e8f40abd-72fa-4928-9552-26c9f2eaf15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(test_n_val_set)\n",
    "# Half data for val set, half data for test set\n",
    "test_set = test_n_val_set[: len(test_n_val_set) // 2]\n",
    "val_set = test_n_val_set[len(test_n_val_set) // 2: ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b829f19a-6462-427b-970b-ff88416ce61e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VizWiz TEST set size 1110\n",
      "VizWiz VAL set size 1131\n"
     ]
    }
   ],
   "source": [
    "vw_test_set = np.asarray(balance(test_set), dtype=object)\n",
    "vw_val_set = np.asarray(balance(val_set), dtype=object)\n",
    "\n",
    "print(f\"VizWiz TEST set size {vw_test_set.shape[0]}\")\n",
    "print(f\"VizWiz VAL set size {vw_val_set.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d82450fc-8f50-482b-a2e6-3a4bd0fc186d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for img in list(vw_test_set[:, 0]):\n",
    "    assert img not in list(vw_val_set[:, 0]), \"ERROR\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e2a4d7-e344-4c4b-8c8d-78400c38af7b",
   "metadata": {},
   "source": [
    "## Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9e03b9d4-5e5e-4685-a591-bafd1cac8a21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files saved\n"
     ]
    }
   ],
   "source": [
    "total_size = len(vw_test_set) + len(vw_val_set) + len(vw_train_set)\n",
    "\n",
    "np.random.shuffle(vw_test_set)\n",
    "np.random.shuffle(vw_train_set)\n",
    "np.random.shuffle(vw_val_set)\n",
    "\n",
    "vw_test_set = vw_test_set[: int(total_size * 0.1)]\n",
    "vw_val_set = vw_val_set[: int(total_size * 0.1)]\n",
    "vw_train_set = vw_train_set[: int(total_size * 0.8)]\n",
    "\n",
    "vw_data = {'train' : vw_train_set,\n",
    "           'val' : vw_val_set,\n",
    "           'test' : vw_test_set,\n",
    "          }\n",
    "\n",
    "class NumpyArrayEncoder(JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        return JSONEncoder.default(self, obj)\n",
    "\n",
    "with open(f'/home/arnau/tfg/GED-TFG/data/vw_{flaw}_dataset.json', 'w') as outfile:\n",
    "    json.dump(vw_data, outfile, cls=NumpyArrayEncoder)\n",
    "print(\"Files saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af4433e-bc62-4179-bb3f-263c7b1cce45",
   "metadata": {},
   "source": [
    "## Multilabel data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fb92fe1d-8094-47ed-94c0-8be1a298310e",
   "metadata": {},
   "outputs": [],
   "source": [
    "blured_train_images_with_text = []\n",
    "blured_val_images_with_text = []\n",
    "\n",
    "clean_train_images_with_text = []\n",
    "clean_val_images_with_text = []\n",
    "\n",
    "frm_train_images_with_text = []\n",
    "frm_val_images_with_text = []\n",
    "\n",
    "frm_blr_train_images_with_text = []\n",
    "frm_blr_val_images_with_text = []\n",
    "\n",
    "\n",
    "annots_qi = {'train': train_qi_data, \n",
    "            'val' : val_qi_data}\n",
    "\n",
    "for split, data in annots_qi.items():\n",
    "    for d in data:\n",
    "        if d[\"image\"] in train_imgs_w_text: # if image has text\n",
    "            if d[\"flaws\"][\"BLR\"] >= 3 and d[\"flaws\"][\"FRM\"] <= 3: # if image is blur\n",
    "                data = [d[\"image\"], 0, 1, 0]\n",
    "                blured_train_images_with_text.append(data) # [img, clean, blr, frm, blr_n_frm]\n",
    "            elif d[\"flaws\"][\"NON\"] >= 3: # if image is clean\n",
    "                data = [d[\"image\"], 1, 0, 0]\n",
    "                clean_train_images_with_text.append(data)\n",
    "            elif d[\"flaws\"][\"FRM\"] >= 3 and d[\"flaws\"][\"BLR\"] <= 3: # if image is out of frame\n",
    "                data = [d[\"image\"], 0, 0, 1]\n",
    "                frm_train_images_with_text.append(data)\n",
    "\n",
    "        elif d[\"image\"] in val_imgs_w_text and d[\"image\"] not in model_test_imgs: \n",
    "            if d[\"flaws\"][\"BLR\"] >= 3 and d[\"flaws\"][\"FRM\"] <= 3:\n",
    "                data = [d[\"image\"], 0, 1, 0]\n",
    "                blured_val_images_with_text.append(data)\n",
    "            elif d[\"flaws\"][\"NON\"] >= 3: \n",
    "                data = [d[\"image\"], 1, 0, 0]\n",
    "                clean_val_images_with_text.append(data)\n",
    "            elif d[\"flaws\"][\"FRM\"] >= 3 and d[\"flaws\"][\"BLR\"] <= 3: # if image is out of frame\n",
    "                data = [d[\"image\"],0, 0, 1]\n",
    "                frm_val_images_with_text.append(data)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "99d3f758-8e61-473d-98f1-709108064adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def undersample(arr, min_len_array):\n",
    "    to_remove = (len(arr) - min_len_array) // 2\n",
    "    \n",
    "    idx_delete_0 = np.where(arr[:, 1] == 0)[0][: to_remove]\n",
    "    idx_delete_1 = np.where(arr[:, 1] == 1)[0][: to_remove]\n",
    "    idx_to_remove = np.hstack([idx_delete_0, idx_delete_1])\n",
    "    res = np.delete(arr, idx_to_remove, axis=0)\n",
    "    \n",
    "    # Equate shapes removing random element\n",
    "    if len(res) != min_len_array:\n",
    "        diff = len(res) - min_len_array \n",
    "        rand_idx_remove = random.sample(range(0, len(res)), diff)\n",
    "        res = np.delete(res, rand_idx_remove, axis=0)  \n",
    "    \n",
    "    return np.array(res, dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "48e65830-81ab-4f3b-b994-dc53bd1d2e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.vstack([np.array(blured_train_images_with_text, dtype=object),\n",
    "                        np.array(frm_train_images_with_text, dtype=object),\n",
    "                        np.array(clean_train_images_with_text, dtype=object)])\n",
    "\n",
    "val_data = np.vstack([np.array(blured_val_images_with_text[: len(blured_val_images_with_text) // 2], dtype=object),\n",
    "                        np.array(frm_val_images_with_text[: len(frm_val_images_with_text) // 2], dtype=object),\n",
    "                        np.array(clean_val_images_with_text[: len(clean_val_images_with_text) // 2], dtype=object)])\n",
    "\n",
    "test_data = np.vstack([np.array(blured_val_images_with_text[len(blured_val_images_with_text) // 2:], dtype=object),\n",
    "                        np.array(frm_val_images_with_text[len(frm_val_images_with_text) // 2:], dtype=object),\n",
    "                        np.array(clean_val_images_with_text[len(clean_val_images_with_text) // 2:], dtype=object)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a78c028d-037a-44cc-9a2e-9256c05e6b15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN\n",
      "clean -- n_samples = 4496\n",
      "BLR -- n_samples = 3218\n",
      "FRM -- n_samples = 4063\n",
      "\n",
      "VAL\n",
      "clean -- n_samples = 524\n",
      "BLR -- n_samples = 475\n",
      "FRM -- n_samples = 567\n",
      "\n",
      "TEST\n",
      "clean -- n_samples = 525\n",
      "BLR -- n_samples = 476\n",
      "FRM -- n_samples = 568\n"
     ]
    }
   ],
   "source": [
    "# Number of samples per class\n",
    "classess = {1 : \"clean\", 2 : \"BLR\", 3 : \"FRM\"}\n",
    "\n",
    "# data augmentation taking clean class as reference\n",
    "print(\"TRAIN\")\n",
    "for cidx, classname in classess.items():\n",
    "    class_samples = train_data[np.where(train_data[:, cidx] == 1)]\n",
    "    print(f\"{classname} -- n_samples = {class_samples.shape[0]}\")\n",
    "    \n",
    "print(\"\\nVAL\")\n",
    "for cidx, classname in classess.items():\n",
    "    class_samples = val_data[np.where(val_data[:, cidx] == 1)]\n",
    "    print(f\"{classname} -- n_samples = {class_samples.shape[0]}\")\n",
    "    \n",
    "print(\"\\nTEST\")\n",
    "for cidx, classname in classess.items():\n",
    "    class_samples = test_data[np.where(test_data[:, cidx] == 1)]\n",
    "    print(f\"{classname} -- n_samples = {class_samples.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "853868e7-7a37-4ada-b045-b7f31dd70ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil, os\n",
    "\n",
    "# Copy images to folder\n",
    "phase = 'val' \n",
    "\n",
    "for img_data in val_data:\n",
    "    img_name = img_data[0]\n",
    "    #all_lbls = np.array(img_data[1:])\n",
    "    #target = np.where(all_lbls == 1)[0][0]\n",
    "    #classidx = target + 1\n",
    "    source_file = val_dir + img_name\n",
    "    destination_folder = f'/media/arnau/SSD/VizWiz/multiclass/{phase}/'\n",
    "    \n",
    "    if img_name not in os.listdir(destination_folder):\n",
    "        shutil.copy2(source_file, destination_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8a7a8ec5-e3d6-446c-adb5-ed6d259c9d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_size = len(train_data) + len(val_data) + len(test_data)\n",
    "\n",
    "np.random.shuffle(train_data)\n",
    "np.random.shuffle(val_data)\n",
    "np.random.shuffle(test_data)\n",
    "\n",
    "# **No need to split (80/10/10) since it is alredy that proportion**\n",
    "\n",
    "vw_mc_data = {'train' : train_data,\n",
    "           'val' : val_data,\n",
    "           'test' : test_data,\n",
    "          }\n",
    "\n",
    "class NumpyArrayEncoder(JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        return JSONEncoder.default(self, obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "44d07bf8-6f20-445e-8d42-696fb53b61e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files saved\n"
     ]
    }
   ],
   "source": [
    "with open(f'/home/arnau/tfg/GED-TFG/data/vw_mc_dataset.json', 'w') as outfile:\n",
    "    json.dump(vw_mc_data, outfile, cls=NumpyArrayEncoder)\n",
    "print(\"Files saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1294c757-4c35-4684-a870-6e00ca5052fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfg env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
