{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c98f3725-9c7c-4bd7-a55f-8a08a17bd18d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import cv2\n",
    "from json import JSONEncoder\n",
    "import shutil\n",
    "import skimage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bed86db-51fd-4c5e-ac73-00faaf7a3792",
   "metadata": {
    "tags": []
   },
   "source": [
    "# VizWiz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82195045-d984-415f-9487-577abc967546",
   "metadata": {
    "tags": []
   },
   "source": [
    "[Data for quality issues](https://vizwiz.org/tasks-and-datasets/image-quality-issues/)  \n",
    "[Data for text presence](https://vizwiz.org/tasks-and-datasets/image-captioning/)  \n",
    "[Data for VQA](https://vizwiz.org/tasks-and-datasets/vqa/)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e3923b-2b76-412b-bb7f-766432890b2d",
   "metadata": {},
   "source": [
    "## Data for binary classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e02fdfb-7c48-4d59-b421-e2887a98feb1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Annotations for text presence in image\n",
    "annots_txtp_dir = '/media/arnau/SSD/VizWiz/data/captioning/annotations/'\n",
    "\n",
    "# Annotattions for image quality issues\n",
    "annots_qi_dir = '/media/arnau/SSD/VizWiz/data/quality_issues/annotations/'\n",
    "\n",
    "# Annotations for VQA data (questions, answers...)\n",
    "annots_vqa_dir = '/media/arnau/SSD/VizWiz/data/vqa/annotations/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "961a1fff-0221-4ea5-9304-e0260cd972ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Images dirs\n",
    "train_dir = '/media/arnau/SSD/VizWiz/data/captioning/train/'\n",
    "val_dir = '/media/arnau/SSD/VizWiz/data/captioning/val/'\n",
    "test_dir = '/media/arnau/SSD/VizWiz/data/captioning/test/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11be330d-c6be-4bc3-a542-53f04dc210e5",
   "metadata": {},
   "source": [
    "Train / Val / Test splits images **annotations**. Used to retrieve if there is **text present** in images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c842096c-4a9f-402c-b9a0-552404db67c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(annots_txtp_dir + \"train.json\", encoding='UTF-8') as tr_json_file:\n",
    "    train_data = json.load(tr_json_file)\n",
    "    train_annots = train_data['images']\n",
    "\n",
    "with open(annots_txtp_dir + \"val.json\", encoding='UTF-8') as v_json_file:\n",
    "    val_data = json.load(v_json_file)\n",
    "    val_annots = val_data['images']\n",
    "\n",
    "with open(annots_txtp_dir + \"test.json\", encoding='UTF-8') as ts_json_file:\n",
    "    test_data = json.load(ts_json_file)\n",
    "    test_annots = test_data['images']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36433e0d-4a7b-46cb-bb10-aeda9d0f17bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load custom test data to ensure that val images are not in test set\n",
    "test_vqa_data = '/media/arnau/SSD/VizWiz/models/hf_model_test_res.json'\n",
    "\n",
    "with open(test_vqa_data) as user_file:\n",
    "    test_data = json.load(user_file)\n",
    "\n",
    "model_test_imgs = list(test_data.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c871e0-6012-4c3b-9190-8c7cac4ba6d0",
   "metadata": {
    "tags": []
   },
   "source": [
    "Images containing text for each split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91100f19-7913-4ae7-88f6-b39079c9a177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14701 training images containing text\n",
      "5018 training images containing text\n",
      "5093 training images containing text\n"
     ]
    }
   ],
   "source": [
    "train_imgs_w_text = []\n",
    "val_imgs_w_text = []\n",
    "test_imgs_w_text = []\n",
    "\n",
    "annots = {'train': train_annots, \n",
    "        'val' : val_annots,\n",
    "        'test' : test_annots}\n",
    "\n",
    "for split, data in annots.items():\n",
    "    for d in data:\n",
    "        if d[\"text_detected\"] == True and split == 'train':\n",
    "            train_imgs_w_text .append(d[\"file_name\"])\n",
    "        elif d[\"text_detected\"] == True and split == 'val':\n",
    "            val_imgs_w_text .append(d[\"file_name\"])\n",
    "        elif d[\"text_detected\"] == True and split == 'test':\n",
    "            test_imgs_w_text .append(d[\"file_name\"])\n",
    "\n",
    "print(f\"{len(train_imgs_w_text)} training images containing text\")\n",
    "print(f\"{len(val_imgs_w_text)} training images containing text\")\n",
    "print(f\"{len(test_imgs_w_text)} training images containing text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495ff46f-25f5-4d63-92bb-8eb8e77291d0",
   "metadata": {},
   "source": [
    "Train / Val / Test splits images **quality annotations**. Used to retrieve **quality flaws** in images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13174fa6-a19c-4f23-9f5c-21b45c5d7377",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(annots_qi_dir + \"train.json\", encoding='UTF-8') as tqif:\n",
    "    train_qi_data = json.load(tqif)\n",
    "    \n",
    "with open(annots_qi_dir + \"val.json\", encoding='UTF-8') as vqif:\n",
    "    val_qi_data = json.load(vqif)\n",
    "    \n",
    "with open(annots_qi_dir + \"test.json\", encoding='UTF-8') as tsqif:\n",
    "    test_qi_data = json.load(tsqif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f153a81-706b-46ec-ada7-e843e969bd1d",
   "metadata": {},
   "source": [
    "Train / Val / Test splits images **VQA annotations**. Used to retrieve **unanswerability** in images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5bfae60b-17bf-4dcb-9f66-c53f7af004b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(annots_vqa_dir + \"train.json\", encoding='UTF-8') as tqif:\n",
    "    train_vqa_data = json.load(tqif)\n",
    "    \n",
    "with open(annots_vqa_dir + \"val.json\", encoding='UTF-8') as vqif:\n",
    "    val_vqa_data = json.load(vqif)\n",
    "    \n",
    "with open(annots_vqa_dir + \"test.json\", encoding='UTF-8') as tsqif:\n",
    "    test_vqa_data = json.load(tsqif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be496eff-e3cc-4271-81c6-40c4d12e69ad",
   "metadata": {},
   "source": [
    "**Specify flaw:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00855300",
   "metadata": {},
   "outputs": [],
   "source": [
    "flaw = \"FRM\"\n",
    "lvl = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13127511-11d4-4c28-baaf-78fa45423ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If flaw == FRM we need unanswerability data\n",
    "train_unanswerable_images = [data[\"image\"] for data in train_vqa_data if data['answerable'] == 0]\n",
    "val_unanswerable_images = [data[\"image\"] for data in val_vqa_data if data['answerable'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd5d0ee8-eb6e-426f-b02e-72d8edbe57b6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1617 training images FRM with text\n",
      "4496 training images clear with text\n",
      "462 val images FRM with text\n",
      "1049 val images clear with text\n"
     ]
    }
   ],
   "source": [
    "flawed_train_images_with_text = []\n",
    "clear_train_images_with_text = []\n",
    "\n",
    "flawed_val_images_with_text = []\n",
    "clear_val_images_with_text = []\n",
    "\n",
    "flawed_test_images_with_text = []\n",
    "clear_test_images_with_text = []\n",
    "\n",
    "annots_qi = {'train': train_qi_data, \n",
    "            'val' : val_qi_data}\n",
    "\n",
    "for split, data in annots_qi.items():\n",
    "    for d in data:\n",
    "        # if image has text\n",
    "        if d[\"image\"] in train_imgs_w_text:\n",
    "            # if flaw is out of frame (and unanswerable)\n",
    "            if flaw == \"FRM\" and d[\"flaws\"][flaw] >= lvl:\n",
    "                if d[\"image\"] in train_unanswerable_images:\n",
    "                    flawed_train_images_with_text.append(d[\"image\"])\n",
    "            # if image is blurred\n",
    "            elif flaw == \"BLR\" and d[\"flaws\"][flaw] >= lvl:\n",
    "                flawed_train_images_with_text.append(d[\"image\"])\n",
    "            # if image is clear\n",
    "            elif d[\"flaws\"][\"NON\"] >= lvl: \n",
    "                clear_train_images_with_text.append(d[\"image\"])\n",
    "        \n",
    "        # Since we use a custom test set (extracted from val set) that also contains \n",
    "        # images from VizWiz, we have to check that the validation images are not in the test set. \n",
    "        # More on why we do this in vqa_hf notebook\n",
    "        elif d[\"image\"] in val_imgs_w_text and d[\"image\"] not in model_test_imgs:\n",
    "            # if image is out of frame (and unanswerable)\n",
    "            if flaw == \"FRM\" and d[\"flaws\"][flaw] >= lvl:\n",
    "                if d[\"image\"] in val_unanswerable_images:\n",
    "                    flawed_val_images_with_text.append(d[\"image\"])\n",
    "            # if image is blurred\n",
    "            elif flaw == \"BLR\" and d[\"flaws\"][flaw] >= lvl:\n",
    "                flawed_val_images_with_text.append(d[\"image\"])\n",
    "            # if image is clear\n",
    "            elif d[\"flaws\"][\"NON\"] >= lvl: \n",
    "                clear_val_images_with_text.append(d[\"image\"])\n",
    "            \n",
    "            \n",
    "print(f\"{len(flawed_train_images_with_text)} training images {flaw} with text\")\n",
    "print(f\"{len(clear_train_images_with_text)} training images clear with text\")\n",
    "\n",
    "print(f\"{len(flawed_val_images_with_text)} val images {flaw} with text\")\n",
    "print(f\"{len(clear_val_images_with_text)} val images clear with text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e5f08b38-7613-4d21-bfe2-80483f50ef4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def balance(l):  \n",
    "    \"\"\"\n",
    "    Balance a given list to have the same proportion of elements\n",
    "    for each class\n",
    "    \"\"\"\n",
    "    \n",
    "    n = len(l) // 2\n",
    "    arr = np.array(l, dtype=object)\n",
    "    zeros = arr[arr[:,1] == 0]\n",
    "    ones = arr[arr[:,1] == 1]\n",
    "    np.random.shuffle(zeros)\n",
    "    np.random.shuffle(ones)\n",
    "    final_arr = np.concatenate((zeros[:n], ones[:n]))\n",
    "    final_list = list(map(tuple, final_arr))\n",
    "    \n",
    "    return final_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7bb006c-8fb0-4538-8a43-05407feae0c2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Train data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f499cb78-023c-4ee0-9ee5-924cbd63a280",
   "metadata": {
    "tags": []
   },
   "source": [
    "Train set containing flawed and non-flawed (clear) images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "62fa9ffa-6c8b-4cb7-973f-0c15e247a4b6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VizWiz TRAIN set size 4673\n"
     ]
    }
   ],
   "source": [
    "flawed_train_dataset = np.asarray(list(map(lambda im : (im, 1), flawed_train_images_with_text)), dtype=object)\n",
    "clear_train_dataset = np.asarray(list(map(lambda im : (im, 0), clear_train_images_with_text)), dtype=object)\n",
    "\n",
    "vw_train_set = np.asarray(balance(\n",
    "                                np.vstack((flawed_train_dataset, clear_train_dataset)\n",
    "                                         )), \n",
    "                          dtype=object)\n",
    "\n",
    "print(f\"VizWiz TRAIN set size {vw_train_set.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef6343a-3f91-4e8b-93b5-4e7311899af0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Val + Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b970f1ab-3987-4874-93f7-187fb6237dcc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "flawed_val_dataset = np.asarray(list(map(lambda im : (im, 1), flawed_val_images_with_text)), dtype=object)\n",
    "clear_val_dataset = np.asarray(list(map(lambda im : (im, 0), clear_val_images_with_text)), dtype=object)\n",
    "\n",
    "test_n_val_set = np.asarray(balance(\n",
    "                                np.vstack((flawed_val_dataset, clear_val_dataset))\n",
    "                        ), \n",
    "                        dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e8f40abd-72fa-4928-9552-26c9f2eaf15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(test_n_val_set)\n",
    "# Half data for val set, half data for test set\n",
    "test_set = test_n_val_set[: len(test_n_val_set) // 2]\n",
    "val_set = test_n_val_set[len(test_n_val_set) // 2: ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b829f19a-6462-427b-970b-ff88416ce61e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VizWiz TEST set size 544\n",
      "VizWiz VAL set size 526\n"
     ]
    }
   ],
   "source": [
    "vw_test_set = np.asarray(balance(test_set), dtype=object)\n",
    "vw_val_set = np.asarray(balance(val_set), dtype=object)\n",
    "\n",
    "print(f\"VizWiz TEST set size {vw_test_set.shape[0]}\")\n",
    "print(f\"VizWiz VAL set size {vw_val_set.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d82450fc-8f50-482b-a2e6-3a4bd0fc186d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure there is not test data in val set\n",
    "for img in list(vw_test_set[:, 0]):\n",
    "    assert img not in list(vw_val_set[:, 0]), \"ERROR\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f40084-de0a-439b-91d8-b37b3cf53ca8",
   "metadata": {},
   "source": [
    "Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9e03b9d4-5e5e-4685-a591-bafd1cac8a21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files saved\n"
     ]
    }
   ],
   "source": [
    "total_size = len(vw_test_set) + len(vw_val_set) + len(vw_train_set)\n",
    "\n",
    "np.random.shuffle(vw_test_set)\n",
    "np.random.shuffle(vw_train_set)\n",
    "np.random.shuffle(vw_val_set)\n",
    "\n",
    "vw_test_set = vw_test_set[: int(total_size * 0.1)]\n",
    "vw_val_set = vw_val_set[: int(total_size * 0.1)]\n",
    "vw_train_set = vw_train_set[: int(total_size * 0.8)]\n",
    "\n",
    "vw_data = {'train' : vw_train_set,\n",
    "           'val' : vw_val_set,\n",
    "           'test' : vw_test_set,\n",
    "          }\n",
    "\n",
    "class NumpyArrayEncoder(JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        return JSONEncoder.default(self, obj)\n",
    "\n",
    "with open(f'/home/arnau/tfg/GED-TFG/data/vw_{flaw}_dataset.json', 'w') as outfile:\n",
    "    json.dump(vw_data, outfile, cls=NumpyArrayEncoder)\n",
    "print(\"Files saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af4433e-bc62-4179-bb3f-263c7b1cce45",
   "metadata": {},
   "source": [
    "## Multiclass data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fb92fe1d-8094-47ed-94c0-8be1a298310e",
   "metadata": {},
   "outputs": [],
   "source": [
    "blurred_train_images_with_text = []\n",
    "blurred_val_images_with_text = []\n",
    "\n",
    "clean_train_images_with_text = []\n",
    "clean_val_images_with_text = []\n",
    "\n",
    "frm_train_images_with_text = []\n",
    "frm_val_images_with_text = []\n",
    "\n",
    "frm_blr_train_images_with_text = []\n",
    "frm_blr_val_images_with_text = []\n",
    "\n",
    "\n",
    "annots_qi = {'train': train_qi_data, \n",
    "            'val' : val_qi_data}\n",
    "\n",
    "for split, data in annots_qi.items():\n",
    "    for d in data:\n",
    "        # if image has text\n",
    "        if d[\"image\"] in train_imgs_w_text: \n",
    "            # if image is blur (and not out of frame)\n",
    "            if d[\"flaws\"][\"BLR\"] >= lvl and d[\"flaws\"][\"FRM\"] <= lvl: \n",
    "                data = [d[\"image\"], 0, 1, 0] # [img, clean, blr, frm]\n",
    "                blurred_train_images_with_text.append(data) \n",
    "            # if image is clear \n",
    "            elif d[\"flaws\"][\"NON\"] >= lvl: \n",
    "                data = [d[\"image\"], 1, 0, 0]\n",
    "                clean_train_images_with_text.append(data)\n",
    "            # if image is out of frame and unanswerable\n",
    "            elif d[\"flaws\"][\"FRM\"] >= lvl and d[\"flaws\"][\"BLR\"] <= lvl:\n",
    "                if d[\"image\"] in train_unanswerable_images:\n",
    "                    data = [d[\"image\"], 0, 0, 1]\n",
    "                    frm_train_images_with_text.append(data)\n",
    "\n",
    "        elif d[\"image\"] in val_imgs_w_text and d[\"image\"] not in model_test_imgs: \n",
    "            if d[\"flaws\"][\"BLR\"] >= lvl and d[\"flaws\"][\"FRM\"] <= lvl:\n",
    "                data = [d[\"image\"], 0, 1, 0]\n",
    "                blurred_val_images_with_text.append(data)\n",
    "            elif d[\"flaws\"][\"NON\"] >= lvl: \n",
    "                data = [d[\"image\"], 1, 0, 0]\n",
    "                clean_val_images_with_text.append(data)\n",
    "            elif d[\"flaws\"][\"FRM\"] >= lvl and d[\"flaws\"][\"BLR\"] <= lvl:\n",
    "                if d[\"image\"] in val_unanswerable_images:\n",
    "                    data = [d[\"image\"],0, 0, 1]\n",
    "                    frm_val_images_with_text.append(data)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "99d3f758-8e61-473d-98f1-709108064adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def undersample(arr, min_len_array):\n",
    "    to_remove = (len(arr) - min_len_array) // 2\n",
    "    \n",
    "    idx_delete_0 = np.where(arr[:, 1] == 0)[0][: to_remove]\n",
    "    idx_delete_1 = np.where(arr[:, 1] == 1)[0][: to_remove]\n",
    "    idx_to_remove = np.hstack([idx_delete_0, idx_delete_1])\n",
    "    res = np.delete(arr, idx_to_remove, axis=0)\n",
    "    \n",
    "    # Equate shapes removing random element\n",
    "    if len(res) != min_len_array:\n",
    "        diff = len(res) - min_len_array \n",
    "        rand_idx_remove = random.sample(range(0, len(res)), diff)\n",
    "        res = np.delete(res, rand_idx_remove, axis=0)  \n",
    "    \n",
    "    return np.array(res, dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "48e65830-81ab-4f3b-b994-dc53bd1d2e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.vstack([np.array(blurred_train_images_with_text, dtype=object),\n",
    "                        np.array(frm_train_images_with_text, dtype=object),\n",
    "                        np.array(clean_train_images_with_text, dtype=object)])\n",
    "\n",
    "val_data = np.vstack([np.array(blurred_val_images_with_text[: len(blurred_val_images_with_text) // 2], dtype=object),\n",
    "                        np.array(frm_val_images_with_text[: len(frm_val_images_with_text) // 2], dtype=object),\n",
    "                        np.array(clean_val_images_with_text[: len(clean_val_images_with_text) // 2], dtype=object)])\n",
    "\n",
    "test_data = np.vstack([np.array(blurred_val_images_with_text[len(blurred_val_images_with_text) // 2:], dtype=object),\n",
    "                        np.array(frm_val_images_with_text[len(frm_val_images_with_text) // 2:], dtype=object),\n",
    "                        np.array(clean_val_images_with_text[len(clean_val_images_with_text) // 2:], dtype=object)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e7027a-e1d3-498f-98a7-293aee7667db",
   "metadata": {},
   "source": [
    "Number of samples per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a78c028d-037a-44cc-9a2e-9256c05e6b15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN\n",
      "clean -- n_samples = 4496\n",
      "BLR -- n_samples = 3218\n",
      "FRM -- n_samples = 990\n",
      "\n",
      "VAL\n",
      "clean -- n_samples = 524\n",
      "BLR -- n_samples = 475\n",
      "FRM -- n_samples = 126\n",
      "\n",
      "TEST\n",
      "clean -- n_samples = 525\n",
      "BLR -- n_samples = 476\n",
      "FRM -- n_samples = 126\n"
     ]
    }
   ],
   "source": [
    "classess = {1 : \"clean\", 2 : \"BLR\", 3 : \"FRM\"}\n",
    "\n",
    "# data augmentation taking clean class as reference\n",
    "print(\"TRAIN\")\n",
    "for cidx, classname in classess.items():\n",
    "    class_samples = train_data[np.where(train_data[:, cidx] == 1)]\n",
    "    print(f\"{classname} -- n_samples = {class_samples.shape[0]}\")\n",
    "    \n",
    "print(\"\\nVAL\")\n",
    "for cidx, classname in classess.items():\n",
    "    class_samples = val_data[np.where(val_data[:, cidx] == 1)]\n",
    "    print(f\"{classname} -- n_samples = {class_samples.shape[0]}\")\n",
    "    \n",
    "print(\"\\nTEST\")\n",
    "for cidx, classname in classess.items():\n",
    "    class_samples = test_data[np.where(test_data[:, cidx] == 1)]\n",
    "    print(f\"{classname} -- n_samples = {class_samples.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "11f29c65-f403-4181-9a39-c699c48a6ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for split in [\"train\", \"val\", \"test\"]:\n",
    "    if split == \"train\":\n",
    "        data = train_data\n",
    "        direc = train_dir\n",
    "    elif split == \"val\":\n",
    "        data = val_data\n",
    "        direc = val_dir\n",
    "    elif split == \"test\":\n",
    "        data = test_data\n",
    "        direc = val_dir # *!*\n",
    "        \n",
    "    for img_data in data:\n",
    "        img_name = img_data[0]\n",
    "        source_file = direc + img_name\n",
    "        destination_folder = f'/media/arnau/SSD/VizWiz/models/multiclass/{split}/'\n",
    "    \n",
    "        if img_name not in os.listdir(destination_folder):\n",
    "            shutil.copy2(source_file, destination_folder)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8a7a8ec5-e3d6-446c-adb5-ed6d259c9d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_size = len(train_data) + len(val_data) + len(test_data)\n",
    "\n",
    "np.random.shuffle(train_data)\n",
    "np.random.shuffle(val_data)\n",
    "np.random.shuffle(test_data)\n",
    "\n",
    "# **No need to split (80/10/10) since it is alredy that proportion**\n",
    "\n",
    "vw_mc_data = {'train' : train_data,\n",
    "           'val' : val_data,\n",
    "           'test' : test_data,\n",
    "          }\n",
    "\n",
    "class NumpyArrayEncoder(JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        return JSONEncoder.default(self, obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "44d07bf8-6f20-445e-8d42-696fb53b61e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files saved\n"
     ]
    }
   ],
   "source": [
    "with open(f'/home/arnau/tfg/GED-TFG/data/vw_MC_dataset.json', 'w') as outfile:\n",
    "    json.dump(vw_mc_data, outfile, cls=NumpyArrayEncoder)\n",
    "print(\"Files saved\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfg env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
