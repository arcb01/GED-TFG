{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e88b10d-c975-4e46-a20b-b0b7ede80a8b",
   "metadata": {},
   "source": [
    "# VQA model accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "12bf7b7f-eb81-43af-aecb-e51072f7222a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version:  1.13.1+cu117\n",
      "Torchvision Version:  0.14.1+cu117\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function \n",
    "from __future__ import division\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torchvision\n",
    "import pprint\n",
    "import pandas as pd\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import time\n",
    "import re, sys\n",
    "import os\n",
    "import copy\n",
    "import shutil\n",
    "import io\n",
    "print(\"PyTorch Version: \",torch.__version__)\n",
    "print(\"Torchvision Version: \",torchvision.__version__)\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Detect if we have a GPU available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a061b5f7-70b1-4483-8321-e12a0159da30",
   "metadata": {},
   "source": [
    "**Load VizWiz test set data** (with PromptCap model output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "f90cba26-d99b-4330-be86-a3667c47e754",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/media/arnau/SSD/VizWiz/models/hf_model_test_res.json', encoding='UTF-8') as jf:\n",
    "    model_output_data = json.load(jf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451cac5e-4941-4f25-b388-d488ce7639f9",
   "metadata": {},
   "source": [
    "**Load VizWiz test set data** (with extra models output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "f0b6f400-1cd5-4443-8be6-318f52670d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/media/arnau/SSD/VizWiz/models/hf_extra_models_test_res.json', encoding='UTF-8') as jf:\n",
    "    other_models_output_data = json.load(jf)\n",
    "\n",
    "# Add GT answers\n",
    "for img_name, data in other_models_output_data.items():\n",
    "    other_models_output_data[img_name][\"gt_answers\"] = model_output_data[img_name][\"gt_answers\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "e409cf14-3a66-42ad-adaa-b3be238b6069",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_imgs = os.listdir('/media/arnau/SSD/VizWiz/data/captioning/val/')\n",
    "test_imgs = list(model_output_data.keys())\n",
    "test_fin_imgs = [img for img in test_imgs if img in val_imgs]\n",
    "\n",
    "if os.path.exists(\"/media/arnau/SSD/VizWiz/data/test_imgs/\") == False:\n",
    "    os.mkdir(\"/media/arnau/SSD/VizWiz/data/test_imgs/\")\n",
    "    \n",
    "    # Copy test images to separate folder\n",
    "    for img in test_fin_imgs:\n",
    "        original = f'/media/arnau/SSD/VizWiz/data/captioning/val/{img}'\n",
    "        target = f\"/media/arnau/SSD/VizWiz/data/test_imgs/{img}\"\n",
    "        shutil.copyfile(original, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "89ac5464-6103-4a96-8c45-95ebf8d1df01",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_model_types = lambda d : [el \n",
    "                               for el in list(d[next(iter(d))].keys()) \n",
    "                               if el not in ['question', 'gt_answers']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e42c6e-a17e-46e8-b73e-6dd60e107bf3",
   "metadata": {},
   "source": [
    "Merge both dictionaries into a single one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "cd031dc3-0e6a-42b8-b908-1ae7fe8efa8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = model_output_data.copy()\n",
    "\n",
    "for typ in get_model_types(other_models_output_data):\n",
    "    for img_name, data in merged.items():\n",
    "        try:\n",
    "            other_model_result = other_models_output_data[img_name][typ]\n",
    "            merged[img_name][typ] = other_model_result\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "model_output_data = merged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26260e5-70c1-4633-9de3-2a7f48919d6f",
   "metadata": {},
   "source": [
    "Show all models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "9efb2923-54b4-4b77-9cad-721b42461754",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['HF_OCR_google_answer',\n",
       " 'HF_OCR_answer',\n",
       " 'HF_NON_OCR_answer',\n",
       " 'ViLT',\n",
       " 'SF_LAVIS',\n",
       " 'BLIP2-VQA',\n",
       " 'BLIP2-CTX-VQA']"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_model_types(model_output_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85079a5f-4dcc-45da-8c6f-c474aa6c56b0",
   "metadata": {},
   "source": [
    "## Models accuracy\n",
    "Various VQA models evaluated on VizWiz test set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60c6402-d522-4d73-9fa8-2813c070d344",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "9f0bcb0e-b174-476b-a9cf-7639bea3b4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_periods(q):\n",
    "    \n",
    "    outText = q\n",
    "    puncts = [';', r\"/\", '[', ']', '\"', '{', '}',\n",
    "                                 '(', ')', '=', '+', '\\\\', '_', '-',\n",
    "                                 '>', '<', '@', '`', ',', '?', '!']\n",
    "    comma_strip = re.compile(\"(\\d)(\\,)(\\d)\")\n",
    "    periodStrip  = re.compile(\"(?!<=\\d)(\\.)(?!\\d)\")\n",
    "    for p in puncts:\n",
    "        if (p + ' ' in q or ' ' + p in q) or (re.search(comma_strip, q) != None):\n",
    "            outText = outText.replace(p, '')\n",
    "        else:\n",
    "            outText = outText.replace(p, ' ')\n",
    "            outText = periodStrip.sub(\"\",\n",
    "                                           outText,\n",
    "                                           re.UNICODE)\n",
    "    return outText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "c5f9e30c-22f0-4d65-95e3-1434664d9dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def num_words_to_digit(q):\n",
    "    \n",
    "    outText = []\n",
    "    tempText = q.lower().split()\n",
    "    manualMap  = { 'none': '0',\n",
    "                    'zero': '0',\n",
    "                    'one': '1',\n",
    "                     'two': '2',\n",
    "                     'three': '3',\n",
    "                     'four': '4',\n",
    "                     'five': '5',\n",
    "                     'six': '6',\n",
    "                     'seven': '7',\n",
    "                     'eight': '8',\n",
    "                     'nine': '9',\n",
    "                     'ten': '10'}\n",
    "    articles     = ['a',\n",
    "                             'an',\n",
    "                             'the'\n",
    "                    ]\n",
    "    contractions = {\"aint\": \"ain't\", \"arent\": \"aren't\", \"cant\": \"can't\", \"couldve\": \"could've\", \"couldnt\": \"couldn't\", \\\n",
    "                            \"couldn'tve\": \"couldn't've\", \"couldnt've\": \"couldn't've\", \"didnt\": \"didn't\", \"doesnt\": \"doesn't\", \"dont\": \"don't\", \"hadnt\": \"hadn't\", \\\n",
    "                            \"hadnt've\": \"hadn't've\", \"hadn'tve\": \"hadn't've\", \"hasnt\": \"hasn't\", \"havent\": \"haven't\", \"hed\": \"he'd\", \"hed've\": \"he'd've\", \\\n",
    "                            \"he'dve\": \"he'd've\", \"hes\": \"he's\", \"howd\": \"how'd\", \"howll\": \"how'll\", \"hows\": \"how's\", \"Id've\": \"I'd've\", \"I'dve\": \"I'd've\", \\\n",
    "                            \"Im\": \"I'm\", \"Ive\": \"I've\", \"isnt\": \"isn't\", \"itd\": \"it'd\", \"itd've\": \"it'd've\", \"it'dve\": \"it'd've\", \"itll\": \"it'll\", \"let's\": \"let's\", \\\n",
    "                            \"maam\": \"ma'am\", \"mightnt\": \"mightn't\", \"mightnt've\": \"mightn't've\", \"mightn'tve\": \"mightn't've\", \"mightve\": \"might've\", \\\n",
    "                            \"mustnt\": \"mustn't\", \"mustve\": \"must've\", \"neednt\": \"needn't\", \"notve\": \"not've\", \"oclock\": \"o'clock\", \"oughtnt\": \"oughtn't\", \\\n",
    "                            \"ow's'at\": \"'ow's'at\", \"'ows'at\": \"'ow's'at\", \"'ow'sat\": \"'ow's'at\", \"shant\": \"shan't\", \"shed've\": \"she'd've\", \"she'dve\": \"she'd've\", \\\n",
    "                            \"she's\": \"she's\", \"shouldve\": \"should've\", \"shouldnt\": \"shouldn't\", \"shouldnt've\": \"shouldn't've\", \"shouldn'tve\": \"shouldn't've\", \\\n",
    "                            \"somebody'd\": \"somebodyd\", \"somebodyd've\": \"somebody'd've\", \"somebody'dve\": \"somebody'd've\", \"somebodyll\": \"somebody'll\", \\\n",
    "                            \"somebodys\": \"somebody's\", \"someoned\": \"someone'd\", \"someoned've\": \"someone'd've\", \"someone'dve\": \"someone'd've\", \\\n",
    "                            \"someonell\": \"someone'll\", \"someones\": \"someone's\", \"somethingd\": \"something'd\", \"somethingd've\": \"something'd've\", \\\n",
    "                            \"something'dve\": \"something'd've\", \"somethingll\": \"something'll\", \"thats\": \"that's\", \"thered\": \"there'd\", \"thered've\": \"there'd've\", \\\n",
    "                            \"there'dve\": \"there'd've\", \"therere\": \"there're\", \"theres\": \"there's\", \"theyd\": \"they'd\", \"theyd've\": \"they'd've\", \\\n",
    "                            \"they'dve\": \"they'd've\", \"theyll\": \"they'll\", \"theyre\": \"they're\", \"theyve\": \"they've\", \"twas\": \"'twas\", \"wasnt\": \"wasn't\", \\\n",
    "                            \"wed've\": \"we'd've\", \"we'dve\": \"we'd've\", \"weve\": \"we've\", \"werent\": \"weren't\", \"whatll\": \"what'll\", \"whatre\": \"what're\", \\\n",
    "                            \"whats\": \"what's\", \"whatve\": \"what've\", \"whens\": \"when's\", \"whered\": \"where'd\", \"wheres\": \"where's\", \"whereve\": \"where've\", \\\n",
    "                            \"whod\": \"who'd\", \"whod've\": \"who'd've\", \"who'dve\": \"who'd've\", \"wholl\": \"who'll\", \"whos\": \"who's\", \"whove\": \"who've\", \"whyll\": \"why'll\", \\\n",
    "                            \"whyre\": \"why're\", \"whys\": \"why's\", \"wont\": \"won't\", \"wouldve\": \"would've\", \"wouldnt\": \"wouldn't\", \"wouldnt've\": \"wouldn't've\", \\\n",
    "                            \"wouldn'tve\": \"wouldn't've\", \"yall\": \"y'all\", \"yall'll\": \"y'all'll\", \"y'allll\": \"y'all'll\", \"yall'd've\": \"y'all'd've\", \\\n",
    "                            \"y'alld've\": \"y'all'd've\", \"y'all'dve\": \"y'all'd've\", \"youd\": \"you'd\", \"youd've\": \"you'd've\", \"you'dve\": \"you'd've\", \\\n",
    "                            \"youll\": \"you'll\", \"youre\": \"you're\", \"youve\": \"you've\"\n",
    "                            }\n",
    "    for word in tempText:\n",
    "        word = manualMap.setdefault(word, word)\n",
    "        if word not in articles:\n",
    "            outText.append(word)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    for wordId, word in enumerate(outText):\n",
    "        if word in contractions:\n",
    "            outText[wordId] = contractions[word]\n",
    "    outText = ' '.join(outText)\n",
    "    return outText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "8c1a1e4a-34de-4424-9c70-53513e90110c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateProgress(progress):\n",
    "    \n",
    "    barLength = 20\n",
    "    status = \"\"\n",
    "    if isinstance(progress, int):\n",
    "        progress = float(progress)\n",
    "    if not isinstance(progress, float):\n",
    "        progress = 0\n",
    "        status = \"error: progress var must be float\\r\\n\"\n",
    "    if progress < 0:\n",
    "        progress = 0\n",
    "        status = \"Halt...\\r\\n\"\n",
    "    if progress >= 1:\n",
    "        progress = 1\n",
    "        status = \"Done...\\r\\n\"\n",
    "    block = int(round(barLength*progress))\n",
    "    text = \"\\rFinshed Percent: [{0}] {1}% {2}\".format( \"#\"*block + \"-\"*(barLength-block), int(progress*100), status)\n",
    "    sys.stdout.write(text)\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891890c7-7dec-4199-9ba0-da73929c2a02",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "44f55e0e-ae3f-412e-a7bb-36eb13b31644",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_acc_promptcap(model_output_data, model_type):\n",
    "    \"\"\" \n",
    "    Calculates accuracy metric for VQA model\n",
    "    \"\"\"\n",
    "    \n",
    "    accQA       = []\n",
    "    step = 0\n",
    "    \n",
    "    data = model_output_data.copy()\n",
    "    \n",
    "    for img, img_info in data.items():\n",
    "        question = img_info[\"question\"]\n",
    "        gtanswers = img_info[\"gt_answers\"]\n",
    "        for ans in gtanswers:\n",
    "            if type(ans['answer']) == str:\n",
    "                ans['answer'] = ans['answer'].replace('\\n', ' ').replace('\\t', ' ').strip().lower()\n",
    "            else:\n",
    "                ans['answer'] = ' '.join(ans['answer'])\n",
    "        try:\n",
    "            resAns = img_info[model_type]\n",
    "            resAns = resAns.replace('\\n', ' ')\n",
    "            resAns = resAns.replace('\\t', ' ')\n",
    "            resAns = resAns.strip()\n",
    "            resAns = resAns.lower()\n",
    "        except:\n",
    "            resAns = ''\n",
    "        gtAcc = []\n",
    "        gtAnswers = [ans[\"answer\"] for ans in gtanswers]\n",
    "\n",
    "        if len(set(gtAnswers)) > 1:\n",
    "            for ans in gtanswers:\n",
    "                if type(ans['answer']) == str:\n",
    "                    ans[\"answer\"] = remove_periods(ans['answer'])\n",
    "                    ans[\"answer\"] = num_words_to_digit(ans['answer'])\n",
    "                else:\n",
    "                    ans[\"answer\"] = remove_periods(' '.join(ans['answer']))\n",
    "                    ans[\"answer\"] = num_words_to_digit(' '.join(ans['answer']))\n",
    "            resAns = remove_periods(resAns)\n",
    "            resAns = num_words_to_digit(resAns)\n",
    "\n",
    "        for ans in gtanswers:\n",
    "            # otherGTAns = [item for item in gtanswers if item!=ans]\n",
    "            matchingAns = [ans for ans in gtanswers if ans['answer'] == resAns]\n",
    "            acc = min(1, float(len(matchingAns))/3)\n",
    "            gtAcc.append(acc)\n",
    "\n",
    "        avgGTAcc = float(sum(gtAcc))/len(gtAcc)\n",
    "        accQA.append(avgGTAcc)\n",
    "\n",
    "        if step%100 == 0:\n",
    "            updateProgress(step/float(len(data.keys())))\n",
    "        step = step + 1\n",
    "    \n",
    "    return accQA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "a24987b1-6077-4844-b0a3-78e9d833de19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: [0.34572715 0.37863144 0.43511959]\n",
      "Standard Deviation: [0.28117462 0.27816829 0.30877242]\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "img_list = list(model_output_data.keys())\n",
    "np.random.shuffle(img_list)\n",
    "img_list = img_list[:25]\n",
    "\n",
    "resized_images = []\n",
    "\n",
    "# Resize the images to a consistent shape\n",
    "target_shape = (300, 300)  # Set your desired target shape here\n",
    "\n",
    "image_path = '/media/arnau/SSD/VizWiz/data/captioning/val/'\n",
    "\n",
    "for img_name in img_list:\n",
    "    image = cv2.imread(image_path + img_name)\n",
    "    resized_image = cv2.resize(image, target_shape)\n",
    "    resized_images.append(resized_image)\n",
    "\n",
    "# Convert the resized_images list to a NumPy array\n",
    "image_array = np.array(resized_images)\n",
    "\n",
    "# Calculate the mean\n",
    "mean = np.mean(image_array, axis=(0, 1, 2)) / 255.0\n",
    "\n",
    "# Calculate the standard deviation\n",
    "std = np.std(image_array, axis=(0, 1, 2)) / 255.0\n",
    "\n",
    "print(\"Mean:\", mean)\n",
    "print(\"Standard Deviation:\", std)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "c1cc814a-923c-4aad-86b5-9c7498a3d795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finshed Percent: [###################-] 93% --> HF_OCR_google_answer: 8.6062%\n",
      "Finshed Percent: [###################-] 93% --> HF_OCR_answer: 7.0159%\n",
      "Finshed Percent: [###################-] 93% --> HF_NON_OCR_answer: 10.6330%\n",
      "Finshed Percent: [###################-] 93% --> ViLT: 9.5104%\n",
      "Finshed Percent: [###################-] 93% --> SF_LAVIS: 15.8403%\n",
      "Finshed Percent: [###################-] 93% --> BLIP2-VQA: 17.2123%\n"
     ]
    }
   ],
   "source": [
    "default_results = []\n",
    "\n",
    "for typ in get_model_types(model_output_data)[:-1]:\n",
    "    res = evaluate_acc_promptcap(model_output_data, typ)\n",
    "    acc = (sum(res) / len(res)) * 100\n",
    "    default_results.append((typ, acc))\n",
    "    print(f\"--> {typ}: {acc:.4f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea52aef-291c-4fc4-881c-9a694fac2304",
   "metadata": {},
   "source": [
    "## Models accuracy using filtering models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a962b338-6de5-40a3-a4e9-a86827c5322a",
   "metadata": {},
   "source": [
    "Models accuracy evaluted on VizWiz test set with a prior filtering of blurred images using a custom blur detection model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "6db8f037-4264-42e5-b86f-6d20bb6ea446",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def model_inference(img, model_params, multiclass=None):\n",
    "    \"\"\"\n",
    "    Returns 1 if the input image is blurred otherwise returns 0\n",
    "    \"\"\"\n",
    "    \n",
    "    #model, device, thr = model_params\n",
    "    model, device = model_params\n",
    "\n",
    "    trnsfm = {\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.33011644, 0.40721696, 0.51554057], \n",
    "                             std=[0.26439891, 0.26886196, 0.27917049])\n",
    "    ])\n",
    "    }\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    if type(img) == str:\n",
    "        test_path = '/media/arnau/SSD/VizWiz/data/captioning/val/'\n",
    "        img_path = os.path.join(test_path, img)\n",
    "        img = Image.open(img_path)\n",
    "\n",
    "    img = trnsfm[\"val\"](img)\n",
    "    img = img[None, :, :, :] \n",
    "    img = img.to(device)\n",
    "    outputs = model(img)\n",
    "\n",
    "    if not multiclass:\n",
    "        pred = torch.sigmoid(outputs.data).cpu().detach().numpy().tolist()[0][0]\n",
    "        pred_res = pred\n",
    "        #pred_res = 1 if float(pred) > float(thr) else 0\n",
    "    else:\n",
    "        # prob of being class0 (clear)\n",
    "        pred = F.softmax(outputs.data).cpu().detach().numpy()[0][0] \n",
    "        #pred_res = 0 if float(pred) > float(thr) else 1\n",
    "        pred_res = 1 - pred # prob flawed = 1 - prob clear\n",
    "        #pred_res = torch.argmax(outputs.data, 1).cpu().detach().numpy()[0]\n",
    "\n",
    "    return pred_res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "2ade40e5-7c04-4092-a3c0-1f89c8a67d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model(model_path, num_classes):\n",
    "    \"\"\"\n",
    "    Initializes the blur model\n",
    "    \"\"\"\n",
    "    \n",
    "    # GPU device\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    model = models.convnext_tiny(weights='IMAGENET1K_V1')\n",
    "\n",
    "    num_ftrs = model.classifier[-1].in_features\n",
    "    model.classifier[-1] = nn.Linear(num_ftrs, num_classes)\n",
    "    \n",
    "    # Load custom blur model\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model = model.to(device)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "0e34f050-9a3b-4c64-b5a4-2d963187a90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clear_images(model_output_data, model_params, multiclass):\n",
    "    \"\"\"\n",
    "    Given a dataset of images, returns a filtered array where the blurred images\n",
    "    have been removed\n",
    "    \"\"\"\n",
    "    \n",
    "    clear_images = {}\n",
    "    \n",
    "    for img, img_info in model_output_data.items():\n",
    "        # Flaw model filter with %\n",
    "        flaw_detection = model_inference(img, model_params, multiclass) # Returns 1 if img is blur otherwise 0\n",
    "        if flaw_detection != 1:\n",
    "            clear_images[img] = img_info\n",
    "            \n",
    "    return clear_images\n",
    " \n",
    "    \n",
    "def percent_imgs_rejected(og_data, filt_data):\n",
    "    #imgs_rejected = len(og_data.keys()) - len(filt_data.keys())\n",
    "    \n",
    "    #if len(filt_data.keys()) == 0:\n",
    "        #p_imgs_rejected = 0.0\n",
    "        \n",
    "    #p_imgs_rejected = (imgs_rejected / len(og_data.keys())) * 100\n",
    "    \n",
    "    p_imgs_rejected =  100. - 100. * len(filt_data.keys()) / len(og_data.keys())\n",
    "    \n",
    "    return p_imgs_rejected\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "0c6a9112-c5c3-4d19-b6c9-e2d2b5c25770",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_path):\n",
    "    flaw = model_path.split(\"_\")[1]\n",
    "    num_classes = 1 if flaw in [\"BLR\", \"FRM\"] else 4\n",
    "\n",
    "\n",
    "    model = initialize_model(model_path, num_classes, )\n",
    "\n",
    "    model_params = (model, device)\n",
    "\n",
    "    last_layer = model_params[0].classifier[-1]\n",
    "    num_classes = last_layer.out_features\n",
    "    multiclass = True if num_classes > 1 else False\n",
    "\n",
    "    return model_params, multiclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "62c78fd5-2e81-4662-a22b-e387a549775e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flaw_model_vqa_acc(vqa_model_type, flaw_model_params):\n",
    "    \"\"\"\n",
    "    For a given Flaw detection model returns a pandas \n",
    "    DataFrame with every image and its corresponding prediction\n",
    "    along with its VQA accuracy\n",
    "    \"\"\"\n",
    "\n",
    "    wdata = {\n",
    "            \"img_name\" : [],\n",
    "            \"prob_flawed\" : [],\n",
    "            \"vqa_acc\" : []\n",
    "            }\n",
    "\n",
    "    # For each img in the test set calculate it's blur_pred and vqa_acc\n",
    "    for i, (img_name, img_info) in enumerate(model_output_data.items()):\n",
    "        # Prediction of the flawed model (e.g 0.98)\n",
    "        img_pred = model_inference(img_name, flaw_model_params[0], multiclass=flaw_model_params[1])\n",
    "        # VQA acc of the image for a specific model \n",
    "        img_vqa_acc = evaluate_acc_promptcap({img_name : img_info}, vqa_model_type)[0]\n",
    "\n",
    "        # Save data to df\n",
    "        wdata[\"img_name\"].append(img_name)\n",
    "        wdata[\"prob_flawed\"].append(img_pred)\n",
    "        wdata[\"vqa_acc\"].append(img_vqa_acc)\n",
    "\n",
    "    # Create dataframe sorted by highest probability of flawed images\n",
    "    df_imgs_vqa_acc = pd.DataFrame.from_dict(wdata).sort_values(\"prob_flawed\", ascending=False)\n",
    "    \n",
    "    return df_imgs_vqa_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "27b56661-ebbe-4a62-804b-259ba19fbfcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_vqa_acc(flaw_model_type):\n",
    "    \"\"\"\n",
    "    Given a Flaw detection model creates a pandas dataframe\n",
    "    with its accuracy improvement for every rejection percentage\n",
    "    Saves the dataframe to a csv file.\n",
    "    \"\"\"\n",
    "\n",
    "    dict_vqa_acc = {    \"vqa_model_type\" : [],\n",
    "                        \"rej_perc\" : [],\n",
    "                        \"vqa_acc\" : []\n",
    "                    }\n",
    "\n",
    "\n",
    "    model_path = '../outputs/best_' + flaw_model_type + '_convnext.pth'\n",
    "    flaw_model_params = load_model(model_path)\n",
    "    # For every vqa model\n",
    "    for vqa_model_type in get_model_types(model_output_data)[:-1]:\n",
    "        # For every rejection percentage\n",
    "        for rr in np.arange(0., 1.0, 0.05):\n",
    "            # Get df with vqa_acc for every image\n",
    "            df_imgs_vqa_acc = flaw_model_vqa_acc(vqa_model_type, flaw_model_params)\n",
    "\n",
    "            n_rej_imgs = int(rr * len(df_imgs_vqa_acc))\n",
    "            \n",
    "            # NOTE: df with good clean images (discarding high acc flaw prob.)\n",
    "            df_imgs_vqa_acc = df_imgs_vqa_acc[n_rej_imgs:]\n",
    "\n",
    "            if rr == 1.0:\n",
    "                acc = 100.\n",
    "            else:\n",
    "                acc = (df_imgs_vqa_acc[\"vqa_acc\"].sum() / len(df_imgs_vqa_acc)) * 100\n",
    "            \n",
    "                #print(f\"Percentage of rej_imgs = {rr*100:.2f}, vqa_acc = {acc:.2f}\")\n",
    "\n",
    "            # add data to dict\n",
    "            dict_vqa_acc[\"vqa_model_type\"].append(vqa_model_type)\n",
    "            dict_vqa_acc[\"rej_perc\"].append(rr * 100)\n",
    "            dict_vqa_acc[\"vqa_acc\"].append(acc)\n",
    "\n",
    "    df = pd.DataFrame.from_dict(dict_vqa_acc)\n",
    "    df.to_csv(f\"../data/{flaw_model_type}_vqa_acc.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "df0452c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_rej_acc_plot(df, flaw):\n",
    "  \"\"\"\n",
    "  Draw rejection percentage vs vqa accuracy plot for each vqa model\n",
    "  of a specific flaw detection model\n",
    "  \"\"\"\n",
    "\n",
    "  fig, ax = plt.subplots(figsize=(6,4))\n",
    "\n",
    "  map_type = { \"HF_OCR_google_answer\" : \"PromptCap + GoogleOCR\",\n",
    "                  \"HF_OCR_answer\" : \"PromptCap + EasyOCR\",\n",
    "                  \"HF_NON_OCR_answer\": \"PromptCap\",\n",
    "              \"SF_LAVIS\" : \"BLIP-VQA\"\n",
    "            }\n",
    "\n",
    "\n",
    "  for typ in get_model_types(model_output_data)[:-1]:\n",
    "    data = df[df[\"vqa_model_type\"] == typ].sort_values('rej_perc')\n",
    "\n",
    "    if typ in map_type.keys():\n",
    "      typ = map_type[typ]\n",
    "\n",
    "    ax.plot(data[\"rej_perc\"].tolist(),\n",
    "            data[\"vqa_acc\"].tolist(), label=typ)\n",
    "    ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.2),\n",
    "          fancybox=True, shadow=True, ncol=5, fontsize=7)\n",
    "    \n",
    "    ax.set_xlabel(\"Percentage of rejected images\")\n",
    "    ax.set_axisbelow(True)\n",
    "    ax.yaxis.grid(color='gray', linewidth=0.15)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    ax.spines['bottom'].set_color('#DDDDDD')\n",
    "    ax.tick_params(bottom=False, left=False)\n",
    "    vals = ax.get_xticks()\n",
    "    ax.set_xticklabels(['{:,.0%}'.format(x/100) for x in vals])\n",
    "    ax.set_ylabel(\"Accuracy (%)\")\n",
    "\n",
    "  fig.savefig(f\"/media/arnau/SSD/VizWiz/plots/{flaw}_vqa_acc.png\", dpi=300, bbox_inches='tight') \n",
    "\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "d26b6faa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finshed Percent: [--------------------] 0% "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3255/3533484795.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pred = F.softmax(outputs.data).cpu().detach().numpy()[0][0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finshed Percent: [--------------------] 0% "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3255/3533484795.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pred = F.softmax(outputs.data).cpu().detach().numpy()[0][0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finshed Percent: [--------------------] 0% "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3255/3533484795.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pred = F.softmax(outputs.data).cpu().detach().numpy()[0][0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finshed Percent: [--------------------] 0% "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3255/3533484795.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pred = F.softmax(outputs.data).cpu().detach().numpy()[0][0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finshed Percent: [--------------------] 0% "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3255/3533484795.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pred = F.softmax(outputs.data).cpu().detach().numpy()[0][0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finshed Percent: [--------------------] 0% "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3255/3533484795.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pred = F.softmax(outputs.data).cpu().detach().numpy()[0][0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finshed Percent: [--------------------] 0% "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3255/3533484795.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pred = F.softmax(outputs.data).cpu().detach().numpy()[0][0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finshed Percent: [--------------------] 0% "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3255/3533484795.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pred = F.softmax(outputs.data).cpu().detach().numpy()[0][0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finshed Percent: [--------------------] 0% "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3255/3533484795.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pred = F.softmax(outputs.data).cpu().detach().numpy()[0][0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finshed Percent: [--------------------] 0% "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3255/3533484795.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pred = F.softmax(outputs.data).cpu().detach().numpy()[0][0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finshed Percent: [--------------------] 0% "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3255/3533484795.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pred = F.softmax(outputs.data).cpu().detach().numpy()[0][0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finshed Percent: [--------------------] 0% "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3255/3533484795.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pred = F.softmax(outputs.data).cpu().detach().numpy()[0][0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finshed Percent: [--------------------] 0% "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3255/3533484795.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pred = F.softmax(outputs.data).cpu().detach().numpy()[0][0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finshed Percent: [--------------------] 0% "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3255/3533484795.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pred = F.softmax(outputs.data).cpu().detach().numpy()[0][0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finshed Percent: [--------------------] 0% "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3255/3533484795.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pred = F.softmax(outputs.data).cpu().detach().numpy()[0][0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finshed Percent: [--------------------] 0% "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3255/3533484795.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pred = F.softmax(outputs.data).cpu().detach().numpy()[0][0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finshed Percent: [--------------------] 0% "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3255/3533484795.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pred = F.softmax(outputs.data).cpu().detach().numpy()[0][0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finshed Percent: [--------------------] 0% "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3255/3533484795.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pred = F.softmax(outputs.data).cpu().detach().numpy()[0][0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finshed Percent: [--------------------] 0% "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3255/3533484795.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pred = F.softmax(outputs.data).cpu().detach().numpy()[0][0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finshed Percent: [--------------------] 0% "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3255/3533484795.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pred = F.softmax(outputs.data).cpu().detach().numpy()[0][0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finshed Percent: [--------------------] 0% "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3255/3533484795.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pred = F.softmax(outputs.data).cpu().detach().numpy()[0][0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finshed Percent: [--------------------] 0% "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3255/3533484795.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pred = F.softmax(outputs.data).cpu().detach().numpy()[0][0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finshed Percent: [--------------------] 0% "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3255/3533484795.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pred = F.softmax(outputs.data).cpu().detach().numpy()[0][0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finshed Percent: [--------------------] 0% "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3255/3533484795.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pred = F.softmax(outputs.data).cpu().detach().numpy()[0][0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finshed Percent: [--------------------] 0% "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3255/3533484795.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pred = F.softmax(outputs.data).cpu().detach().numpy()[0][0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finshed Percent: [--------------------] 0% "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3255/3533484795.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pred = F.softmax(outputs.data).cpu().detach().numpy()[0][0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finshed Percent: [--------------------] 0% "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3255/3533484795.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pred = F.softmax(outputs.data).cpu().detach().numpy()[0][0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finshed Percent: [--------------------] 0% "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3255/3533484795.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pred = F.softmax(outputs.data).cpu().detach().numpy()[0][0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finshed Percent: [--------------------] 0% "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3255/3533484795.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pred = F.softmax(outputs.data).cpu().detach().numpy()[0][0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finshed Percent: [--------------------] 0% "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3255/3533484795.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pred = F.softmax(outputs.data).cpu().detach().numpy()[0][0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finshed Percent: [--------------------] 0% "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3255/3533484795.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pred = F.softmax(outputs.data).cpu().detach().numpy()[0][0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finshed Percent: [--------------------] 0% "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3255/3533484795.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pred = F.softmax(outputs.data).cpu().detach().numpy()[0][0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finshed Percent: [--------------------] 0% "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3255/3533484795.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pred = F.softmax(outputs.data).cpu().detach().numpy()[0][0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finshed Percent: [--------------------] 0% "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3255/3533484795.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pred = F.softmax(outputs.data).cpu().detach().numpy()[0][0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finshed Percent: [--------------------] 0% "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3255/3533484795.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pred = F.softmax(outputs.data).cpu().detach().numpy()[0][0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finshed Percent: [--------------------] 0% "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3255/3533484795.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pred = F.softmax(outputs.data).cpu().detach().numpy()[0][0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finshed Percent: [--------------------] 0% "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3255/3533484795.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pred = F.softmax(outputs.data).cpu().detach().numpy()[0][0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finshed Percent: [--------------------] 0% "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3255/3533484795.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pred = F.softmax(outputs.data).cpu().detach().numpy()[0][0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finshed Percent: [--------------------] 0% "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3255/3533484795.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pred = F.softmax(outputs.data).cpu().detach().numpy()[0][0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finshed Percent: [--------------------] 0% "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3255/3533484795.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pred = F.softmax(outputs.data).cpu().detach().numpy()[0][0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finshed Percent: [--------------------] 0% "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3255/3533484795.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pred = F.softmax(outputs.data).cpu().detach().numpy()[0][0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finshed Percent: [--------------------] 0% "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3255/3533484795.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pred = F.softmax(outputs.data).cpu().detach().numpy()[0][0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finshed Percent: [--------------------] 0% "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3255/3533484795.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pred = F.softmax(outputs.data).cpu().detach().numpy()[0][0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finshed Percent: [--------------------] 0% "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3255/3533484795.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pred = F.softmax(outputs.data).cpu().detach().numpy()[0][0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finshed Percent: [--------------------] 0% "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3255/3533484795.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pred = F.softmax(outputs.data).cpu().detach().numpy()[0][0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finshed Percent: [--------------------] 0% "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3255/3533484795.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pred = F.softmax(outputs.data).cpu().detach().numpy()[0][0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finshed Percent: [--------------------] 0% "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3255/3533484795.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pred = F.softmax(outputs.data).cpu().detach().numpy()[0][0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finshed Percent: [--------------------] 0% "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3255/3533484795.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pred = F.softmax(outputs.data).cpu().detach().numpy()[0][0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finshed Percent: [--------------------] 0% "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3255/3533484795.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pred = F.softmax(outputs.data).cpu().detach().numpy()[0][0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finshed Percent: [--------------------] 0% "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3255/3533484795.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pred = F.softmax(outputs.data).cpu().detach().numpy()[0][0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finshed Percent: [--------------------] 0% "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3255/3533484795.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pred = F.softmax(outputs.data).cpu().detach().numpy()[0][0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finshed Percent: [--------------------] 0% "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3255/3533484795.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pred = F.softmax(outputs.data).cpu().detach().numpy()[0][0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finshed Percent: [--------------------] 0% "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3255/3533484795.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pred = F.softmax(outputs.data).cpu().detach().numpy()[0][0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finshed Percent: [--------------------] 0% "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3255/3533484795.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pred = F.softmax(outputs.data).cpu().detach().numpy()[0][0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finshed Percent: [--------------------] 0% "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[168], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m df_frm \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m\"\u001b[39m\u001b[39m../data/FRM_vqa_acc.csv\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m df_blr \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m\"\u001b[39m\u001b[39m../data/BLR_vqa_acc.csv\u001b[39m\u001b[39m\"\u001b[39m)                                \n\u001b[0;32m----> 4\u001b[0m get_model_vqa_acc(\u001b[39m\"\u001b[39;49m\u001b[39mMC\u001b[39;49m\u001b[39m\"\u001b[39;49m) \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(\u001b[39m\"\u001b[39m\u001b[39m../data/MC_vqa_acc.csv\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m      5\u001b[0m df_mc \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m\"\u001b[39m\u001b[39m../data/MC_vqa_acc.csv\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m draw_rej_acc_plot(df_frm, \u001b[39m\"\u001b[39m\u001b[39mFRM\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[166], line 21\u001b[0m, in \u001b[0;36mget_model_vqa_acc\u001b[0;34m(flaw_model_type)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[39mfor\u001b[39;00m vqa_model_type \u001b[39min\u001b[39;00m get_model_types(model_output_data)[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]:\n\u001b[1;32m     18\u001b[0m     \u001b[39m# For every rejection percentage\u001b[39;00m\n\u001b[1;32m     19\u001b[0m     \u001b[39mfor\u001b[39;00m rr \u001b[39min\u001b[39;00m np\u001b[39m.\u001b[39marange(\u001b[39m0.\u001b[39m, \u001b[39m1.0\u001b[39m, \u001b[39m0.05\u001b[39m):\n\u001b[1;32m     20\u001b[0m         \u001b[39m# Get df with vqa_acc for every image\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m         df_imgs_vqa_acc \u001b[39m=\u001b[39m flaw_model_vqa_acc(vqa_model_type, flaw_model_params)\n\u001b[1;32m     23\u001b[0m         n_rej_imgs \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(rr \u001b[39m*\u001b[39m \u001b[39mlen\u001b[39m(df_imgs_vqa_acc))\n\u001b[1;32m     25\u001b[0m         \u001b[39m# *!* df with good clean images (discarding high acc flaw prob.)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[165], line 17\u001b[0m, in \u001b[0;36mflaw_model_vqa_acc\u001b[0;34m(vqa_model_type, flaw_model_params)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[39m# For each img in the test set calculate it's blur_pred and vqa_acc\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[39mfor\u001b[39;00m i, (img_name, img_info) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(model_output_data\u001b[39m.\u001b[39mitems()):\n\u001b[1;32m     16\u001b[0m     \u001b[39m# Prediction of the flawed model (e.g 0.98)\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m     img_pred \u001b[39m=\u001b[39m model_inference(img_name, flaw_model_params[\u001b[39m0\u001b[39;49m], multiclass\u001b[39m=\u001b[39;49mflaw_model_params[\u001b[39m1\u001b[39;49m])\n\u001b[1;32m     18\u001b[0m     \u001b[39m# VQA acc of the image for a specific model \u001b[39;00m\n\u001b[1;32m     19\u001b[0m     img_vqa_acc \u001b[39m=\u001b[39m evaluate_acc_promptcap({img_name : img_info}, vqa_model_type)[\u001b[39m0\u001b[39m]\n",
      "Cell \u001b[0;32mIn[161], line 27\u001b[0m, in \u001b[0;36mmodel_inference\u001b[0;34m(img, model_params, multiclass)\u001b[0m\n\u001b[1;32m     24\u001b[0m     img_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(test_path, img)\n\u001b[1;32m     25\u001b[0m     img \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mopen(img_path)\n\u001b[0;32m---> 27\u001b[0m img \u001b[39m=\u001b[39m trnsfm[\u001b[39m\"\u001b[39;49m\u001b[39mval\u001b[39;49m\u001b[39m\"\u001b[39;49m](img)\n\u001b[1;32m     28\u001b[0m img \u001b[39m=\u001b[39m img[\u001b[39mNone\u001b[39;00m, :, :, :] \n\u001b[1;32m     29\u001b[0m img \u001b[39m=\u001b[39m img\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/tfg/GED-TFG/env/lib/python3.10/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[39m=\u001b[39m t(img)\n\u001b[1;32m     96\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/tfg/GED-TFG/env/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/tfg/GED-TFG/env/lib/python3.10/site-packages/torchvision/transforms/transforms.py:346\u001b[0m, in \u001b[0;36mResize.forward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    338\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[1;32m    339\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    340\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m    341\u001b[0m \u001b[39m        img (PIL Image or Tensor): Image to be scaled.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[39m        PIL Image or Tensor: Rescaled image.\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 346\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mresize(img, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msize, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minterpolation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_size, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mantialias)\n",
      "File \u001b[0;32m~/tfg/GED-TFG/env/lib/python3.10/site-packages/torchvision/transforms/functional.py:474\u001b[0m, in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation, max_size, antialias)\u001b[0m\n\u001b[1;32m    472\u001b[0m         warnings\u001b[39m.\u001b[39mwarn(\u001b[39m\"\u001b[39m\u001b[39mAnti-alias option is always applied for PIL Image input. Argument antialias is ignored.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    473\u001b[0m     pil_interpolation \u001b[39m=\u001b[39m pil_modes_mapping[interpolation]\n\u001b[0;32m--> 474\u001b[0m     \u001b[39mreturn\u001b[39;00m F_pil\u001b[39m.\u001b[39;49mresize(img, size\u001b[39m=\u001b[39;49moutput_size, interpolation\u001b[39m=\u001b[39;49mpil_interpolation)\n\u001b[1;32m    476\u001b[0m \u001b[39mreturn\u001b[39;00m F_t\u001b[39m.\u001b[39mresize(img, size\u001b[39m=\u001b[39moutput_size, interpolation\u001b[39m=\u001b[39minterpolation\u001b[39m.\u001b[39mvalue, antialias\u001b[39m=\u001b[39mantialias)\n",
      "File \u001b[0;32m~/tfg/GED-TFG/env/lib/python3.10/site-packages/torchvision/transforms/functional_pil.py:252\u001b[0m, in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39misinstance\u001b[39m(size, \u001b[39mlist\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(size) \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m):\n\u001b[1;32m    250\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mGot inappropriate size arg: \u001b[39m\u001b[39m{\u001b[39;00msize\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 252\u001b[0m \u001b[39mreturn\u001b[39;00m img\u001b[39m.\u001b[39;49mresize(\u001b[39mtuple\u001b[39;49m(size[::\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m]), interpolation)\n",
      "File \u001b[0;32m~/tfg/GED-TFG/env/lib/python3.10/site-packages/PIL/Image.py:2156\u001b[0m, in \u001b[0;36mImage.resize\u001b[0;34m(self, size, resample, box, reducing_gap)\u001b[0m\n\u001b[1;32m   2152\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg)\n\u001b[1;32m   2154\u001b[0m size \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(size)\n\u001b[0;32m-> 2156\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mload()\n\u001b[1;32m   2157\u001b[0m \u001b[39mif\u001b[39;00m box \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2158\u001b[0m     box \u001b[39m=\u001b[39m (\u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msize\n",
      "File \u001b[0;32m~/tfg/GED-TFG/env/lib/python3.10/site-packages/PIL/ImageFile.py:269\u001b[0m, in \u001b[0;36mImageFile.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(msg)\n\u001b[1;32m    268\u001b[0m b \u001b[39m=\u001b[39m b \u001b[39m+\u001b[39m s\n\u001b[0;32m--> 269\u001b[0m n, err_code \u001b[39m=\u001b[39m decoder\u001b[39m.\u001b[39;49mdecode(b)\n\u001b[1;32m    270\u001b[0m \u001b[39mif\u001b[39;00m n \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    271\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Load FRM and BLR vqa_acc dataframes\n",
    "df_frm = pd.read_csv(\"../data/FRM_vqa_acc.csv\")\n",
    "df_blr = pd.read_csv(\"../data/BLR_vqa_acc.csv\")                                \n",
    "get_model_vqa_acc(\"MC\") if not os.path.exists(\"../data/MC_vqa_acc.csv\") else None\n",
    "df_mc = pd.read_csv(\"../data/MC_vqa_acc.csv\")\n",
    "\n",
    "draw_rej_acc_plot(df_frm, \"FRM\")\n",
    "draw_rej_acc_plot(df_blr, \"BLR\")\n",
    "draw_rej_acc_plot(df_mc, \"MC\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf245cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3099/75636509.py:1: FutureWarning: The default value of numeric_only in DataFrame.mean is deprecated. In a future version, it will default to False. In addition, specifying 'numeric_only=None' is deprecated. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
      "  mean_models_vqa_acc = df[df[\"rej_perc\"] == 0].mean()[\"vqa_acc\"]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Mean VQA models acc improvement\n",
    "\n",
    "mean_models_vqa_acc = df_blr[df_blr[\"rej_perc\"] == 0].mean()[\"vqa_acc\"]\n",
    "\n",
    "data = { \"model_type\" : [\"Default (base)\"],\n",
    "            \"acc\" : [mean_models_vqa_acc],\n",
    "        }\n",
    "\n",
    "# For every flaw detection model results\n",
    "for i, df in enumerate([df_blr, df_frm]):\n",
    "    df[\"rej_perc\"] = df[\"rej_perc\"].astype(int)\n",
    "    df_50 = df[(df[\"rej_perc\"] == 50)] #& (df[\"rej_perc\"] <= 60)]\n",
    "    mean_flaw_vqa_acc = df_50.groupby(\"vqa_model_type\").mean().mean()[\"vqa_acc\"]\n",
    "\n",
    "    flaw = \"Blur\" if i == 0 else \"Out of Frame\"\n",
    "    data[\"model_type\"].append(flaw)\n",
    "    data[\"acc\"].append(mean_flaw_vqa_acc)\n",
    "\n",
    "flaw_vqa_df = pd.DataFrame.from_dict(data, orient='index').T\n",
    "g = sns.catplot(x=\"model_type\", y=\"acc\", data=flaw_vqa_df,\n",
    "                     aspect=2, kind=\"bar\", legend=False, palette=[\"#90a955\"],\n",
    "                     height=3, width=.4)\n",
    "g.ax.set_axisbelow(True)\n",
    "g.ax.yaxis.grid(color='gray', linewidth=0.15)\n",
    "g.set(xlabel=\"Model approach\", ylabel=\"Accuracy (%)\")\n",
    "g.fig.set_size_inches(5,3)\n",
    "g.despine(left=True, bottom=True)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939c9368",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save VQA models results after implementing flaw detection models\n",
    "\n",
    "map_type = { \"HF_OCR_google_answer\" : \"PromptCap + GoogleOCR\",\n",
    "                \"HF_OCR_answer\" : \"PromptCap + EasyOCR\",\n",
    "                \"HF_NON_OCR_answer\": \"PromptCap\",\n",
    "            \"SF_LAVIS\" : \"BLIP-VQA\",\n",
    "            \"ViLT\" : \"ViLT\",\n",
    "            \"BLIP2-VQA\" : \"BLIP2-VQA\",\n",
    "            \n",
    "        }\n",
    "\n",
    "map_flaw = {0 : \"BLR\", 1 : \"FRM\", 2 : \"MC\"}\n",
    "vqa_models_def_acc = df_blr[df_blr[\"rej_perc\"] == 0]\n",
    "\n",
    "for i, df in enumerate([df_blr, df_frm]):\n",
    "    vqa_models_flaw_acc = df[df[\"rej_perc\"] == 50]\n",
    "    flaw_imprv_df = pd.merge(vqa_models_def_acc, vqa_models_flaw_acc, on=\"vqa_model_type\", \n",
    "                 suffixes=(\"_base\", f\"_{map_flaw[i]}\"))\n",
    "    flaw_imprv_df = flaw_imprv_df.drop(columns=[\"rej_perc_base\", f\"rej_perc_{map_flaw[i]}\"])\n",
    "    flaw_imprv_df[\"vqa_model_type\"] = c[\"vqa_model_type\"].map(map_type)\n",
    "    sf = f\"/media/arnau/SSD/VizWiz/results/{map_flaw[i]}_vqa_models_acc.csv\"\n",
    "    flaw_imprv_df.to_csv(sf, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88105e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3047/136380710.py:22: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.set_xticklabels(labels)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mins = np.append(df_blr.groupby(\"vqa_model_type\")[\"vqa_acc\"].min().min(), \n",
    "          df_frm.groupby(\"vqa_model_type\")[\"vqa_acc\"].min().min())\n",
    "maxes = np.append(df_blr.groupby(\"vqa_model_type\")[\"vqa_acc\"].max().max(), \n",
    "          df_frm.groupby(\"vqa_model_type\")[\"vqa_acc\"].max().max())\n",
    "means = np.append(df_blr.groupby(\"vqa_model_type\")[\"vqa_acc\"].mean().mean(), \n",
    "          df_frm.groupby(\"vqa_model_type\")[\"vqa_acc\"].mean().mean())\n",
    "std = np.append(df_blr.groupby(\"vqa_model_type\")[\"vqa_acc\"].std().std(), \n",
    "          df_frm.groupby(\"vqa_model_type\")[\"vqa_acc\"].std().std())\n",
    "\n",
    "# create stacked errorbars:\n",
    "fig, ax = plt.subplots()\n",
    "#ax.errorbar(np.arange(len(mins)), means, std, fmt='ok', lw=3)\n",
    "ax.errorbar(np.arange(len(mins)), means, [means - mins, maxes - means],\n",
    "             fmt='.k', ecolor='green', lw=2)\n",
    "ax.set_title(\"Accuracy improvement across models for every approach\")\n",
    "\n",
    "labels = ['' for item in ax.get_xticklabels()]\n",
    "labels[1] = \"Blur\"\n",
    "labels[3] = \"Out of frame\"\n",
    "labels[7] = \"VQA-CTX\"\n",
    "\n",
    "ax.set_xticklabels(labels)\n",
    "ax.set_xlabel('Model approach', fontsize=11)\n",
    "ax.set_ylabel('Accuracy improvement (%)', fontsize=11)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9279b7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for flaw_model_type in [\"BLR\", \"FRM\"]:\n",
    "    model_path = '../outputs/best_' + flaw_model_type + '_convnext.pth'\n",
    "    flaw_model_params = load_model(model_path)\n",
    "    dict_vqa_acc = get_dict_vqa_acc()\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a617cec6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2d0681",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3f22b4-8852-4c1e-b0b8-71538f4ceafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\"model_type\": [],\n",
    "        \"thr\": [], \n",
    "        \"p_imgs_rej\": [],\n",
    "      \"acc\": []}\n",
    "\n",
    "# Remap models naming\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1906980-4145-4628-b226-7a35a3c0097b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[39m# For every rejection rate, compares the default model (rej_rates = 0.0) with \u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[39mfor\u001b[39;00m rr \u001b[39min\u001b[39;00m rej_rates:\n\u001b[0;32m----> 6\u001b[0m     filtered_data \u001b[39m=\u001b[39m get_clear_images(model_output_data, (model, device, rr), multiclass\u001b[39m=\u001b[39mmulticlass)\n\u001b[1;32m      7\u001b[0m     \u001b[39mfor\u001b[39;00m typ \u001b[39min\u001b[39;00m get_model_types(model_output_data)[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]: \u001b[39m# Don't use the BLIP2-CTX-VQA model yet (the last one)\u001b[39;00m\n\u001b[1;32m      8\u001b[0m         res \u001b[39m=\u001b[39m evaluate_acc_promptcap(filtered_data, typ)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "blur_results = []\n",
    "rej_rates = np.flip(np.arange(0., 1.1, 0.1), 0)\n",
    "\n",
    "# For every rejection rate, compares the default model (rej_rates = 0.0) with \n",
    "for rr in rej_rates:\n",
    "    filtered_data = get_clear_images(model_output_data, (model, device, rr), multiclass=multiclass)\n",
    "    for typ in get_model_types(model_output_data)[:-1]: # Don't use the BLIP2-CTX-VQA model yet (the last one)\n",
    "        res = evaluate_acc_promptcap(filtered_data, typ)\n",
    "        p_imgs_rej = percent_imgs_rejected(model_output_data, filtered_data)\n",
    "        if len(filtered_data) == 0:\n",
    "            acc = 100.\n",
    "        else:\n",
    "            acc = (sum(res) / len(res)) * 100\n",
    "        blur_results.append((typ, acc))\n",
    "        #print(f\"--> {typ}: {acc:.4f}%\")\n",
    "        try:\n",
    "            model_type = map_type[typ]\n",
    "        except:\n",
    "            model_type = typ\n",
    "        data[\"model_type\"].append(model_type)\n",
    "        data[\"acc\"].append(acc)\n",
    "        data[\"thr\"].append(rr)\n",
    "        data[\"p_imgs_rej\"].append(p_imgs_rej)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba265e9-d82b-4690-8509-a5551a530d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(data)\n",
    "#df.sort_values('rej_rate', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f41ba2f-31f3-4b8d-beff-8369034e5d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values('p_imgs_rej', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e249cd-b4f1-46c2-9555-022559fe85fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d6b73ef1-e0b4-4821-86a9-43ccd3751512",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90a412c-c338-4d37-9184-aadf4c74b0f0",
   "metadata": {},
   "source": [
    "**Visualization** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29db46b-ad64-41a1-9d3d-6589a346dd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df[\"acc\"] != 100.]\n",
    "\n",
    "plt.figure(figsize=(7,4))\n",
    "for typ in get_model_types(model_output_data)[:-1]:\n",
    "    try:\n",
    "        typ = map_type[typ]\n",
    "    except:\n",
    "        typ = typ\n",
    "    data = df[df[\"model_type\"] == typ].sort_values('p_imgs_rej')\n",
    "\n",
    "    plt.plot(data[\"p_imgs_rej\"].tolist(),\n",
    "            data[\"acc\"].tolist(), label=typ)\n",
    "    plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.2),\n",
    "          fancybox=True, shadow=True, ncol=5, fontsize=7)\n",
    "    plt.xlabel(\"Percentage of rejected images\")\n",
    "    plt.ylabel(\"Accuracy (%)\")\n",
    "plt.title(\"VQA models accuracies using a multiclass filtering model\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aeca6ff-1fff-4ce7-b308-00f64bef84e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_df(model):\n",
    "    cdata = {\"model_type\": [],\n",
    "                      \"acc\": [],\n",
    "                   \"acc_w_blur_model\": []\n",
    "            }\n",
    "                #\"acc_imprv\" : []}\n",
    "\n",
    "    thr = 0.1\n",
    "    filt_data = get_clear_images(model_output_data, (model, device, thr), multiclass=multiclass)\n",
    "\n",
    "    for typ in get_model_types(model_output_data)[:-1]:\n",
    "        res = evaluate_acc_promptcap(model_output_data, typ)\n",
    "        res_filt = evaluate_acc_promptcap(filt_data, typ)\n",
    "        try:\n",
    "            model_type = map_type[typ]\n",
    "        except:\n",
    "            model_type = typ\n",
    "        acc = (sum(res) / len(res)) * 100\n",
    "        accwflaw = (sum(res_filt) / len(res_filt)) * 100\n",
    "        cdata[\"model_type\"].append(model_type)\n",
    "        cdata[\"acc\"].append(acc)\n",
    "        cdata[\"acc_w_blur_model\"].append(accwflaw)\n",
    "        #cdata[\"acc_imprv\"].append(accwflaw - acc)\n",
    "        \n",
    "    return pd.DataFrame.from_dict(cdata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0954605-a4f1-48d8-81af-1042c4099341",
   "metadata": {},
   "source": [
    "Accuries comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a675c275-bbd1-44dd-bb09-36b449493a43",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'calculate_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[110], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df_c \u001b[39m=\u001b[39m calculate_df(model)\n\u001b[1;32m      2\u001b[0m df_c\n",
      "\u001b[0;31mNameError\u001b[0m: name 'calculate_df' is not defined"
     ]
    }
   ],
   "source": [
    "df_c = calculate_df(model)\n",
    "df_c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08627a56-4cb2-4cd1-885b-015641e99963",
   "metadata": {},
   "source": [
    "## Models improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3360e5-513a-4ea8-8584-6b39bf8eb3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing package\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# plot data in stack manner of bar type\n",
    "df_c.plot(x='model_type', kind='bar', stacked=False, title='')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175f960f-c923-492c-afc9-969482384569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average model improvement across all models\n",
    "#mean = (df_c[\"acc_w_blur_model\"].mean() + df_c[\"acc\"].mean()) / 2\n",
    "#avg_imp = (df_c[\"acc_imprv\"].mean() / mean) * 100\n",
    "#avg_imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085e3dc4-7872-4292-8ca3-d1c3009116b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_classes = [\"BLR\", \"FRM\", \"MC\"]\n",
    "     \n",
    "c = []\n",
    "for mc in model_classes:\n",
    "    # Load and prepare filtering model\n",
    "    num_classes = 1 if mc in [\"BLR\", \"FRM\"] else 4\n",
    "    multiclass = True if num_classes > 1 else False\n",
    "    model_path = f'../outputs/best_{mc}_convnext.pth'\n",
    "    model = initialize_model(model_path, num_classes)\n",
    "    model_params = (model, device)\n",
    "    thr = 0.1\n",
    "    \n",
    "    # Filter data\n",
    "    filt_data = get_clear_images(model_output_data, (model, device, thr), multiclass=multiclass)\n",
    "    # Create df with results\n",
    "    df_c = calculate_df(model)\n",
    "\n",
    "    vqa_model_acc_w_improv = ((df_c[\"acc_w_blur_model\"] - df_c[\"acc\"]) / (df_c[\"acc\"])) * 100\n",
    "    \n",
    "    df_c[\"model_class\"] = mc\n",
    "    df_c[\"vqa_model_acc_w_improv\"] = vqa_model_acc_w_improv\n",
    "    df_c.set_index('model_class', inplace=True)\n",
    "    c.append(df_c)\n",
    "    \n",
    "df_f = pd.concat(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2baa8196-fe1f-497c-92eb-b52a548eac56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_m = pd.DataFrame.from_dict(df_f)\n",
    "#df_m.set_index('model_class', inplace=True)\n",
    "#df_m.plot(x='model_type', kind='bar', stacked=False, title='')\n",
    "#plt.show()\n",
    "#df_m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933356c2-7e4c-4461-90da-6202b77c33dc",
   "metadata": {},
   "source": [
    "## Models accuracy using Vision Language Models with context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89af1525-d7f4-450e-9872-0cd96a012f79",
   "metadata": {},
   "source": [
    "Can a visual language model (BLIP2-VQA) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0041e33-ce7b-4b40-82a7-8f5f560b9131",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_blip2 = evaluate_acc_promptcap(model_output_data, \"BLIP2-VQA\")\n",
    "res_blip2_ctx = evaluate_acc_promptcap(filt_data, \"BLIP2-CTX-VQA\")\n",
    "\n",
    "acc_blip2 = (sum(res_blip2) / len(res_blip2)) * 100\n",
    "acc_blip2_ctx = (sum(res_blip2_ctx) / len(res_blip2_ctx)) * 100\n",
    "\n",
    "df_f.loc[\"VQA-CTX\", \"model_type\"] = \"BLIP2-VQA\"\n",
    "df_f.loc[\"VQA-CTX\", \"acc\"] = acc_blip2\n",
    "df_f.loc[\"VQA-CTX\", \"acc_w_blur_model\"] = acc_blip2_ctx # BLIP2-CTX-VQA acc\n",
    "df_f.loc[\"VQA-CTX\", \"vqa_model_acc_w_improv\"] = ((acc_blip2_ctx - acc_blip2) / acc_blip2) * 100\n",
    "df_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da2547c-9bd4-4681-ac31-621fdfa9cc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "mins = df_f.groupby(\"model_class\").min()[\"vqa_model_acc_w_improv\"].to_numpy()\n",
    "maxes = df_f.groupby(\"model_class\").max()[\"vqa_model_acc_w_improv\"].to_numpy()\n",
    "means = df_f.groupby(\"model_class\").mean()[\"vqa_model_acc_w_improv\"].to_numpy()\n",
    "std = df_f.groupby(\"model_class\").std()[\"vqa_model_acc_w_improv\"].to_numpy()\n",
    "\n",
    "# create stacked errorbars:\n",
    "fig, ax = plt.subplots()\n",
    "#ax.errorbar(np.arange(len(mins)), means, std, fmt='ok', lw=3)\n",
    "ax.errorbar(np.arange(len(mins)), means, [means - mins, maxes - means],\n",
    "             fmt='.k', ecolor='green', lw=2)\n",
    "ax.set_title(\"Accuracy improvement across models for every approach\")\n",
    "\n",
    "labels = ['' for item in ax.get_xticklabels()]\n",
    "labels[1] = \"Blur\"\n",
    "labels[3] = \"Out of frame\"\n",
    "labels[5] = \"Multi-class\"\n",
    "labels[7] = \"VQA-CTX\"\n",
    "\n",
    "ax.set_xticklabels(labels)\n",
    "ax.set_xlabel('Model approach', fontsize=11)\n",
    "ax.set_ylabel('Accuracy improvement (%)', fontsize=11)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfg env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 (main, Mar 10 2023, 10:55:28) [GCC 11.3.0]"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
